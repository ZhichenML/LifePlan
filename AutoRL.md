### AutoRL

1. [*Network In Network*, by Min Lin & Shuicheng Yan et al. in ICLR, 2014](https://openreview.net/forum?id=ylE6yojDR5yqX) <!--"includes micro multi-layer
   perceptrons into the filters of convolutional layers to extract more complicated features."[DenseNet]-->

   > "includes micro multi-layer perceptrons into the filters of convolutional layers to extract more complicated features." [8]

2. [Deeply-Supervised Nets, by Chen-Yu Lee et al. in AISTATS, 2015](http://proceedings.mlr.press/v38/lee15a.pdf)

3. [Training Very Deep Networks, by Rupesh Kumar Srivastava et al. in NIPS, 2015](https://papers.nips.cc/paper/5850-training-very-deep-networks.pdf): *Highway network*

   ------

4. [Going Deeper with Convolutions, by Christian Szegedy et al. in CVPR, 2015](https://arxiv.org/abs/1409.4842): *Wider Networks*<!--Wider GoogLeNet with an inception module which concatenates feature-maps produced by filters of different sizes--> 

5. [Rethinking the Inception Architecture for Computer Vision, by Christian Szegedy et al. in CVPR, 2016](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf):

   ------

6. [Semi-supervised Learning with Ladder Networks](http://papers.nips.cc/paper/5947-semi-supervised-learning-with-ladder-ne): *Ladder Networks*

7. [Deconstructing the Ladder Network Architecture](http://proceedings.mlr.press/v48/pezeshki16.pdf)

   ---

8. [***Deep Residual Learning for Image Recognition***, by Kaiming He et al. in CVPR, 2016 ](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf): <u>*ResNet*, H(x) = F(x) + x, the rest of the network is refered to approximate F(x) = H(x) - x, by adding input x to network blocks.</u>

9. [Deep Networks with Stochastic Depth, by Gao Huang et al. in ECCV, 2016](https://arxiv.org/abs/1603.09382): *Stochastic Depth Network*

10. [Identity Mappings in Deep Residual Networks, by Kaiming He et al. in ECCV, 2016](https://arxiv.org/abs/1603.05027): *Pre-activation ResNet*

11. [Densely Connected Convolutional Networks, by Gao Huang et al. in CVPR, 2017](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf): *DenseNet*

12. [***Exploring Randomly Wired Neural Networks for Image Recognition***, by Saining Xie et al. in Arxiv, 2019](https://arxiv.org/pdf/1904.01569.pdf): <u>*RandomWired Network*, Thestrategy to generate the network, or network generator stated in the paper, introduce prior bias to the generated network and limit the network search space to a subspce. To circumvent the prior bias, this paper tries to explot random graphs as network generator and transform the graph into neural networks. The node operation is predifined and universal. It is similar to our idea of letting the network emerge itself, rather than pre-define any network types. </u> 

13. 

