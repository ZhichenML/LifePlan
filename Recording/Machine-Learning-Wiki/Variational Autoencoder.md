## Variational Inference for Latent Variable Models by Regulazation and Stochastic Optimization


Consider a latent variable model, where observed variable <img src="https://latex.codecogs.com/gif.latex?x" title="x" />  is generated by hidden variable <img src="https://latex.codecogs.com/gif.latex?z" title="z" />, and <img src="https://latex.codecogs.com/gif.latex?z" title="z" /> and <img src="https://latex.codecogs.com/gif.latex?x" title="x" /> are both parameterized by  <img src="https://latex.codecogs.com/gif.latex?\theta" title="\theta" />. We want to maximize the likelihood of data <img src="https://latex.codecogs.com/gif.latex?x" title="x" />  by learning optimal <img src="https://latex.codecogs.com/gif.latex?\theta" title="\theta" />. For this purpose, we also want to know the optimal distribution of hidden variable <img src="https://latex.codecogs.com/gif.latex?z" title="z" />, i.e. <img src="https://latex.codecogs.com/gif.latex?q_\phi(z|x)" title="q_\phi(z|x)" />.

Previously we have derived the evidence lower bound ([ELBO](https://github.com/Scott-Alex/Machine-Learning-Wiki/blob/master/Variational%20Inference.md)) by either maximize the likelihood of observed data or reorganizing the KL distance (which I think is more informative because you also know the quantity of the evidence and the ELBO).  That is 

<!--logp(x) \geq E_q[logp(x,z)-logq(z|\lambda)]-->
<img src="https://latex.codecogs.com/gif.latex?logp(x)&space;\geq&space;E_q[logp(x,z)-logq(z|\lambda)]" title="logp(x) \geq E_q[logp(x,z)-logq(z|\lambda)]" />

More exactly, one may want to calculate the approximated gradient by Monte Carlo sampling as we have done in [variational optimization](https://github.com/Scott-Alex/Machine-Learning-Wiki/blob/master/Variational%20Optimization.md). the approximated gradient is 
<!--\frac{\partial}{\partial \phi } E_z[f(z)]_{q(z|\phi)} = E_z[f(z)logq(z|\phi)]_{q(z|\phi)}-->
<img src="https://latex.codecogs.com/gif.latex?\frac{\partial}{\partial&space;\phi&space;}&space;E_z[f(z)]_{q(z|\phi)}&space;=&space;E_z[f(z)logq(z|\phi)]_{q(z|\phi)}" title="\frac{\partial}{\partial \phi } E_z[f(z)]_{q(z|\phi)} = E_z[f(z)logq(z|\phi)]_{q(z|\phi)}" />

However, despite we have got the bound, we are unable to solve the problem efficiently because the Monte Carlo sampling has a large variance to be used practically.

Therefore, we reorganized the bound as:
<!--\int q(z|x)log\frac{p_\lambda(x|z)p(z)}{q(z|x)}dz = \int q(z|x)logp_\lambda(x|z) - KL(q(z|x)||p(z))-->
<img src="https://latex.codecogs.com/gif.latex?\int&space;q(z|x)log\frac{p_\lambda(x|z)p(z)}{q(z|x)}dz&space;=&space;\int&space;q(z|x)logp_\lambda(x|z)&space;-&space;KL(q(z|x)||p(z))" title="\int q(z|x)log\frac{p_\lambda(x|z)p(z)}{q(z|x)}dz = \int q(z|x)logp_\lambda(x|z) - KL(q(z|x)||p(z))" />

where the first term is the generative process and indicates the negative generation error.
The second term is the KL distance between the posterior distribution and the prior distribution of <img src="https://latex.codecogs.com/gif.latex?z" title="z" />. By optimizing this objective, we are actually minimizing the data fitting error and a regularizer term, which reduces the variance.

The KL distance can be integrated analytically under mild assumptions on the distribution. 
For example, assuming <!--P_\theta(z) \sim N(0,I)--> <img src="https://latex.codecogs.com/gif.latex?P_\theta(z)&space;\sim&space;N(0,I)" title="P_\theta(z) \sim N(0,I)" />, <!--q_\phi(z|x) \sim N(u,\Sigma)--> <img src="https://latex.codecogs.com/gif.latex?q_\phi(z|x)&space;\sim&space;N(u,\Sigma)" title="q_\phi(z|x) \sim N(u,\Sigma)" />, then 

<!--KL[q_\phi(z|x)||P_\theta(z)]=\int q_\phi(z|x)log q_\phi(z|x)dz - \int q_\phi(z|x)log P_\theta(z)dz-->
<img src="https://latex.codecogs.com/gif.latex?KL[q_\phi(z|x)||P_\theta(z)]=\int&space;q_\phi(z|x)log&space;q_\phi(z|x)dz&space;-&space;\int&space;q_\phi(z|x)log&space;P_\theta(z)dz" title="KL[q_\phi(z|x)||P_\theta(z)]=\int q_\phi(z|x)log q_\phi(z|x)dz - \int q_\phi(z|x)log P_\theta(z)dz" />

<!--\int q_\phi(z|x)log q_\phi(z|x)dz \\
=\int N(u,\Sigma )log\frac{1}{(2\pi)^{J/2}det(\Sigma)^{1/2}}exp\{-\frac{1}{2}(z-u)^T\Sigma^{-1}(z-u)\}dz\\
=\int N(u,\Sigma )[-\frac{J}{2}log 2\pi - \frac{1}{2}logdet(\Sigma)-\frac{1}{2}(z-u)^T\Sigma^{-1}(z-u)]dz \\
=-\frac{J}{2}log 2\pi - \frac{1}{2}logdet(\Sigma)-\frac{\Sigma^{-1}}{2}\int N(u,\Sigma )(z-u)^T(z-u)dz\\
=-\frac{J}{2}log 2\pi - \frac{1}{2}logdet(\Sigma)-\frac{\Sigma^{-1}}{2}\int N(u,\Sigma )[z^2-2uz+u^2] dz\\
=-\frac{J}{2}log 2\pi - \frac{1}{2}logdet(\Sigma)-\frac{\Sigma^{-1}}{2}[u^2+\Sigma-2u^2+U^2]\\
=-\frac{J}{2}log 2\pi - \frac{1}{2}\sum_{j}log(\sigma_j^2)-\frac{J}{2}\\-->
<img src="https://latex.codecogs.com/gif.latex?\int&space;q_\phi(z|x)log&space;q_\phi(z|x)dz&space;\\&space;=\int&space;N(u,\Sigma&space;)log\frac{1}{(2\pi)^{J/2}det(\Sigma)^{1/2}}exp\{-\frac{1}{2}(z-u)^T\Sigma^{-1}(z-u)\}dz\\&space;=\int&space;N(u,\Sigma&space;)[-\frac{J}{2}log&space;2\pi&space;-&space;\frac{1}{2}logdet(\Sigma)-\frac{1}{2}(z-u)^T\Sigma^{-1}(z-u)]dz&space;\\&space;=-\frac{J}{2}log&space;2\pi&space;-&space;\frac{1}{2}logdet(\Sigma)-\frac{\Sigma^{-1}}{2}\int&space;N(u,\Sigma&space;)(z-u)^T(z-u)dz\\&space;=-\frac{J}{2}log&space;2\pi&space;-&space;\frac{1}{2}logdet(\Sigma)-\frac{\Sigma^{-1}}{2}\int&space;N(u,\Sigma&space;)[z^2-2uz&plus;u^2]&space;dz\\&space;=-\frac{J}{2}log&space;2\pi&space;-&space;\frac{1}{2}logdet(\Sigma)-\frac{\Sigma^{-1}}{2}[u^2&plus;\Sigma-2u^2&plus;U^2]\\&space;=-\frac{J}{2}log&space;2\pi&space;-&space;\frac{1}{2}\sum_{j}log(\sigma_j^2)-\frac{J}{2}\\" title="\int q_\phi(z|x)log q_\phi(z|x)dz \\ =\int N(u,\Sigma )log\frac{1}{(2\pi)^{J/2}det(\Sigma)^{1/2}}exp\{-\frac{1}{2}(z-u)^T\Sigma^{-1}(z-u)\}dz\\ =\int N(u,\Sigma )[-\frac{J}{2}log 2\pi - \frac{1}{2}logdet(\Sigma)-\frac{1}{2}(z-u)^T\Sigma^{-1}(z-u)]dz \\ =-\frac{J}{2}log 2\pi - \frac{1}{2}logdet(\Sigma)-\frac{\Sigma^{-1}}{2}\int N(u,\Sigma )(z-u)^T(z-u)dz\\ =-\frac{J}{2}log 2\pi - \frac{1}{2}logdet(\Sigma)-\frac{\Sigma^{-1}}{2}\int N(u,\Sigma )[z^2-2uz+u^2] dz\\ =-\frac{J}{2}log 2\pi - \frac{1}{2}logdet(\Sigma)-\frac{\Sigma^{-1}}{2}[u^2+\Sigma-2u^2+U^2]\\ =-\frac{J}{2}log 2\pi - \frac{1}{2}\sum_{j}log(\sigma_j^2)-\frac{J}{2}\\" />

and

<!-- \int q_\phi(z|x)log P_\theta(z)dz \\
=\int N(u,\Sigma)logN(0,I)dz\\
=-\frac{J}{2}log2\pi-\frac{1}{2}\sum_j(u_j^2+\sigma_j^2)-->
<img src="https://latex.codecogs.com/gif.latex?\int&space;q_\phi(z|x)log&space;P_\theta(z)dz&space;\\&space;=\int&space;N(u,\Sigma)logN(0,I)dz\\&space;=-\frac{J}{2}log2\pi-\frac{1}{2}\sum_j(u_j^2&plus;\sigma_j^2)" title="\int q_\phi(z|x)log P_\theta(z)dz \\ =\int N(u,\Sigma)logN(0,I)dz\\ =-\frac{J}{2}log2\pi-\frac{1}{2}\sum_j(u_j^2+\sigma_j^2)" />

Then,
<!--KL[q_\phi(z|x)||P_\theta(z)]=\int q_\phi(z|x)log q_\phi(z|x)dz - \int q_\phi(z|x)log P_\theta(z)dz\\
=-\frac{1}{2}\sum_{j}log\sigma_{j}^2 - \frac{J}{2}+\frac{1}{2}\sum_j(u^2_j+\sigma_j^2)\\
=-\frac{1}{2}\sum_j(1+log\sigma_{j}^2-u^2_j-\sigma_j^2)-->
<img src="https://latex.codecogs.com/gif.latex?KL[q_\phi(z|x)||P_\theta(z)]=\int&space;q_\phi(z|x)log&space;q_\phi(z|x)dz&space;-&space;\int&space;q_\phi(z|x)log&space;P_\theta(z)dz\\&space;=-\frac{1}{2}\sum_{j}log\sigma_{j}^2&space;-&space;\frac{J}{2}&plus;\frac{1}{2}\sum_j(u^2_j&plus;\sigma_j^2)\\&space;=-\frac{1}{2}\sum_j(1&plus;log\sigma_{j}^2-u^2_j-\sigma_j^2)" title="KL[q_\phi(z|x)||P_\theta(z)]=\int q_\phi(z|x)log q_\phi(z|x)dz - \int q_\phi(z|x)log P_\theta(z)dz\\ =-\frac{1}{2}\sum_{j}log\sigma_{j}^2 - \frac{J}{2}+\frac{1}{2}\sum_j(u^2_j+\sigma_j^2)\\ =-\frac{1}{2}\sum_j(1+log\sigma_{j}^2-u^2_j-\sigma_j^2)" />

Therefore, only the expected reconstruction error needs to be estimated by sampling.

We solve the problem by sampling from <img src="https://latex.codecogs.com/gif.latex?q_\phi(z|x)" title="q_\phi(z|x)" />,
<!--z^{(s)}\sim q_\phi(z|x)-->
<img src="https://latex.codecogs.com/gif.latex?z^{(s)}\sim&space;q_\phi(z|x)" title="z^{(s)}\sim q_\phi(z|x)" />
In this way, we convert the probabilistic variable into deterministic variable.
The final objective function is:
<!--L = \frac{1}{S}\sum_{s}logP_\theta(x|z^{(s)})-KL[q_{\phi}(z|x)||P_\theta(z)]-->
<img src="https://latex.codecogs.com/gif.latex?L&space;=&space;\frac{1}{S}\sum_{s}logP_\theta(x|z^{(s)})-KL[q_{\phi}(z|x)||P_\theta(z)]" title="L = \frac{1}{S}\sum_{s}logP_\theta(x|z^{(s)})-KL[q_{\phi}(z|x)||P_\theta(z)]" />

Each time we sampling <img src="https://latex.codecogs.com/gif.latex?M" title="M" /> datapoints from the dataset, and calculate an estimator of evidence lower bound,
<!--\frac{M}{N}\sum_i^M L(\theta,\phi,x^{(i)})-->
<img src="https://latex.codecogs.com/gif.latex?\frac{M}{N}\sum_i^M&space;L(\theta,\phi,x^{(i)})" title="\frac{M}{N}\sum_i^M L(\theta,\phi,x^{(i)})" />

If the minibatch is large enough, the sampling number of <img src="https://latex.codecogs.com/gif.latex?z" title="z" /> can be set as 1.
Then the gradients 

<img src="https://latex.codecogs.com/gif.latex?\frac{\partial}{\partial&space;(\theta,phi)}&space;L(\theta,\phi,x)" title="\frac{\partial}{\partial (\theta,phi)} L(\theta,\phi,x)" />

 are calculated and the parameter updating can be performed by SGD or Adagrad.

## Variational AutoEncoder
Variational Autoencoder employs recognizer with Gaussian output as <img src="https://latex.codecogs.com/gif.latex?q_\phi(z|x)" title="q_\phi(z|x)" />, and generator with Gaussian output as <img src="https://latex.codecogs.com/gif.latex?P_\theta(x,z)" title="P_\theta(x,z)" />. Reparameterization is employed to optimize the mean and variance of Gaussian distritbution respectively.

The pipeline is, the encoder outputs the hidden variable distribution, then sample one value for the generator. The gradient for the reconstruction error is back propagated to learning the parameters.


## Conclusion
When the number of discrete latent variables is too large or the latent variable is continuous, the posterior of hidden variable is intractable, When the dataset is too large, MC sampling is costly for an epoch. In addition, the sample result may induce large variance. To solve these problems,  the objective is reconstructed to regularize the hidden variable to reduce the variance, minibatch updating to scale to large dataset, and one MC sample per datapoint to decode.

Variatinal autoencoder is helpful for reducing the variance of sampling.

## Ref
[Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf)

[Frans's blog](http://kvfrans.com/a-intuitive-explanation-of-natural-gradient-descent/)
