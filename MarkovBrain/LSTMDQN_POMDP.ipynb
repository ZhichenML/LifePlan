{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import os, sys\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(15)\n",
    "tf.random.set_seed(15)\n",
    "random.seed(15)\n",
    "\n",
    "np.set_printoptions(precision=3, threshold=np.inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze(object):\n",
    "    WALL = 2\n",
    "    EMPTY = 8\n",
    "    LEFT = 0\n",
    "    RIGHT = 1 # right or forward\n",
    "    BONUS = 1000\n",
    "    def __init__(self, width, length): \n",
    "        self.length = length\n",
    "        self.width = width\n",
    "        self.maze = np.ones((self.width, self.length)) * Maze.WALL\n",
    "\n",
    "        self.generate_maze()\n",
    "        \n",
    "        #set self.maze_mask\n",
    "        #self.shortest_solutions\n",
    "        self.get_shortest_solutions()\n",
    "        \n",
    "        #self.longest_shortest, used to calculate objective value\n",
    "        self.get_longest_shortest_solutions()\n",
    "        \n",
    "        # used to normalize objective value\n",
    "        self.best_score = self.get_attainable_score()\n",
    "\n",
    "        #initialize the agent position in the maze\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def generate_maze(self):\n",
    "        # generate walls, doors\n",
    "        \n",
    "        spaces = np.random.randint(low=1, high=4, size=self.length)\n",
    "        cum_spaces = np.cumsum(spaces) # leave the first col empty\n",
    " \n",
    "        for ind, val in enumerate(cum_spaces):\n",
    "            if val >= self.length-1:\n",
    "                self.wall_position = cum_spaces[:ind]\n",
    "                break\n",
    "        if self.wall_position[0] > 1:\n",
    "            self.wall_position[0] = 1\n",
    "        if self.wall_position[-1] < self.length-1:\n",
    "            self.wall_position = np.append(self.wall_position, self.length-1)\n",
    "                \n",
    "        self.road_position = np.array([]).astype(np.int)\n",
    "        for ind in np.arange(self.length-1):\n",
    "            if ind not in self.wall_position:\n",
    "                self.road_position = np.append(self.road_position, ind)\n",
    "        \n",
    "        for i in self.road_position:\n",
    "            self.maze[1:-1,i]=Maze.EMPTY\n",
    "        \n",
    "        self.door_position = np.random.randint(low=1, high=self.width-1, size=len(self.wall_position))\n",
    "        #print(self.door_position)\n",
    "    \n",
    "        # get door position\n",
    "        self.door_position = np.zeros(len(self.wall_position), dtype = np.int)\n",
    "        self.door_position[-1] = np.random.randint(low=1, high=self.width-1) #1~self.width-2 available door position\n",
    "        for ind in np.arange(len(self.wall_position)-2, -1, -1):\n",
    "            if self.wall_position[ind] == self.wall_position[ind+1] -1: # two walls together\n",
    "                self.door_position[ind] = self.door_position[ind+1]\n",
    "                \n",
    "            else:\n",
    "                self.door_position[ind] = np.random.randint(low=1, high=self.width-1)\n",
    "        \n",
    "        # Fill door cue\n",
    "        self.maze[ self.door_position[-1], self.wall_position[-1] ] = Maze.RIGHT # default last door due\n",
    "        for i in np.arange(len(self.wall_position)-2, -1, -1):\n",
    "            if self.door_position[i+1] < self.door_position[i]:\n",
    "                self.maze[self.door_position[i], self.wall_position[i]] = Maze.LEFT\n",
    "            else: \n",
    "                self.maze[self.door_position[i], self.wall_position[i]] = Maze.RIGHT\n",
    "                \n",
    "                \n",
    "                \n",
    "       \n",
    "                \n",
    "    def print_maze(self, x=-1, y=-1):\n",
    "        if x>=0 and y>=0:\n",
    "            tmp = self.maze[x,y]\n",
    "            self.maze[x,y] = -1 # position of the agent\n",
    "            \n",
    "        print(\"  \", end=\"\")    \n",
    "        #for i in np.arange(self.length):\n",
    "        #    print('%d ' % i, end='')\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        for j in np.arange(self.width):\n",
    "            print('%d ' % j, end='')\n",
    "            for i in np.arange(self.length):\n",
    "            \n",
    "                if self.maze[j,i]==Maze.WALL: # wall position\n",
    "                    print('H ',end='')\n",
    "                elif self.maze[j,i]==Maze.EMPTY:\n",
    "                    print('  ',end='')# road\n",
    "                elif self.maze[j,i]==-1:\n",
    "                    print('T ',end='')\n",
    "                    self.maze[x,y]= tmp\n",
    "                else:\n",
    "                    print('%d ' % self.maze[j,i], end='')\n",
    "            print('\\n')\n",
    "\n",
    "        \n",
    "    def get_shortest_solutions(self):\n",
    "        # get the shortest length to the end of maze from each layer\n",
    "        \n",
    "        self.maze_mask = np.zeros(self.length, dtype=np.int)\n",
    "        for ind, val in enumerate(self.wall_position):\n",
    "            self.maze_mask[val] = self.door_position[ind]\n",
    "       \n",
    "        self.shortest_solutions = np.zeros(self.length, dtype=np.int)\n",
    "        step = 0\n",
    "        next_wall = self.length-1\n",
    "        for ind in np.arange(self.length-2, -1, -1):\n",
    "            if self.maze_mask[ind] == 0: # road\n",
    "                step += 1\n",
    "                self.shortest_solutions[ind] = step\n",
    "            else: # wall\n",
    "                step += np.abs(self.maze_mask[next_wall] - self.maze_mask[ind])+1 #1 out the door, +diff for vert.\n",
    "                self.shortest_solutions[ind] = step\n",
    "                next_wall = ind\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_distance_escape(self, x, y):\n",
    "        # get the shortest distance to escape from the current position\n",
    "        vertical_distance = 0\n",
    "        if y in self.road_position:\n",
    "            for next_wall_ind in np.arange(y+1, y+4, 1):\n",
    "                if next_wall_ind in self.wall_position: break\n",
    "            vertical_distance = np.abs(self.maze_mask[next_wall_ind] - x)\n",
    "        return self.shortest_solutions[y]+vertical_distance\n",
    "                \n",
    "\n",
    "        \n",
    "    def get_longest_shortest_solutions(self):\n",
    "        # get the shortest length from corner of starting to the end out maze\n",
    "        left = self.get_distance_escape(1,0)\n",
    "        right = self.get_distance_escape(self.width-2,0)\n",
    "        \n",
    "        self.longest_shortest = np.maximum(left, right)+5 # higher than true value\n",
    "    \n",
    "    \n",
    "    def get_attainable_score(self):\n",
    "        position = []\n",
    "        x = self.door_position[0] # in front of the first door\n",
    "        y = 0\n",
    "        position.append([x,y])\n",
    "        \n",
    "        score = np.float32(0)\n",
    "        door_signal=self.maze[self.door_position[0], 1]\n",
    "        r=[]\n",
    "        self.steps = 0\n",
    "        \n",
    "        while True:\n",
    "            pass_wall = False\n",
    "            self.steps += 1\n",
    "            if self.maze[x, y+1]!=Maze.WALL: # road\n",
    "                y += 1\n",
    "                pass_wall=True\n",
    "                if y in self.wall_position:\n",
    "                    door_signal = self.maze[x,y]\n",
    "            else: # wall\n",
    "                if door_signal == 0 and self.maze[x-1,y]==Maze.WALL: # init location make door signal no more signal\n",
    "                    door_signal = 1\n",
    "                if door_signal == 1 and self.maze[x+1,y]==Maze.WALL:\n",
    "                    door_signal = 0\n",
    "                x += int(door_signal*2-1)\n",
    "                \n",
    "            position.append([x,y])\n",
    "            r.append((self.longest_shortest - self.get_distance_escape(x,y) )/self.longest_shortest-1+int(pass_wall))\n",
    "            score += (self.longest_shortest - self.get_distance_escape(x,y) )/self.longest_shortest-1+int(pass_wall)\n",
    "            if y == self.length-1:\n",
    "                r[-1] += Maze.BONUS\n",
    "                score += Maze.BONUS\n",
    "                break\n",
    "        \n",
    "        #print(position)\n",
    "        self.average_reward = np.mean(r)\n",
    "     \n",
    "        return score\n",
    "        \n",
    "    \"\"\"\n",
    "    def get_attainable_score(self):\n",
    "        position = []\n",
    "        x = self.door_position[0] # in front of the first door\n",
    "        y = 0\n",
    "        score = np.float32(0)\n",
    "        pass_maze = 0\n",
    "        door_signal=self.maze[self.door_position[0], 1]\n",
    "        r=[]\n",
    "        for _ in np.arange(300, -1, -1):\n",
    "            position.append([x,y])\n",
    "            if y != self.length-1:\n",
    "                r.append((self.longest_shortest - self.get_distance_escape(x,y) )/self.longest_shortest + pass_maze)\n",
    "                score += (self.longest_shortest - self.get_distance_escape(x,y) )/self.longest_shortest + pass_maze\n",
    "            if self.maze[x, y+1]!=Maze.WALL: # road\n",
    "                y += 1\n",
    "                if y in self.wall_position:\n",
    "                    door_signal = self.maze[x,y]\n",
    "                if y == self.length-1:\n",
    "                    pass_maze += 1\n",
    "                    y=0\n",
    "            else: # wall\n",
    "                if door_signal == 0 and self.maze[x-1,y]==Maze.WALL: # init location make door signal no more signal\n",
    "                    door_signal = 1\n",
    "                if door_signal == 1 and self.maze[x+1,y]==Maze.WALL:\n",
    "                    door_signal = 0\n",
    "                x += int(door_signal*2-1)\n",
    "        \n",
    "        #print(position)\n",
    "        self.average_reward = np.mean(r)\n",
    "     \n",
    "        return score\n",
    "    \"\"\"\n",
    "    \n",
    "    def reset(self):\n",
    "        self.score = 0 \n",
    "        \n",
    "        self.position = np.array([self.door_position[-1], 0]) # in front of the last door\n",
    "        self.trajectory = []\n",
    "        self.trajectory.append(self.position)\n",
    "        \n",
    "        \n",
    "        x, y = self.position\n",
    "        observation = self.perception()\n",
    "        \n",
    "        return observation\n",
    "        \n",
    "    def perception(self):\n",
    "        x, y = self.position\n",
    "        observation = np.zeros(6)\n",
    "        \n",
    "        if self.maze[x,y+1] == Maze.WALL:\n",
    "            observation[0]=1\n",
    "        else: observation[0]=0\n",
    "        \n",
    "        if self.maze[x-1,y+1] == Maze.WALL:\n",
    "            observation[1]=1\n",
    "        else: observation[1]=0\n",
    "        \n",
    "        if self.maze[x+1,y+1] == Maze.WALL:\n",
    "            observation[2] = 1\n",
    "        else: observation[2]=0\n",
    "        \n",
    "        if self.maze[x-1,y] == Maze.WALL:\n",
    "            observation[4]=1\n",
    "        else: observation[4]=0\n",
    "        \n",
    "        if self.maze[x+1,y] == Maze.WALL:\n",
    "            observation[5]=1\n",
    "        else: observation[5]=0\n",
    "        \n",
    "        if y in self.wall_position:\n",
    "            observation[3] = self.maze[x, y]\n",
    "            \n",
    "        return observation\n",
    "            \n",
    "    def step(self, action):\n",
    "        \n",
    "        x, y = self.position\n",
    "\n",
    "        crash_wall = False\n",
    "        pass_wall = False\n",
    "        if action == 0: #down == 1 and up == 0:\n",
    "            if self.maze[x+1,y]==Maze.WALL:\n",
    "                crash_wall = True\n",
    " \n",
    "            if  self.maze[x+1,y] != Maze.WALL:\n",
    "                self.position = x+1, y\n",
    "                self.trajectory.append(self.position)\n",
    "\n",
    "        elif action == 1: #down == 0 and up == 1:\n",
    "            if self.maze[x-1,y] == Maze.WALL:\n",
    "                crash_wall = True\n",
    "            \n",
    "            if  self.maze[x-1,y] != Maze.WALL:\n",
    "                self.position = x-1, y\n",
    "                self.trajectory.append(self.position)\n",
    "\n",
    "\n",
    "        elif action == 2: #down == 1 and up == 1 or down == 0 and up == 0:\n",
    "        \n",
    "            if self.maze[x,y+1] != Maze.WALL:\n",
    "                pass_wall = True\n",
    "                self.position = x,y+1\n",
    "                self.trajectory.append(self.position)\n",
    "            else:\n",
    "                crash_wall = True\n",
    "                \n",
    "        #elif down == 0 and up == 0:\n",
    "        #    self.position = x, y\n",
    "        #    self.trajectory.append(self.position)\n",
    "            \n",
    "        \n",
    "        x,y = self.position\n",
    "        reward = (self.longest_shortest - self.get_distance_escape(x,y))/self.longest_shortest -1 \n",
    "\n",
    "        reward += int(pass_wall) - int(crash_wall)\n",
    "\n",
    "        self.score += reward    \n",
    "        fitness = self.get_fitness()\n",
    "        \n",
    "  \n",
    "        \n",
    "        if y == self.length-1:# at the end of the maze \n",
    "            done = True\n",
    "            observation_ = np.ones(6)\n",
    "            reward += Maze.BONUS # the final reward should be larger than sum of small reward on the way\n",
    "            self.score += Maze.BONUS\n",
    "            fitness = self.get_fitness()\n",
    "        else:\n",
    "            done = False\n",
    "            observation_ = self.perception()\n",
    "\n",
    "\n",
    "        return observation_, reward, done\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_fitness(self):\n",
    "        \n",
    "        return self.score#/self.best_score \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Mar 14 10:38:25 2018\n",
    "\n",
    "@author: lenovo\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. /(1 + np.exp(-x))\n",
    "    \n",
    "def stable_sigmoid1(x):\n",
    "    import math\n",
    "    return math.exp(-np.logaddexp(0,-x))\n",
    "\n",
    "def stable_sigmoid2(x):\n",
    "    if x >= 0:\n",
    "        return 1. /(1 + np.exp(-x))\n",
    "    else:\n",
    "        z = np.exp(x)\n",
    "        return z /(1+z)\n",
    "\n",
    "def sigmoid_derivative (value):\n",
    "    return value * (1-value)\n",
    "\n",
    "def tanh_derivative(value):\n",
    "    return 1. - value ** 2\n",
    "\n",
    "def random_init(a, b, *args):\n",
    "    np.random.seed(0)\n",
    "    return np.random.rand(*args) * (b - a) + a\n",
    "\n",
    "class LSTMParam:\n",
    "    \n",
    "    def __init__(self, hidden_size, x_dim):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.x_dim = x_dim\n",
    "        concate_dim = hidden_size + x_dim\n",
    "        # init network parameters\n",
    "        self.Wg = random_init(-0.1, 0.1, hidden_size, concate_dim)\n",
    "        self.Wi = random_init(-0.1, 0.1, hidden_size, concate_dim)\n",
    "        self.Wf = random_init(-0.1, 0.1, hidden_size, concate_dim)\n",
    "        self.Wo = random_init(-0.1, 0.1, hidden_size, concate_dim)\n",
    "        self.bg = random_init(-0.1, 0.1, hidden_size)\n",
    "        self.bi = random_init(-0.1, 0.1, hidden_size)\n",
    "        self.bf = random_init(-0.1, 0.1, hidden_size)\n",
    "        self.bo = random_init(-0.1, 0.1, hidden_size)\n",
    "        # init network derivatives\n",
    "        self.Wg_diff = np.zeros((hidden_size, concate_dim)) \n",
    "        self.Wi_diff = np.zeros((hidden_size, concate_dim)) \n",
    "        self.Wf_diff = np.zeros((hidden_size, concate_dim)) \n",
    "        self.Wo_diff = np.zeros((hidden_size, concate_dim)) \n",
    "        self.bg_diff = np.zeros(hidden_size) \n",
    "        self.bi_diff = np.zeros(hidden_size) \n",
    "        self.bf_diff = np.zeros(hidden_size) \n",
    "        self.bo_diff = np.zeros(hidden_size)\n",
    "    \n",
    "#        self.Wg_diff = np.zeros_like(self.Wg)\n",
    "#        self.Wi_diff = np.zeros_like(self.Wi)\n",
    "#        self.Wf_diff = np.zeros_like(self.Wf)\n",
    "#        self.Wo_diff = np.zeros_like(self.Wo)\n",
    "#        self.bg_diff = np.zeros_like(self.bg)\n",
    "#        self.bi_diff = np.zeros_like(self.bi)\n",
    "#        self.bf_diff = np.zeros_like(self.bf)\n",
    "#        self.bo_diff = np.zeros_like(self.bo)\n",
    "        \n",
    "        \n",
    "    def apply_diff(self, sample_cnt, lr = 1,):\n",
    "        self.Wg -= self.Wg_diff * lr / sample_cnt\n",
    "        self.Wi -= self.Wi_diff * lr / sample_cnt\n",
    "        self.Wf -= self.Wf_diff * lr / sample_cnt\n",
    "        self.Wo -= self.Wo_diff * lr / sample_cnt\n",
    "        self.bg -= self.bg_diff * lr / sample_cnt\n",
    "        self.bi -= self.bi_diff * lr / sample_cnt\n",
    "        self.bf -= self.bf_diff * lr / sample_cnt\n",
    "        self.bo -= self.bo_diff * lr / sample_cnt\n",
    "        # re-set the derivatives as zeros\n",
    "        self.Wg_diff = np.zeros_like(self.Wg)\n",
    "        self.Wi_diff = np.zeros_like(self.Wi)\n",
    "        self.Wf_diff = np.zeros_like(self.Wf)\n",
    "        self.Wo_diff = np.zeros_like(self.Wo)\n",
    "        self.bg_diff = np.zeros_like(self.bg)\n",
    "        self.bi_diff = np.zeros_like(self.bi)\n",
    "        self.bf_diff = np.zeros_like(self.bf)\n",
    "        self.bo_diff = np.zeros_like(self.bo)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "class LSTMstate:\n",
    "    def __init__(self, hidden_size):\n",
    "        self.g = np.zeros(hidden_size)\n",
    "        self.i = np.zeros(hidden_size)\n",
    "        self.f = np.zeros(hidden_size)\n",
    "        self.o = np.zeros(hidden_size)\n",
    "        self.s = np.zeros(hidden_size)\n",
    "        self.h = np.zeros(hidden_size)\n",
    "        \n",
    "        self.diff_h = np.zeros_like(self.h)\n",
    "        self.diff_s = np.zeros_like(self.s)\n",
    "        \n",
    "class LSTMnode:\n",
    "    \n",
    "    def __init__(self, LSTMParam, LSTMstate):\n",
    "        self.Param = LSTMParam\n",
    "        self.state = LSTMstate\n",
    "        self.concate_x = None\n",
    "    \n",
    "    \n",
    "    def forward(self, x, h_prev = None, s_prev = None):\n",
    "        if s_prev is None: s_prev = np.zeros_like(self.state.s)\n",
    "        if h_prev is None: h_prev = np.zeros_like(self.state.h)\n",
    "        \n",
    "        self.s_prev = s_prev\n",
    "        self.h_prev = h_prev\n",
    "        \n",
    "        \"\"\" it should be the last hidden state \"\"\"\n",
    "        #print(x)\n",
    "        #print(h_prev)\n",
    "        concate_x = np.hstack((x, h_prev))\n",
    "        \"\"\" it should be the sigmoid function for the gates \"\"\"\n",
    "        self.state.g = np.tanh(np.dot(self.Param.Wg, concate_x) + self.Param.bg)\n",
    "        self.state.i = sigmoid(np.dot(self.Param.Wi, concate_x) + self.Param.bi)\n",
    "        self.state.f = sigmoid(np.dot(self.Param.Wf, concate_x) + self.Param.bf)\n",
    "        self.state.o = sigmoid(np.dot(self.Param.Wo, concate_x) + self.Param.bo)\n",
    "        self.state.s = self.state.f * self.s_prev + self.state.i * self.state.g \n",
    "        self.state.h = self.state.s * self.state.o\n",
    "        self.concate_x = concate_x\n",
    "           \n",
    "    \n",
    "    def BPTT(self, diff_h, diff_s):\n",
    "        # there intermediate results not strored in LSTMnode\n",
    "        ds = self.state.o * diff_h + diff_s\n",
    "        do = self.state.s * diff_h\n",
    "        di = self.state.g * ds\n",
    "        dg = self.state.i * ds \n",
    "        df = self.s_prev * ds\n",
    "        \n",
    "        # derivative go through the activation function\n",
    "        di_input = sigmoid_derivative(self.state.i) * di\n",
    "        df_input = sigmoid_derivative(self.state.f) * df\n",
    "        do_input = sigmoid_derivative(self.state.o) * do\n",
    "        dg_input = tanh_derivative(self.state.g) * dg\n",
    "        \n",
    "        # derivative of parameters, *property of Param.\n",
    "        self.Param.Wg_diff += np.outer(dg_input, self.concate_x)\n",
    "        self.Param.Wi_diff += np.outer(di_input, self.concate_x)\n",
    "        self.Param.Wf_diff += np.outer(df_input, self.concate_x)\n",
    "        self.Param.Wo_diff += np.outer(do_input, self.concate_x)\n",
    "        self.Param.bg_diff += dg_input;\n",
    "        self.Param.bi_diff += di_input;\n",
    "        self.Param.bf_diff += df_input;\n",
    "        self.Param.bo_diff += do_input;\n",
    "        \n",
    "        # derivative of last hidden state, * used before defined\n",
    "        diff_concate_x = np.zeros_like(self.concate_x)\n",
    "        diff_concate_x += np.dot(self.Param.Wg.T, dg_input)\n",
    "        diff_concate_x += np.dot(self.Param.Wi.T, di_input)\n",
    "        diff_concate_x += np.dot(self.Param.Wf.T, df_input)\n",
    "        diff_concate_x += np.dot(self.Param.Wo.T, do_input)\n",
    "        \n",
    "        self.state.diff_h = diff_concate_x[self.Param.x_dim:]\n",
    "        self.state.diff_s = ds * self.state.f\n",
    "         \n",
    "        \n",
    "class LSTMnetwork:\n",
    "    \n",
    "    def __init__(self, Param):\n",
    "        self.n_output = 3\n",
    "        self.Param = Param\n",
    "        self.x_list = []\n",
    "        self.node_list = []\n",
    "    \n",
    "    def x_list_clear(self):\n",
    "        self.x_list = []\n",
    "        self.node_list = []\n",
    "        \n",
    "    def predict(self, x):\n",
    "        self.x_list.append(x)\n",
    "        if len(self.x_list) > len(self.node_list):\n",
    "            Lstmstate = LSTMstate(self.Param.hidden_size)\n",
    "            self.node_list.append(LSTMnode(self.Param, Lstmstate))\n",
    "            \n",
    "        idx = len(self.x_list)-1\n",
    "        if idx == 0:\n",
    "            self.node_list[idx].forward(self.x_list[idx])\n",
    "        else:\n",
    "            s_prev = self.node_list[idx-1].state.s\n",
    "            h_prev = self.node_list[idx-1].state.h\n",
    "            self.node_list[idx].forward(self.x_list[idx],s_prev,h_prev)\n",
    "     \n",
    "         \n",
    "    def get_loss(self, y_list, loss_layer):\n",
    "        \n",
    "        assert len(self.x_list) == len(y_list)\n",
    "        idx = len(self.x_list)-1\n",
    "        pred = self.node_list[idx].state.h\n",
    "        loss = loss_layer.loss(pred, y_list[idx])\n",
    "        diff_h = loss_layer.diff(pred, y_list[idx])\n",
    "        \n",
    "        \"\"\" it should be zeros not zeros_like \"\"\"\n",
    "        diff_s = np.zeros(self.Param.hidden_size)\n",
    "        self.node_list[idx].BPTT(diff_h, diff_s)\n",
    "        \n",
    "        idx -= 1\n",
    "        \n",
    "        while idx >= 0:\n",
    "            loss += loss_layer.loss(self.node_list[idx].state.h, y_list[idx])\n",
    "            diff_h = loss_layer.diff(self.node_list[idx].state.h, y_list[idx])\n",
    "            diff_h += self.node_list[idx+1].state.diff_h\n",
    "            diff_s = self.node_list[idx+1].state.diff_s\n",
    "            self.node_list[idx].BPTT(diff_h, diff_s)\n",
    "            idx -= 1\n",
    "            \n",
    "            \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Mar 14 22:25:14 2018\n",
    "\n",
    "@author: Zhichen\n",
    "2018/3/16 linear regression layer parameters are too large, solved by dividing the number of time points.\n",
    "RNN don't need to devide sample number, but linear regression must.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from mylstm import LSTMParam, LSTMnetwork\n",
    "\n",
    "\n",
    "\n",
    "class linear_regression_layer:\n",
    "    \"\"\"the functions are all @classmethod\"\"\"\n",
    "    \"\"\" self is a must \"\"\"\n",
    "    @classmethod\n",
    "    def random_init(self, a, b, *args): \n",
    "        np.random.seed(0)\n",
    "        return np.random.rand(*args) * (b - a) + a\n",
    "    \n",
    "    \"\"\" self is needed in calling the function, otherwise the dim is wrong\n",
    "    classmethod don't need self parameter\"\"\"\n",
    "    @classmethod\n",
    "    def __init__(self, out_dim, in_dim):\n",
    "        self.W = self.random_init( -0.1, 0.1, in_dim) \n",
    "        self.b = self.random_init( -0.1, 0.1, out_dim) \n",
    "        \n",
    "        self.diff_W = np.zeros_like(self.W)\n",
    "        self.diff_b = np.zeros_like(self.b)\n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def loss(self, inputs, label):\n",
    "        self.output = np.dot(self.W, inputs) + self.b \n",
    "        loss = (self.output - label) ** 2 #+ 0.5 * np.sum(list(map(lambda x: x **2, self.W)))\n",
    "        #loss = np.sum(list(map(lambda x: (x[0] - x[1]) ** 2, zip(list(self.output), label))))\n",
    "        return loss\n",
    "    \n",
    "    @classmethod\n",
    "    def diff(self, pred, label):\n",
    "        diff_input = np.zeros_like(pred) \n",
    "        d_input = 2 * (self.output - label)\n",
    "        #d_input = 2 * list(map(lambda x: x[0] - x[1], zip(self.output, label)))\n",
    "        diff_input = self.W * d_input\n",
    "        \n",
    "        self.diff_W += d_input * pred  #+ self.W\n",
    "        self.diff_b += np.sum(d_input)\n",
    "        \n",
    "        \n",
    "        return diff_input\n",
    "    \n",
    "    @classmethod\n",
    "    def output_layer_diff(self, sample_cnt, lr = 1):\n",
    "        self.W -= lr * self.diff_W / sample_cnt\n",
    "        self.b -= lr * self.diff_b / sample_cnt\n",
    "       \n",
    "        self.diff_W = np.zeros_like(self.W)\n",
    "        self.diff_b = np.zeros_like(self.b)\n",
    "\n",
    "        \n",
    "        \n",
    "def example_LR():\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    x_dim = 50\n",
    "    y_dim = 1\n",
    "    hidden_size = 50\n",
    "\n",
    "    Lstmparam = LSTMParam(hidden_size, x_dim)\n",
    "    Lstmnet = LSTMnetwork(Lstmparam)\n",
    "    linear_regression_layer(y_dim, hidden_size)\n",
    "    \n",
    "    #generate a dataset (a sequence)\n",
    "    y_list = [-0.5, 0.2, 0.1, -0.5]\n",
    "    x_list = [np.random.random(x_dim) for _ in y_list]\n",
    "    # the data is the same, check the derivative\n",
    "    for iter_epoch in range(100):\n",
    "        print(\"iter %2s\" % iter_epoch, end=\": \")\n",
    "        for ind in range(len(y_list)):\n",
    "            Lstmnet.predict(x_list[ind])\n",
    "            \n",
    "        print(\"y_pred = [\" +\n",
    "              \", \".join([\"% 2.5f\" % Lstmnet.node_list[ind].state.h[0] for ind in range(len(y_list))]) +\n",
    "              \"]\", end=\", \")\n",
    "            \n",
    "        loss = Lstmnet.get_loss(y_list, linear_regression_layer)\n",
    "        print(\"loss:\", \"%.3e\" % loss)\n",
    "    \n",
    "        Lstmparam.apply_diff(len(y_list), lr = 0.1)\n",
    "        linear_regression_layer.output_layer_diff(len(y_list), lr = 0.1)\n",
    "        Lstmnet.x_list_clear()\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.plot(range(num_epoch), Epoch_loss)\n",
    "    plt.xlabel('Training Epoches')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('LSTM Training Loss')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class Euclidean_loss_layer(object):\n",
    "    # class method, otherwise, parameter missing\n",
    "    def __init__(self, n_output):\n",
    "        self.n_output = n_output\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "\n",
    "        return np.sum((pred[:self.n_output] - label) **2)\n",
    "    \n",
    "    #def loss(self, predict, label):\n",
    "    #    return list(map(lambda x: (x[o] - x[1])**2, zip(predict, label)))\n",
    "#    @classmethod\n",
    "#    def diff(self, predict, label): #* wrong derivative\n",
    "#        derivative = np.zeros_like(predict)\n",
    "#        derivative[0] = 2 * (predict[0] -label)\n",
    "#        return derivative\n",
    "\n",
    "    def diff(self, pred, label):\n",
    "        diff = np.zeros_like(pred)\n",
    "        diff[:self.n_output] = 2 * (pred[:self.n_output] - label)\n",
    "        return diff\n",
    "\n",
    "\n",
    "def example():\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    x_dim = 10\n",
    "    hidden_size = 5\n",
    "\n",
    "    Lstmparam = LSTMParam(hidden_size, x_dim)\n",
    "    Lstmnet = LSTMnetwork(Lstmparam)\n",
    "    loss_layer = Euclidean_loss_layer(n_output = 1)\n",
    "    \n",
    "    y_list = [-0.5, 0.2, 0.1, -0.5]\n",
    "    x_list = [np.random.random(x_dim) for _ in y_list]\n",
    "    # the data is the same, check the derivative\n",
    "    \n",
    "    eps_loss = []\n",
    "    num_epoch = 100\n",
    "    for iter_epoch in range(num_epoch):\n",
    "        print(\"iter\", \"%2s\" % str(iter_epoch), end=\": \")\n",
    "        for ind in range(len(y_list)):\n",
    "            Lstmnet.predict(x_list[ind])\n",
    "            \n",
    "        print(\"y_pred = [\" +\n",
    "              \", \".join([\"% 2.5f\" % Lstmnet.node_list[ind].state.h[0] for ind in range(len(y_list))]) +\n",
    "              \"]\", end=\", \")\n",
    "            \n",
    "        loss = Lstmnet.get_loss(y_list, loss_layer)\n",
    "        print(\"loss:\", \"%.3e\" % loss)\n",
    "        eps_loss.append(loss)\n",
    "    \n",
    "        Lstmparam.apply_diff(len(y_list),lr = 0.1)\n",
    "        Lstmnet.x_list_clear()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(num_epoch), eps_loss)\n",
    "    plt.xlabel('Training Epoches')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('LSTM Training Loss')\n",
    "    plt.show()\n",
    "    \n",
    "#if __name__ == \"__main__\":\n",
    "    #example()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin DQN here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "class CalculateEpsilon:\n",
    "    def __init__(self, init_eps, decay_rate=0.99999):\n",
    "        self.init_eps = init_eps\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "    def __call__(self):\n",
    "        return_eps = self.init_eps\n",
    "        self.init_eps *= self.decay_rate\n",
    "        return return_eps\n",
    "    \n",
    "    \n",
    "\"\"\"=======================================================================\"\"\"\n",
    "\n",
    "\n",
    "def prepare_batch(experience_replay, batch_size):\n",
    "    random.shuffle(experience_replay)\n",
    "    return [experience_replay[i] for i in range(batch_size)]\n",
    "\n",
    "def prepare_batch_seq(experience_replay):\n",
    "    return experience_replay\n",
    "\n",
    "def preprocess_observation(observation):\n",
    "    \"\"\"Just A Reminder\"\"\"\n",
    "    return observation\n",
    "\n",
    "\n",
    "\"\"\"=======================================================================\"\"\"\n",
    "\n",
    "def run_agent_greedy_eps(agent, latest_obs, epsilon, environment, debug=False):\n",
    "    \"\"\"\n",
    "    Running DQN Agent Based on Greedy Epsilon exploration Scheme\n",
    "\n",
    "    Args:\n",
    "        latest_obs -- Latest Observation, which will be fed to the network\n",
    "            directly.\n",
    "        epsilon -- The current epsilon.\n",
    "        environment -- The current environment.\n",
    "        debug -- If True, Show the predicted Q of the Deep Q Agent.\n",
    "\n",
    "    Returns:\n",
    "        action_int -- action in integer format, for executing in the environment\n",
    "        action -- one hot representation of actions.\n",
    "        Q_value -- The predicted Q-Value\n",
    "    \"\"\"\n",
    "\n",
    "    obs = latest_obs #np.expand_dims(latest_obs, axis=0)\n",
    "    Q_value = agent.call(obs)[:agent.n_output]\n",
    "    if debug:\n",
    "        print(Q_value)\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        action_int = np.random.choice(len(Q_value))\n",
    "    else:\n",
    "        max_index = np.argwhere(Q_value == np.max(Q_value)).flatten().tolist()\n",
    "        action_int = np.random.choice(max_index)\n",
    "        #action_int = int(np.argmax(Q_value.numpy(), axis=1)[0])\n",
    "\n",
    "    action = np.zeros((1, len(Q_value)))\n",
    "    action[:, action_int] = 1\n",
    "    return action_int, action, Q_value\n",
    "\n",
    "\n",
    "def calculate_training_loss(agent_mirror, target_mirror, current_obs, actions, next_obs, rewards, is_done_mask, gamma):\n",
    "    \"\"\"\n",
    "    Calculate the training loss\n",
    "\n",
    "    Args:\n",
    "        agent -- the current agent we want to train.\n",
    "        target_agent -- the target Q-Network\n",
    "        current_obs -- Tensor of the current observation\n",
    "        actions -- Tensor of choosen actions\n",
    "        next_obs -- Tensor of observation after taking action\n",
    "        rewards -- Tensor of reward after taking the action\n",
    "        is_done_mask -- Telling whether this leads to the end of eps\n",
    "        gamma -- Discount Reward\n",
    "\n",
    "    Returns:\n",
    "        loss -- training loss\n",
    "    \"\"\"\n",
    "\n",
    "    agent = copy.deepcopy(agent_mirror)\n",
    "    agent.model.x_list_clear()\n",
    "    target_agent = copy.deepcopy(target_mirror)\n",
    "    target_agent.model.x_list_clear()\n",
    "    \n",
    "    T = len(current_obs)\n",
    "    rpt_loss=[]\n",
    "    for rpt in range(5):\n",
    "\n",
    "        for ind in range(T):\n",
    "            agent.model.predict(current_obs[ind, :])\n",
    "\n",
    "\n",
    "        # get label sequence\n",
    "        target = np.zeros((T, agent.n_output))\n",
    "        #print(actions)\n",
    "        for ind in range(T):\n",
    "            next_Q_value = target_agent.call(next_obs[ind,:])[:agent.n_output]\n",
    "            next_Q = np.max(next_Q_value)\n",
    "\n",
    "            Q_target = rewards[ind] + gamma * next_Q * is_done_mask[ind]\n",
    "\n",
    "            target[ind, :] = agent.model.node_list[ind].state.h[:agent.n_output]\n",
    "            target[ind,actions[ind]] = Q_target\n",
    "\n",
    "\n",
    "        loss = agent.model.get_loss(target, agent.loss_layer)\n",
    "        rpt_loss.append(loss)\n",
    "\n",
    "        agent.model.Param.apply_diff(len(target), lr = 0.01)\n",
    "        agent.model.x_list_clear()\n",
    "        target_agent.model.x_list_clear()\n",
    "\n",
    "    agent_mirror.model.Param = copy.deepcopy(agent.model.Param)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMQModel(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_output = 3\n",
    "        hidden_size  = 64\n",
    "        x_dim = 6\n",
    "        Lstmparam = LSTMParam(hidden_size, x_dim)\n",
    "        Lstmnet = LSTMnetwork(Lstmparam)\n",
    "        \n",
    "        self.model = Lstmnet\n",
    "        self.loss_layer = Euclidean_loss_layer(n_output = self.n_output)\n",
    "        \n",
    "    def call(self, obs):\n",
    "        self.model.predict(obs)\n",
    "        idx = len(self.model.node_list)-1\n",
    "        return self.model.node_list[idx].state.h\n",
    "                \n",
    "        #loss = Lstmnet.get_loss(y_list, loss_layer)\n",
    "        \n",
    "        #Lstmparam.apply_diff(len(y_list),lr = 0.1)\n",
    "        #Lstmnet.x_list_clear()\n",
    "        \n",
    "class Memory():\n",
    "    \n",
    "    def __init__(self,memsize):\n",
    "        self.memsize = memsize\n",
    "        self.memory = collections.deque(maxlen=self.memsize)\n",
    "    \n",
    "    def add_episode(self,epsiode):\n",
    "        self.memory.append(epsiode)\n",
    "    \n",
    "    def get_batch(self, batch_size, time_step):\n",
    "        sampled_epsiodes = random.sample(self.memory,batch_size)\n",
    "        batch = []\n",
    "        for episode in sampled_epsiodes:\n",
    "            point = np.random.randint(0,len(episode)+1-time_step)\n",
    "            batch.append(episode[point:point+time_step])\n",
    "            \n",
    "        return np.stack(batch)\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\"\"\"\n",
    "from RL.models.cartpole import CartpoleModel\n",
    "from RL.DeepQ.Exploration import CalculateEpsilon\n",
    "from RL.DeepQ.Agent import run_agent_greedy_eps, calculate_training_loss\n",
    "from RL.DeepQ.utils import prepare_batch, preprocess_cartpole\n",
    "\"\"\"\n",
    "np.random.seed(48)\n",
    "tf.random.set_seed(48)\n",
    "random.seed(48)\n",
    "#logger = logging.getLogger(os.path.basename(sys.argv[0]))\n",
    "\n",
    "import gym\n",
    "\n",
    "def play():\n",
    "    # -------------------- * --------------------\n",
    "    \"\"\"\n",
    "    argparser = argparse.ArgumentParser('DQN', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    argparser.add_argument('--replay-size', action=\"store\", type=int, default=10000)\n",
    "    argparser.add_argument('--epsilon-decay-rate', action=\"store\", type=float, default=0.99999)\n",
    "    argparser.add_argument('--gamma', action=\"store\", type=float, default=0.99)\n",
    "    argparser.add_argument('--episode-train', action=\"store\", type=int, default=2000)\n",
    "    argparser.add_argument('--batch-size', action=\"store\", type=int, default=32)\n",
    "    argparser.add_argument('--moving-target-rate', action=\"store\", type=int, default=0.0005)\n",
    "\n",
    "    argparser.add_argument('--learning-rate', action=\"store\", type=float, default=2e-5)\n",
    "\n",
    "    argparser.add_argument('--output-save-img', action=\"store\", type=str, default=None)\n",
    "\n",
    "    args = argparser.parse_args(argv)\n",
    "    \"\"\"\n",
    "    replay_size = 1000\n",
    "    eps_decay_rate = 0.999\n",
    "    gamma = 0.99\n",
    "    eps_train = 30000\n",
    "    batch_size = 64\n",
    "    learning_rate = 1e-4\n",
    "    output_save_img = './Fig/'+'LstmDQN.pdf'\n",
    "    target_rate = 0.01\n",
    "    max_step = 300\n",
    "    update_target_freq = 10000\n",
    "    update_freq = 16\n",
    "    save_freq = 100\n",
    "    seq_len = 32\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------    \n",
    "    agent = LSTMQModel()\n",
    "    target_agent = copy.deepcopy(agent)\n",
    "    #experience_replay = collections.deque([], maxlen=replay_size) ## remind to use it\n",
    "    experience_replay = Memory(replay_size)\n",
    "    \n",
    "    epsilon = CalculateEpsilon(0.5)\n",
    "\n",
    "    reward_curve = np.zeros(eps_train)\n",
    "    loss_stat = []\n",
    "    \n",
    "    # ------------------Fill memory-------------------------   \n",
    "    for memory_ind in range(replay_size):\n",
    "        env = Maze(10, 50)\n",
    "        observation = env.reset()\n",
    "        observation = preprocess_observation(observation)\n",
    "        step = 0\n",
    "        experience_replay_eps = []\n",
    "        #last_action = np.zeros(agent.n_output)\n",
    "        while True:\n",
    "\n",
    "            action_int = np.random.randint(0, 3)\n",
    "            \n",
    "            next_observation, reward, is_done = env.step(action_int)\n",
    "            next_observation = preprocess_observation(next_observation)\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            experience_replay_eps.append( (observation, action_int, reward, next_observation, is_done) )\n",
    "\n",
    "            observation = next_observation\n",
    "            \n",
    "            if is_done == True or step>max_step:\n",
    "                break\n",
    "                \n",
    "        experience_replay.add_episode(experience_replay_eps)\n",
    "    print('Populated with %d Episodes' % len(experience_replay.memory))\n",
    "        \n",
    "        \n",
    "    # -------------------Training------------------------ \n",
    "    total_step = 0\n",
    "    \n",
    "    for current_eps_num in range(eps_train):\n",
    "        if current_eps_num % 1 ==0: print('.',end='')\n",
    "        if current_eps_num >0 and current_eps_num%50 ==0: print(' ')\n",
    "            \n",
    "        \n",
    "        env = Maze(10, 50)\n",
    "        observation = env.reset()\n",
    "        observation = preprocess_observation(observation)\n",
    "        eps_reward = []\n",
    "        step = 0\n",
    "        experience_replay_eps = []\n",
    "        \n",
    "        while True:\n",
    "        \n",
    "            action_int, action_onehot, Q_val = run_agent_greedy_eps(agent, observation, epsilon(), env)\n",
    "            \n",
    "            next_observation, reward, is_done = env.step(action_int)\n",
    "            next_observation = preprocess_observation(next_observation)\n",
    "            \n",
    "            eps_reward.append(reward)\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "            \n",
    "            experience_replay_eps.append( (observation, action_int, reward, next_observation, is_done) )\n",
    "\n",
    "            observation = next_observation\n",
    "            \n",
    "            #======================================Learning=========================================\n",
    "            # Need To do Interpolation Later\n",
    "            # if frames % 5000 == 0:\n",
    "            if total_step % update_target_freq == 0:\n",
    "                target_agent.model.Param.Wg = (1- target_rate) * target_agent.model.Param.Wg+ target_rate *agent.model.Param.Wg\n",
    "                target_agent.model.Param.Wi = (1- target_rate) * target_agent.model.Param.Wi+ target_rate *agent.model.Param.Wi\n",
    "                target_agent.model.Param.Wf = (1- target_rate) * target_agent.model.Param.Wf+ target_rate *agent.model.Param.Wf\n",
    "                target_agent.model.Param.Wo = (1- target_rate) * target_agent.model.Param.Wo+ target_rate *agent.model.Param.Wo\n",
    "                target_agent.model.Param.bg = (1- target_rate) * target_agent.model.Param.bg+ target_rate *agent.model.Param.bg\n",
    "                target_agent.model.Param.bi = (1- target_rate) * target_agent.model.Param.bi+ target_rate *agent.model.Param.bi\n",
    "                target_agent.model.Param.bf = (1- target_rate) * target_agent.model.Param.bf+ target_rate *agent.model.Param.bf\n",
    "                target_agent.model.Param.bo = (1- target_rate) * target_agent.model.Param.bo+ target_rate *agent.model.Param.bo\n",
    "\n",
    "            \"\"\"\n",
    "            if frames % batch_size==0 or is_done==True or frames > max_step: # \n",
    "                \n",
    "                all_observation, all_actions, all_reward, \\\n",
    "                    all_next_observation, all_is_done = zip(*prepare_batch_seq(experience_replay_eps))\n",
    "                \n",
    "                all_observation = np.squeeze(all_observation)\n",
    "                all_actions = np.squeeze(all_actions)\n",
    "                all_reward = np.squeeze(all_reward)\n",
    "                all_next_observation = np.squeeze(all_next_observation)\n",
    "                all_is_done = np.squeeze(all_is_done)\n",
    "           \n",
    "\n",
    "                # Packing Everything to Tensor\n",
    "                current_obs = np.stack(all_observation)\n",
    "                actions = np.stack(all_actions)\n",
    "                next_obs = np.stack(all_next_observation)\n",
    "                rewards =  np.array(all_reward)\n",
    "                is_done_mask = np.array([not a for a in all_is_done]).astype(np.int32)\n",
    "                \n",
    "                # Calculating Loss\n",
    "                loss = calculate_training_loss(\n",
    "                    agent,\n",
    "                    target_agent,\n",
    "                    current_obs,\n",
    "                    actions,\n",
    "                    next_obs,\n",
    "                    rewards,\n",
    "                    is_done_mask,\n",
    "                    gamma\n",
    "                )\"\"\"\n",
    "\n",
    "            #if frames%500 == 0 and frames != 0:\n",
    "            #    logger.info(f\"At {frames}: The loss is {loss} and the epsilon is {epsilon.init_eps}\")\n",
    "            \n",
    "            if total_step % update_freq == 0:\n",
    "        \n",
    "                #loss = Lstmnet.get_loss(y_list, loss_layer)\n",
    "                #print(\"loss:\", \"%.3e\" % loss)\n",
    "                #eps_loss.append(loss)\n",
    "\n",
    "                #Lstmparam.apply_diff(len(y_list),lr = 0.1)\n",
    "                #Lstmnet.x_list_clear()\n",
    "                agent_tmp = copy.deepcopy(agent)\n",
    "                agent_tmp.model.x_list_clear()\n",
    "                \n",
    "                batch = experience_replay.get_batch(batch_size=batch_size, time_step=seq_len)\n",
    "                for batch_ind, seqence in enumerate(batch):\n",
    "                    \n",
    "                    all_observation, all_actions, all_reward, \\\n",
    "                        all_next_observation, all_is_done = zip(*prepare_batch_seq(seqence))\n",
    "\n",
    "                    all_observation = np.squeeze(all_observation)\n",
    "                    all_actions = np.squeeze(all_actions)\n",
    "                    all_reward = np.squeeze(all_reward)\n",
    "                    all_next_observation = np.squeeze(all_next_observation)\n",
    "                    all_is_done = np.squeeze(all_is_done)\n",
    "\n",
    "\n",
    "                    # Packing Everything to Tensor\n",
    "                    current_obs = np.stack(all_observation)\n",
    "                    actions = np.stack(all_actions)\n",
    "                    next_obs = np.stack(all_next_observation)\n",
    "                    rewards =  np.array(all_reward)\n",
    "                    is_done_mask = np.array([not a for a in all_is_done]).astype(np.int32)\n",
    "\n",
    "\n",
    "                    T = len(current_obs)\n",
    "\n",
    "\n",
    "                    for rpt in range(1):\n",
    "\n",
    "                        if len(agent_tmp.model.node_list)==0:\n",
    "                            for ind in range(T):\n",
    "                                agent_tmp.model.predict(current_obs[ind, :])\n",
    "                        \n",
    "                        # get label sequence\n",
    "                        if len(target_agent.model.node_list) == 0:\n",
    "                            target = np.zeros((T, agent_tmp.n_output))\n",
    "                            #print(actions)\n",
    "                            for ind in range(T):\n",
    "                                next_Q_value = target_agent.call(next_obs[ind,:])[:agent.n_output]\n",
    "                                next_Q = np.max(next_Q_value)\n",
    "\n",
    "                                Q_target = rewards[ind] + gamma * next_Q * is_done_mask[ind]\n",
    "                                \n",
    "                                target[ind, :] = agent_tmp.model.node_list[ind].state.h[:agent.n_output]\n",
    "                                target[ind, actions[ind]] = Q_target\n",
    "                \n",
    "                        loss = agent_tmp.model.get_loss(target, agent_tmp.loss_layer)\n",
    "                        #rpt_loss.append(loss)\n",
    "                        loss_stat.append(loss)\n",
    "\n",
    "                        agent_tmp.model.Param.apply_diff(len(target), lr = learning_rate)\n",
    "                        agent_tmp.model.x_list_clear()\n",
    "                        \n",
    "                    target_agent.model.x_list_clear()\n",
    "\n",
    "                agent.model.Param = copy.deepcopy(agent_tmp.model.Param)\n",
    "                \n",
    "            if is_done == True or step >= max_step:\n",
    "                agent.model.x_list_clear()\n",
    "                target_agent.model.x_list_clear()\n",
    "                \n",
    "                break\n",
    "\n",
    "                \n",
    "        experience_replay.add_episode(experience_replay_eps)\n",
    "        \n",
    "        reward_curve[current_eps_num] = np.mean(eps_reward)/env.average_reward\n",
    "        \n",
    "        if current_eps_num >0 and current_eps_num % save_freq == 0:\n",
    "            perf = {}\n",
    "            perf['loss_stat'] = loss_stat\n",
    "            perf['reward_curve'] = reward_curve\n",
    "            perf['experience_replay'] = experience_replay\n",
    "            save_obj(name='./Fig/LSTM_POMDP',obj=perf)\n",
    "            save_obj(name='./Fig/agent0', obj =agent)\n",
    "            \n",
    "            #print(reward_curve)\n",
    "            fig, ax = plt.subplots()\n",
    "            plt.plot(np.arange(len(reward_curve[:current_eps_num])), reward_curve[:current_eps_num], 'r-')\n",
    "            plt.xlabel(\"Number of Episodes\")\n",
    "            plt.ylabel(\"Performance\")\n",
    "            #plt.legend() # 显示图例\n",
    "            plt.savefig(output_save_img)\n",
    "    \n",
    "    #print(reward_curve)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(np.arange(len(reward_curve)), reward_curve, 'r-')\n",
    "    plt.xlabel(\"Number of Episodes\")\n",
    "    plt.ylabel(\"Performance\")\n",
    "    #plt.legend() # 显示图例\n",
    "    if output_save_img is not None:\n",
    "        plt.savefig(output_save_img)\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    #print(' '.join(sys.argv))\n",
    "    play()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
