{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import os, sys\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "np.random.seed(15)\n",
    "tf.random.set_random_seed(15)\n",
    "random.seed(15)\n",
    "\n",
    "np.set_printoptions(precision=2, threshold=np.inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze(object):\n",
    "    WALL = 2\n",
    "    EMPTY = 8\n",
    "    LEFT = 0\n",
    "    RIGHT = 1 # right or forward\n",
    "    BONUS = 1000\n",
    "    \n",
    "    def __init__(self, width, length): \n",
    "        self.length = length\n",
    "        self.width = width\n",
    "        self.maze = np.ones((self.width, self.length)) * Maze.WALL\n",
    "\n",
    "        self.generate_maze()\n",
    "        \n",
    "        #set self.maze_mask\n",
    "        #self.shortest_solutions\n",
    "        self.get_shortest_solutions()\n",
    "        \n",
    "        #self.longest_shortest, used to calculate objective value\n",
    "        self.get_longest_shortest_solutions()\n",
    "        \n",
    "        # used to normalize objective value\n",
    "        self.best_score = self.get_attainable_score()\n",
    "\n",
    "        #initialize the agent position in the maze\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def generate_maze(self):\n",
    "        # generate walls, doors\n",
    "        \n",
    "        spaces = np.random.randint(low=1, high=4, size=self.length)\n",
    "        cum_spaces = np.cumsum(spaces) # leave the first col empty\n",
    " \n",
    "        for ind, val in enumerate(cum_spaces):\n",
    "            if val >= self.length-1:\n",
    "                self.wall_position = cum_spaces[:ind]\n",
    "                break\n",
    "        if self.wall_position[0] > 1:\n",
    "            self.wall_position[0] = 1\n",
    "        if self.wall_position[-1] < self.length-1:\n",
    "            self.wall_position = np.append(self.wall_position, self.length-1)\n",
    "                \n",
    "        self.road_position = np.array([]).astype(np.int)\n",
    "        for ind in np.arange(self.length-1):\n",
    "            if ind not in self.wall_position:\n",
    "                self.road_position = np.append(self.road_position, ind)\n",
    "        \n",
    "        for i in self.road_position:\n",
    "            self.maze[1:-1,i]=Maze.EMPTY\n",
    "        \n",
    "        self.door_position = np.random.randint(low=1, high=self.width-1, size=len(self.wall_position))\n",
    "        #print(self.door_position)\n",
    "    \n",
    "        # get door position\n",
    "        self.door_position = np.zeros(len(self.wall_position), dtype = np.int)\n",
    "        self.door_position[-1] = np.random.randint(low=1, high=self.width-1) #1~self.width-2 available door position\n",
    "        for ind in np.arange(len(self.wall_position)-2, -1, -1):\n",
    "            if self.wall_position[ind] == self.wall_position[ind+1] -1: # two walls together\n",
    "                self.door_position[ind] = self.door_position[ind+1]\n",
    "                \n",
    "            else:\n",
    "                self.door_position[ind] = np.random.randint(low=1, high=self.width-1)\n",
    "        \n",
    "        # Fill door cue\n",
    "        self.maze[ self.door_position[-1], self.wall_position[-1] ] = Maze.RIGHT # default last door due\n",
    "        for i in np.arange(len(self.wall_position)-2, -1, -1):\n",
    "            if self.door_position[i+1] < self.door_position[i]:\n",
    "                self.maze[self.door_position[i], self.wall_position[i]] = Maze.LEFT\n",
    "            else: \n",
    "                self.maze[self.door_position[i], self.wall_position[i]] = Maze.RIGHT\n",
    "                \n",
    "                \n",
    "                \n",
    "       \n",
    "                \n",
    "    def print_maze(self, x=-1, y=-1):\n",
    "        if x>=0 and y>=0:\n",
    "            tmp = self.maze[x,y]\n",
    "            self.maze[x,y] = -1 # position of the agent\n",
    "            \n",
    "        print(\"  \", end=\"\")    \n",
    "        #for i in np.arange(self.length):\n",
    "        #    print('%d ' % i, end='')\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        for j in np.arange(self.width):\n",
    "            print('%d ' % j, end='')\n",
    "            for i in np.arange(self.length):\n",
    "            \n",
    "                if self.maze[j,i]==Maze.WALL: # wall position\n",
    "                    print('H ',end='')\n",
    "                elif self.maze[j,i]==Maze.EMPTY:\n",
    "                    print('  ',end='')# road\n",
    "                elif self.maze[j,i]==-1:\n",
    "                    print('T ',end='')\n",
    "                    self.maze[x,y]= tmp\n",
    "                else:\n",
    "                    print('%d ' % self.maze[j,i], end='')\n",
    "            print('\\n')\n",
    "\n",
    "        \n",
    "    def get_shortest_solutions(self):\n",
    "        # get the shortest length to the end of maze from each layer\n",
    "        \n",
    "        self.maze_mask = np.zeros(self.length, dtype=np.int)\n",
    "        for ind, val in enumerate(self.wall_position):\n",
    "            self.maze_mask[val] = self.door_position[ind]\n",
    "       \n",
    "        self.shortest_solutions = np.zeros(self.length, dtype=np.int)\n",
    "        step = 0\n",
    "        next_wall = self.length-1\n",
    "        for ind in np.arange(self.length-2, -1, -1):\n",
    "            if self.maze_mask[ind] == 0: # road\n",
    "                step += 1\n",
    "                self.shortest_solutions[ind] = step\n",
    "            else: # wall\n",
    "                step += np.abs(self.maze_mask[next_wall] - self.maze_mask[ind])+1 #1 out the door, +diff for vert.\n",
    "                self.shortest_solutions[ind] = step\n",
    "                next_wall = ind\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_distance_escape(self, x, y):\n",
    "        # get the shortest distance to escape from the current position\n",
    "        vertical_distance = 0\n",
    "        if y in self.road_position:\n",
    "            for next_wall_ind in np.arange(y+1, y+4, 1):\n",
    "                if next_wall_ind in self.wall_position: break\n",
    "            vertical_distance = np.abs(self.maze_mask[next_wall_ind] - x)\n",
    "        return self.shortest_solutions[y]+vertical_distance\n",
    "                \n",
    "\n",
    "        \n",
    "    def get_longest_shortest_solutions(self):\n",
    "        # get the shortest length from corner of starting to the end out maze\n",
    "        left = self.get_distance_escape(1,0)\n",
    "        right = self.get_distance_escape(self.width-2,0)\n",
    "        \n",
    "        self.longest_shortest = np.maximum(left, right)+5 # higher than true value\n",
    "    \n",
    "    \n",
    "    def get_attainable_score(self):\n",
    "        position = []\n",
    "        x = self.door_position[0] # in front of the first door\n",
    "        y = 0\n",
    "        position.append([x,y])\n",
    "        \n",
    "        score = np.float32(0)\n",
    "        door_signal=self.maze[self.door_position[0], 1]\n",
    "        r=[]\n",
    "        self.steps = 0\n",
    "        \n",
    "        while True:\n",
    "            self.steps += 1\n",
    "            if self.maze[x, y+1]!=Maze.WALL: # road\n",
    "                y += 1\n",
    "                if y in self.wall_position:\n",
    "                    door_signal = self.maze[x,y]\n",
    "            else: # wall\n",
    "                if door_signal == 0 and self.maze[x-1,y]==Maze.WALL: # init location make door signal no more signal\n",
    "                    door_signal = 1\n",
    "                if door_signal == 1 and self.maze[x+1,y]==Maze.WALL:\n",
    "                    door_signal = 0\n",
    "                x += int(door_signal*2-1)\n",
    "                \n",
    "            position.append([x,y])\n",
    "            r.append((self.longest_shortest - self.get_distance_escape(x,y) )/self.longest_shortest-1)\n",
    "            score += (self.longest_shortest - self.get_distance_escape(x,y) )/self.longest_shortest-1\n",
    "            if y == self.length-1:\n",
    "                r[-1] += Maze.BONUS\n",
    "                score += Maze.BONUS\n",
    "                break\n",
    "\n",
    "        self.average_reward = np.mean(r)\n",
    "     \n",
    "        return score\n",
    "        \n",
    "    \"\"\"\n",
    "    def get_attainable_score(self):\n",
    "        position = []\n",
    "        x = self.door_position[0] # in front of the first door\n",
    "        y = 0\n",
    "        score = np.float32(0)\n",
    "        pass_maze = 0\n",
    "        door_signal=self.maze[self.door_position[0], 1]\n",
    "        r=[]\n",
    "        for _ in np.arange(300, -1, -1):\n",
    "            position.append([x,y])\n",
    "            if y != self.length-1:\n",
    "                r.append((self.longest_shortest - self.get_distance_escape(x,y) )/self.longest_shortest + pass_maze)\n",
    "                score += (self.longest_shortest - self.get_distance_escape(x,y) )/self.longest_shortest + pass_maze\n",
    "            if self.maze[x, y+1]!=Maze.WALL: # road\n",
    "                y += 1\n",
    "                if y in self.wall_position:\n",
    "                    door_signal = self.maze[x,y]\n",
    "                if y == self.length-1:\n",
    "                    pass_maze += 1\n",
    "                    y=0\n",
    "            else: # wall\n",
    "                if door_signal == 0 and self.maze[x-1,y]==Maze.WALL: # init location make door signal no more signal\n",
    "                    door_signal = 1\n",
    "                if door_signal == 1 and self.maze[x+1,y]==Maze.WALL:\n",
    "                    door_signal = 0\n",
    "                x += int(door_signal*2-1)\n",
    "        \n",
    "        #print(position)\n",
    "        self.average_reward = np.mean(r)\n",
    "     \n",
    "        return score\n",
    "    \"\"\"\n",
    "    \n",
    "    def reset(self):\n",
    "        self.score = 0 \n",
    "        \n",
    "        self.position = np.array([self.door_position[-1], 0]) # in front of the last door\n",
    "        self.trajectory = []\n",
    "        self.trajectory.append(self.position)\n",
    "        \n",
    "        \n",
    "        x, y = self.position\n",
    "        observation = self.perception()\n",
    "        \n",
    "        return observation\n",
    "        \n",
    "    def perception(self):\n",
    "        x, y = self.position\n",
    "        observation = np.zeros(6)\n",
    "        \n",
    "        if self.maze[x,y+1] == Maze.WALL:\n",
    "            observation[0]=1\n",
    "        else: observation[0]=0\n",
    "        \n",
    "        if self.maze[x-1,y+1] == Maze.WALL:\n",
    "            observation[1]=1\n",
    "        else: observation[1]=0\n",
    "        \n",
    "        if self.maze[x+1,y+1] == Maze.WALL:\n",
    "            observation[2] = 1\n",
    "        else: observation[2]=0\n",
    "        \n",
    "        if self.maze[x-1,y] == Maze.WALL:\n",
    "            observation[4]=1\n",
    "        else: observation[4]=0\n",
    "        \n",
    "        if self.maze[x+1,y] == Maze.WALL:\n",
    "            observation[5]=1\n",
    "        else: observation[5]=0\n",
    "        \n",
    "        if y in self.wall_position:\n",
    "            observation[3] = self.maze[x, y]\n",
    "            \n",
    "        return observation\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        x, y = self.position\n",
    "        \n",
    "        up = int(action[0])\n",
    "        down = int(action[1])\n",
    "        \n",
    "        crash_wall = False\n",
    "        pass_wall = False\n",
    "        if down == 1 and up == 0:\n",
    "            if self.maze[x+1,y]==Maze.WALL:\n",
    "                crash_wall = True\n",
    " \n",
    "            if  self.maze[x+1,y] != Maze.WALL:\n",
    "                self.position = x+1, y\n",
    "                self.trajectory.append(self.position)\n",
    "\n",
    "        elif down == 0 and up == 1:\n",
    "            if self.maze[x-1,y] == Maze.WALL:\n",
    "                crash_wall = True\n",
    "            \n",
    "            if  self.maze[x-1,y] != Maze.WALL:\n",
    "                self.position = x-1, y\n",
    "                self.trajectory.append(self.position)\n",
    "\n",
    "\n",
    "        elif down == 1 and up == 1:\n",
    "        \n",
    "            if self.maze[x,y+1] != Maze.WALL:\n",
    "                pass_wall = True\n",
    "                self.position = x,y+1\n",
    "                self.trajectory.append(self.position)\n",
    "            else:\n",
    "                crash_wall = True\n",
    "                \n",
    "        elif down == 0 and up == 0:\n",
    "            self.position = x, y\n",
    "            self.trajectory.append(self.position)\n",
    "            \n",
    "        \n",
    "        x,y = self.position\n",
    "        reward = (self.longest_shortest - self.get_distance_escape(x,y))/self.longest_shortest -1 \n",
    "        reward = int(pass_wall) - int(crash_wall)\n",
    "        self.score +=  reward    \n",
    "        fitness = self.get_fitness()\n",
    "        \n",
    "  \n",
    "        \n",
    "        if y == self.length-1:# at the end of the maze \n",
    "            done = True\n",
    "            observation_ = 'Terminal'\n",
    "            reward += Maze.BONUS # the final reward should be larger than sum of small reward on the way\n",
    "            self.score += Maze.BONUS\n",
    "            fitness = self.get_fitness()\n",
    "        else:\n",
    "            done = False\n",
    "            observation_ = self.perception()\n",
    "\n",
    "\n",
    "        return observation_, fitness, reward, done\n",
    "            \n",
    "    \"\"\"def step(self, action):\n",
    "        \n",
    "        x, y = self.position\n",
    "        \n",
    "        up = int(action[0])\n",
    "        down = int(action[1])\n",
    "        \n",
    "        if down == 1 and up == 0:\n",
    "            #if self.maze.maze[x+1,y]==Maze.WALL:\n",
    "            #    r-= 1\n",
    " \n",
    "            if  self.maze[x+1,y] != Maze.WALL:\n",
    "                self.position = x+1, y\n",
    "                self.trajectory.append(self.position)\n",
    "\n",
    "        elif down == 0 and up == 1:\n",
    "            #if self.maze.maze[x-1,y] == Maze.WALL:\n",
    "            #    r-= 1\n",
    "            \n",
    "            if  self.maze[x-1,y] != Maze.WALL:\n",
    "                self.position = x-1, y\n",
    "                self.trajectory.append(self.position)\n",
    "\n",
    "\n",
    "        elif down == 1 and up == 1 or down == 0 and up == 0:\n",
    "        \n",
    "            if self.maze[x,y+1] != Maze.WALL:\n",
    "                #r+=20\n",
    "                self.position = x,y+1\n",
    "                self.trajectory.append(self.position)\n",
    "            #else:\n",
    "            #    r-= 1\n",
    "        \n",
    "        x,y = self.position\n",
    "        reward = (self.longest_shortest - self.get_distance_escape(x,y))/self.longest_shortest -1\n",
    "        self.score +=  reward    \n",
    "        fitness = self.get_fitness()\n",
    "        \n",
    "\n",
    "        if y == self.length-1:# at the end of the maze \n",
    "            done = True\n",
    "            observation_ = np.zeros(6)\n",
    "            reward += Maze.BONUS\n",
    "            self.score+= Maze.BONUS\n",
    "            fitness = self.get_fitness()\n",
    "            \n",
    "        else:\n",
    "            done = False\n",
    "            observation_ = self.perception()\n",
    "            \n",
    "        \n",
    "\n",
    "        return observation_, fitness, reward, done\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def get_fitness(self):\n",
    "        \n",
    "        return self.score#/self.best_score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Draw(fitness, xlabel=\"Episodes\", ylabel=\"Fitness\", label = 'fitness trend', constant=0):\n",
    "    plt.plot(np.arange(len(fitness)), fitness, color='blue', label=label,linestyle = '-')\n",
    "    plt.plot(np.arange(len(fitness)), [constant]*len(fitness), color='red', label=\"Best\",linestyle = '-')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend() # 显示图例\n",
    "    plt.show()\n",
    "\n",
    "def num2action(num):\n",
    "    # set the output nodes according to the action\n",
    "    # used when have an action and want to set the output nodes\n",
    "        numbers = {\n",
    "            0 : '011',\n",
    "            1 : '100',\n",
    "            2 : '110',\n",
    "            3 : '111'\n",
    "        }\n",
    "        return numbers.get(num, None)\n",
    "    \n",
    "def action2num(action):\n",
    "    # get the action according to the output nodes\n",
    "    # used when have the action and want to get the action index and update q_table\n",
    "    numbers = {\n",
    "            '011':0,\n",
    "            '100':1,\n",
    "            '110':2,\n",
    "            '111':3\n",
    "        }\n",
    "    return numbers.get(action, None)\n",
    "    \n",
    "    \n",
    "def observation2index(observation): \n",
    "    # observation: array\n",
    "    # get the state index in the q-table\n",
    "    input_val, marker = 0, 1\n",
    "        \n",
    "    for val in observation: # 03456\n",
    "        if val == 1:\n",
    "            input_val += marker\n",
    "        marker *= 2\n",
    "    return int(input_val)\n",
    "\n",
    "\n",
    "\n",
    "class QAgent:\n",
    "\n",
    "    best_input_ids = [0,3,4,5,6]\n",
    "    best_output_ids = [6,7,8]\n",
    "    best_gates = np.array([[0,0,0,0,0,0,1,0],\n",
    "                        [0,0,0,0,1,0,0,0],\n",
    "                        [0,0,0,0,0,0,1,0],\n",
    "                        [0,0,0,0,1,0,0,0],\n",
    "                        [0,0,0,0,0,0,1,0],\n",
    "                        [0,0,0,1,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,1,0],\n",
    "                        [0,0,0,1,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,1,0],\n",
    "                        [0,0,0,0,1,0,0,0],\n",
    "                        [0,0,0,0,0,0,1,0],\n",
    "                        [0,0,0,0,1,0,0,0],\n",
    "                        [0,0,0,0,0,0,1,0],\n",
    "                        [1,0,0,0,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,0,1],\n",
    "                        [1,0,0,0,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,0,1],\n",
    "                        [0,0,0,1,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,0,1],\n",
    "                        [0,0,0,1,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,0,1],\n",
    "                        [0,0,0,1,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,0,1],\n",
    "                        [0,0,0,1,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,0,1],\n",
    "                        [0,0,0,0,1,0,0,0],\n",
    "                        [0,0,0,0,0,0,0,1],\n",
    "                        [0,0,0,0,1,0,0,0],\n",
    "                        [0,0,0,0,0,0,1,0],\n",
    "                        [1,0,0,0,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,0,1],\n",
    "                        [1,0,0,0,0,0,0,0]])\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, input_ids, output_ids, learning_rate=0.01, reward_decay=0.98, e_greedy=0.9):\n",
    "        \n",
    "        self.input_ids=input_ids\n",
    "        self.output_ids=output_ids\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = np.zeros((2**len(input_ids), 2**2))\n",
    "        \n",
    "        self.memory = 0\n",
    "\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # observation 03456\n",
    "        # action 876\n",
    "        # action selection\n",
    "        if np.random.uniform() < self.epsilon: # choose best action\n",
    "            \n",
    "            input_val = observation2index(observation)\n",
    "            \n",
    "            state_action = self.q_table[input_val, :]\n",
    "            # some actions may have the same value, randomly choose on in these actions\n",
    "            max_index = np.argwhere(state_action == np.max(state_action)).flatten().tolist()\n",
    "            output_val = np.random.choice(max_index)\n",
    "            #print(observation, input_val, state_action, max_index, output_val, num2action(output_val))\n",
    "        else:\n",
    "            # choose random action\n",
    "            output_val = np.random.choice(2**2)\n",
    "        \n",
    "        action = num2action(output_val)\n",
    "        self.memory = int(action[-1])\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def learn(self, s, a, r, s_):\n",
    "        input_val = observation2index(s)\n",
    "        action = action2num(str(a))\n",
    "        q_predict = self.q_table[input_val, action]\n",
    "        \n",
    "        next_input_val = observation2index(s_)\n",
    "        if next_input_val != 31:#s_ != 'Terminal':\n",
    "            q_target = r + self.gamma * self.q_table[next_input_val, :].max()  # next state is not terminal\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "        self.q_table[input_val, action] += self.lr * (q_target - q_predict)  # update\n",
    "\n",
    "\n",
    "def update(input_ids, output_ids):\n",
    "\n",
    "    agent = QAgent(input_ids, output_ids)\n",
    "    \n",
    "    experience_buffer = []\n",
    "    \n",
    "    Num_episode = 300\n",
    "    max_step = 300\n",
    "    fitness = np.zeros(Num_episode)\n",
    "    average_reward = np.zeros(Num_episode)\n",
    "    \n",
    "    for episode in range(Num_episode):\n",
    "        if episode > 0 and episode%10 == 0:\n",
    "            print('.',end='')\n",
    "        if episode > 0 and episode%500 == 0: print(\" \")\n",
    "            \n",
    "        maze = Maze(10, 50)\n",
    "        observation = maze.reset()\n",
    "        observation = np.append(observation, agent.memory)\n",
    "        observation = observation[agent.input_ids]\n",
    "        \n",
    "        rewards = []\n",
    "        \n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "        \n",
    "            action = agent.choose_action(observation)        \n",
    "\n",
    "            observation_, fitness[episode], reward, done = maze.step(action)\n",
    "            observation_ = np.append(observation_, agent.memory)\n",
    "            observation_ = observation_[agent.input_ids]\n",
    "            \n",
    "            rewards.append(reward)\n",
    "\n",
    "            #if (episode == Num_episode-1):\n",
    "            #    print(\"step :\", maze.position, observation, action, reward)\n",
    "            # RL learn from this transition\n",
    "            agent.learn(observation, action, reward, observation_)\n",
    "\n",
    "            # swap observation\n",
    "            observation = observation_\n",
    "            \n",
    "            steps += 1\n",
    "            if done or steps>max_step: break\n",
    "            \n",
    "        average_reward[episode] = np.mean(rewards)\n",
    "\n",
    "    print(\"\\n agent's average reward: %f, best average reward: %f. Rate:%f\" % (np.mean(rewards), maze.average_reward, np.mean(rewards)/maze.average_reward))\n",
    "    print(agent.q_table)\n",
    "    Draw(fitness, constant = maze.best_score)\n",
    "    Draw(average_reward, ylabel = \"Average reward\", label = 'Reward trend', constant =maze.average_reward)\n",
    "    \n",
    "    # end of game\n",
    "    print('game over')\n",
    "    with open(\"./Qsave_model/agent.pickle\",\"wb\") as f:\n",
    "        pickle.dump(agent, f)\n",
    "\n",
    "np.random.seed(0)\n",
    "update([0,1,2,3,4,5,6], [6,7,8])#QAgent.best_input_ids, QAgent.best_output_ids\n",
    "\n",
    "#np.random.seed(9)\n",
    "#import cProfile\n",
    "#cProfile.run('test()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Rollout():\n",
    "\n",
    "    agent = QAgent(QAgent.best_input_ids, QAgent.best_output_ids)\n",
    "    agent.q_table = np.array([[25.09, 25.09, 25.1,  25.09],\n",
    "     [23.16, 24.88, 23.18, 23.19],\n",
    "     [ 0,    0,    0,    0  ],\n",
    "     [ 0,    0,    0,    0  ],\n",
    "     [24.12, 24.28, 25.18, 23.88],\n",
    "     [24.52, 22.8,  22.74, 22.67],\n",
    "     [ 0,    0,    0,    0  ],\n",
    "     [ 0,    0,    0,    0  ],\n",
    "     [25.83, 24.29, 24.32, 24.26],\n",
    "     [22.83, 23.62, 22.84, 22.83],\n",
    "     [ 0,    0,    0,    0  ],\n",
    "     [ 0,    0,    0,    0  ],\n",
    "     [25.42, 25.59, 25.31, 25.33],\n",
    "     [ 0,    0,    0,    0  ],\n",
    "     [25.05, 25.04, 25.16, 25.02],\n",
    "     [ 0,    0,    0,    0  ],\n",
    "     [24.99, 25.07, 25.17, 25.04],\n",
    "     [24.92, 23.07, 23.33, 23.12],\n",
    "     [ 0,    0,    0,    0  ],\n",
    "     [ 0,    0,    0,    0  ],\n",
    "     [24.62, 24.42, 24.4,  24.38],\n",
    "     [22.85, 22.64, 22.58, 22.54],\n",
    "     [ 0,    0,    0,    0  ],\n",
    "     [ 0,    0,    0,    0  ],\n",
    "     [24.73, 24.37, 25.15, 24.47],\n",
    "     [22.79, 24.11, 22.75, 22.74],\n",
    "     [ 0,    0,    0,    0  ],\n",
    "     [ 0,    0,    0,    0  ],\n",
    "     [24.02, 24.35, 25.08, 24.21],\n",
    "     [ 0,    0,    0,    0  ],\n",
    "     [24.76, 25.06, 24.76, 24.72],\n",
    "     [ 0,    0,    0,    0  ]])\n",
    "    with open('./Qsave_model/agent.pickle','rb') as f:\n",
    "        agent = pickle.load(f)\n",
    "    \n",
    "    experience_buffer = []\n",
    "\n",
    "    max_step = 300\n",
    "\n",
    "\n",
    "    maze = Maze(10, 50)\n",
    "    maze.print_maze()\n",
    "    observation = maze.reset()\n",
    "    observation = np.append(observation, agent.memory)\n",
    "    observation = observation[agent.input_ids]\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        action = agent.choose_action(observation)        \n",
    "\n",
    "        observation_, fitness, reward, done = maze.step(action)\n",
    "        observation_ = np.append(observation_, agent.memory)\n",
    "        observation_ = observation_[agent.input_ids]\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "        #if (episode == Num_episode-1):\n",
    "        #    print(\"step :\", maze.position, observation, action, reward)\n",
    "        # RL learn from this transition\n",
    "        agent.learn(observation, action, reward, observation_)\n",
    "\n",
    "        # swap observation\n",
    "        observation = observation_\n",
    "\n",
    "        steps += 1\n",
    "        if done or steps>max_step: break\n",
    "            \n",
    "    print(fitness)\n",
    "    print(\"\\n agent's average reward: %f, best average reward: %f. Rate: %f\" % (np.mean(rewards), maze.average_reward, np.mean(rewards)/maze.average_reward))\n",
    "    print(\"\\n agent's steps: %f, best steps: %f. Extra: %f \"% (steps, maze.steps, steps-maze.steps))\n",
    "    print(np.stack(maze.trajectory))\n",
    "    #print(np.stack(rewards))\n",
    "    # end of game\n",
    "    print('game over')\n",
    "\n",
    "Rollout()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
