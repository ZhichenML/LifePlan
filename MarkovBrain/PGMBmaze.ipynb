{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import os, sys\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "np.random.seed(15)\n",
    "tf.random.set_random_seed(15)\n",
    "random.seed(15)\n",
    "\n",
    "np.set_printoptions(precision=2, threshold=np.inf)\n",
    "\n",
    "class Maze(object):\n",
    "    WALL = 2\n",
    "    EMPTY = 8\n",
    "    LEFT = 0\n",
    "    RIGHT = 1 # right or forward\n",
    "    def __init__(self, width, length): \n",
    "        self.length = length\n",
    "        self.width = width\n",
    "        self.maze = np.ones((self.width, self.length)) * Maze.WALL\n",
    "\n",
    "        self.generate_maze()\n",
    "        \n",
    "        #self.maze_mask\n",
    "        #self.shortest_solutions\n",
    "        self.get_shortest_solutions()\n",
    "        \n",
    "        #self.longest_shortest, used to calculate objective value\n",
    "        self.get_longest_shortest_solutions()\n",
    "        \n",
    "        # used to normalize objective value\n",
    "        self.best_score = self.get_attainable_score()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def generate_maze(self):\n",
    "        # generate walls, doors\n",
    "        \n",
    "        spaces = np.random.randint(low=1, high=4, size=self.length)\n",
    "        cum_spaces = np.cumsum(spaces) # leave the first col empty\n",
    " \n",
    "        for ind, val in enumerate(cum_spaces):\n",
    "            if val >= self.length-1:\n",
    "                self.wall_position = cum_spaces[:ind]\n",
    "                break\n",
    "        if self.wall_position[0] > 1:\n",
    "            self.wall_position[0] = 1\n",
    "        if self.wall_position[-1] < self.length-1:\n",
    "            self.wall_position = np.append(self.wall_position, self.length-1)\n",
    "                \n",
    "        self.road_position = np.array([]).astype(np.int)\n",
    "        for ind in np.arange(self.length-1):\n",
    "            if ind not in self.wall_position:\n",
    "                self.road_position = np.append(self.road_position, ind)\n",
    "        \n",
    "        for i in self.road_position:\n",
    "            self.maze[1:-1,i]=Maze.EMPTY\n",
    "        \n",
    "        self.door_position = np.random.randint(low=1, high=self.width-1, size=len(self.wall_position))\n",
    "        #print(self.door_position)\n",
    "    \n",
    "        # get door position\n",
    "        self.door_position = np.zeros(len(self.wall_position), dtype = np.int)\n",
    "        self.door_position[-1] = np.random.randint(low=1, high=self.width-1) #1~self.width-2 available door position\n",
    "        for ind in np.arange(len(self.wall_position)-2, -1, -1):\n",
    "            if self.wall_position[ind] == self.wall_position[ind+1] -1: # two walls together\n",
    "                self.door_position[ind] = self.door_position[ind+1]\n",
    "                \n",
    "            else:\n",
    "                self.door_position[ind] = np.random.randint(low=1, high=self.width-1)\n",
    "        \n",
    "        # Fill door cue\n",
    "        self.maze[ self.door_position[-1], self.wall_position[-1] ] = Maze.RIGHT # default last door due\n",
    "        for i in np.arange(len(self.wall_position)-2, -1, -1):\n",
    "            if self.door_position[i+1] < self.door_position[i]:\n",
    "                self.maze[self.door_position[i], self.wall_position[i]] = Maze.LEFT\n",
    "            else: \n",
    "                self.maze[self.door_position[i], self.wall_position[i]] = Maze.RIGHT\n",
    "                \n",
    "                \n",
    "                \n",
    "       \n",
    "                \n",
    "    def print_maze(self, x=-1, y=-1):\n",
    "        if x>=0 and y>=0:\n",
    "            tmp = self.maze[x,y]\n",
    "            self.maze[x,y] = -1 # position of the agent\n",
    "            \n",
    "        print(\"  \", end=\"\")    \n",
    "        #for i in np.arange(self.length):\n",
    "        #    print('%d ' % i, end='')\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        for j in np.arange(self.width):\n",
    "            print('%d ' % j, end='')\n",
    "            for i in np.arange(self.length):\n",
    "            \n",
    "                if self.maze[j,i]==Maze.WALL: # wall position\n",
    "                    print('H ',end='')\n",
    "                elif self.maze[j,i]==Maze.EMPTY:\n",
    "                    print('  ',end='')# road\n",
    "                elif self.maze[j,i]==-1:\n",
    "                    print('T ',end='')\n",
    "                    self.maze[x,y]= tmp\n",
    "                else:\n",
    "                    print('%d ' % self.maze[j,i], end='')\n",
    "            print('\\n')\n",
    "\n",
    "        \n",
    "    def get_shortest_solutions(self):\n",
    "        # get the shortest length to the end of maze from each layer\n",
    "        \n",
    "        self.maze_mask = np.zeros(self.length, dtype=np.int)\n",
    "        for ind, val in enumerate(self.wall_position):\n",
    "            self.maze_mask[val] = self.door_position[ind]\n",
    "       \n",
    "        self.shortest_solutions = np.zeros(self.length, dtype=np.int)\n",
    "        step = 0\n",
    "        next_wall = self.length-1\n",
    "        for ind in np.arange(self.length-2, -1, -1):\n",
    "            if self.maze_mask[ind] == 0: # road\n",
    "                step += 1\n",
    "                self.shortest_solutions[ind] = step\n",
    "            else: # wall\n",
    "                step += np.abs(self.maze_mask[next_wall] - self.maze_mask[ind])+1 #1 out the door, +diff for vert.\n",
    "                self.shortest_solutions[ind] = step\n",
    "                next_wall = ind\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_distance_escape(self, x, y):\n",
    "        # get the shortest distance to escape from the current position\n",
    "        vertical_distance = 0\n",
    "        if y in self.road_position:\n",
    "            for next_wall_ind in np.arange(y+1, y+4, 1):\n",
    "                if next_wall_ind in self.wall_position: break\n",
    "            vertical_distance = np.abs(self.maze_mask[next_wall_ind] - x)\n",
    "        return self.shortest_solutions[y]+vertical_distance\n",
    "                \n",
    "\n",
    "        \n",
    "    def get_longest_shortest_solutions(self):\n",
    "        # get the shortest length from corner of starting to the end out maze\n",
    "        left = self.get_distance_escape(1,0)\n",
    "        right = self.get_distance_escape(self.width-2,0)\n",
    "        \n",
    "        self.longest_shortest = np.maximum(left, right)+5 # higher than true value\n",
    "    \n",
    "    \n",
    "    def get_attainable_score(self):\n",
    "        position = []\n",
    "        x = self.door_position[0] # in front of the first door\n",
    "        y = 0\n",
    "        score = np.float32(0)\n",
    "        pass_maze = 0\n",
    "        door_signal=self.maze[self.door_position[0], 1]\n",
    "        r=[]\n",
    "        for _ in np.arange(Agent_circular.LIFE-1, -1, -1):\n",
    "            position.append([x,y])\n",
    "            if y != self.length-1:\n",
    "                r.append((self.longest_shortest - self.get_distance_escape(x,y) )/self.longest_shortest + pass_maze)\n",
    "                score += (self.longest_shortest - self.get_distance_escape(x,y) )/self.longest_shortest + pass_maze\n",
    "            if self.maze[x, y+1]!=Maze.WALL: # road\n",
    "                y += 1\n",
    "                if y in self.wall_position:\n",
    "                    door_signal = self.maze[x,y]\n",
    "                if y == self.length-1:\n",
    "                    pass_maze += 1\n",
    "                    y=0\n",
    "            else: # wall\n",
    "                if door_signal == 0 and self.maze[x-1,y]==Maze.WALL: # init location make door signal no more signal\n",
    "                    door_signal = 1\n",
    "                if door_signal == 1 and self.maze[x+1,y]==Maze.WALL:\n",
    "                    door_signal = 0\n",
    "                x += int(door_signal*2-1)\n",
    "        \n",
    "        #print(position)\n",
    "        self.average_reward = np.mean(r)\n",
    "     \n",
    "        return score\n",
    "\n",
    "\n",
    "    \n",
    "#=========================models===========================================================\n",
    "class NeuroGate_control(tf.keras.Model):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.func = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
    "            tf.keras.layers.Dense(units=128, activation=tf.nn.elu),\n",
    "            tf.keras.layers.Dense(units=64, activation=tf.nn.sigmoid),\n",
    "            tf.keras.layers.Dense(units=4)\n",
    "        ])\n",
    "\n",
    "    def call(self, obs):\n",
    "        return self.func(obs)\n",
    "\n",
    "class NeuroGate_memory(tf.keras.Model):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.inputs = np.zeros(input_dim)\n",
    "        self.func = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
    "            tf.keras.layers.Dense(units=128, activation=tf.nn.elu),\n",
    "            tf.keras.layers.Dense(units=64, activation=tf.nn.sigmoid),\n",
    "            tf.keras.layers.Dense(units=2)\n",
    "        ])\n",
    "\n",
    "    def call(self, obs):\n",
    "        return self.func(obs)\n",
    "                   \n",
    "class Agent_circular:\n",
    "    LIFE = 300\n",
    "    num_inputs = 6\n",
    "    num_memory = 1\n",
    "    num_outputs = 2\n",
    "    brain_size = num_inputs + num_memory + num_outputs\n",
    "    \n",
    "    def __init__(self, maze):\n",
    "        \n",
    "        self.maze = maze\n",
    "        self.brain_size = Agent_circular.brain_size\n",
    "        self.brain = np.zeros(self.brain_size)\n",
    "        self.score = np.float32(0)\n",
    "        \n",
    "        self.memory_rate = np.ones(self.brain_size)*0.5\n",
    "        self.control_rate = np.ones(self.brain_size)*0.5\n",
    "        \n",
    "        self.memory_index = [] # np.array([], dtype = np.int)\n",
    "        for ind in np.arange(self.brain_size):\n",
    "            if np.random.binomial(1, self.memory_rate[ind])==1:\n",
    "                self.memory_index.append(ind)\n",
    "                # np.append(self.memory_index, ind)\n",
    "        self.memory_index = np.array(self.memory_index, dtype = np.int)\n",
    "        \n",
    "        self.control_index = []\n",
    "        for ind in np.arange(self.brain_size):\n",
    "            if np.random.binomial(1, self.control_rate[ind]):\n",
    "                self.control_index.append(ind)\n",
    "        self.control_index = np.array(self.control_index, dtype = np.int)    \n",
    "        \n",
    "        self.Gate_memory = NeuroGate_memory(len(self.memory_index))\n",
    "        self.Gate_control = NeuroGate_control(len(self.control_index))\n",
    "    \n",
    "        \n",
    "        self.end = False # reach the end of maze\n",
    "        self.time_step = 0 # +1 for every move\n",
    "        self.thinking_times = 0 # +1 for every step\n",
    "        #self.life = np.maximum(300, 10*self.maze.length)\n",
    "        self.life = Agent_circular.LIFE\n",
    "        self.pass_maze = 0\n",
    "        \n",
    "        #self.position = np.array([self.maze.door_position[0], 0]) # in front of the first door\n",
    "        #self.position = np.array([np.random.choice(np.arange(1,self.maze.width-1)), 0])\n",
    "        self.position = np.array([self.maze.door_position[-1], 0]) # in front of the last door\n",
    "        self.trajectory = np.ones((self.life, 2))*-1\n",
    "        self.trajectory[self.time_step,:] = self.position\n",
    "        \n",
    "        self.door_direction()\n",
    "        self.perception()\n",
    "        \n",
    "\n",
    "        \n",
    "    # reinit when the genome has no changes, used in fitness evaluation\n",
    "    def simple_reinit(self):\n",
    "        \n",
    "        #self.brain[:6] = 0\n",
    "        #self.brain[10:]=0 # keep hidden nodes' state\n",
    "        self.brain = np.zeros(self.brain_size)\n",
    "        self.score = np.float32(0)\n",
    " \n",
    "        self.end = False # reach the end of maze\n",
    "        self.time_step = 0 # +1 for every move\n",
    "        self.thinking_times = 0 # +1 for every step\n",
    "        #self.life = np.maximum(300, 10*self.maze.length)\n",
    "        self.life = Agent_circular.LIFE\n",
    "        self.pass_maze = 0\n",
    "        \n",
    "        #self.position = np.array([np.random.choice(np.arange(1,self.maze.width-1)), 0])\n",
    "        self.position = np.array([self.maze.door_position[-1], 0]) # in front of the last door\n",
    "        self.trajectory = np.ones((self.life, 2))*-1\n",
    "        self.trajectory[self.time_step,:] = self.position\n",
    "        \n",
    "        self.door_direction()\n",
    "        self.perception()\n",
    "\n",
    "          \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    def init_locate(self):\n",
    "        # if the agent reaches the end of maze, pull it back to the origin\n",
    "        \n",
    "        #self.position = np.array([np.random.choice(np.arange(1,self.maze.width-1)), 0])\n",
    "        self.position = np.array([self.maze.door_position[-1], 0]) # in front of the last door\n",
    "        self.end = False\n",
    "    \n",
    "        self.brain[:Agent_circular.num_inputs] = 0 # reset brain\n",
    "        self.brain[Agent_circular.num_inputs+Agent_circular.num_memory:]=0 # keep hidden nodes' state\n",
    "        \n",
    "        self.door_direction()\n",
    "        self.perception()\n",
    "    \n",
    "\n",
    "        \n",
    "    def door_direction(self):\n",
    "        # let the agent know the first door's position\n",
    "        pass\n",
    "        \"\"\"\n",
    "        next_wall = self.maze.wall_position[0] # the first wall\n",
    "        left = self.maze.maze[1:self.position[0], next_wall]\n",
    "        right = self.maze.maze[self.position[0]:self.maze.width-1, next_wall]\n",
    "        \n",
    "        for land in left:\n",
    "            if land != Maze.WALL: \n",
    "                self.brain[3] = 0\n",
    "                break\n",
    "        for land in right:\n",
    "            if land != Maze.WALL: \n",
    "                self.brain[3] = 1\n",
    "                break\n",
    "        \"\"\"\n",
    "                \n",
    "    def perception(self):\n",
    "        x,y = self.position\n",
    "        #print(\"x=%d, y=%d\", (x,y))\n",
    "        # reset agent's input before set new values\n",
    "        #self.brain[0:3] = 0\n",
    "        #self.brain[4:6] =0\n",
    "        self.brain[:Agent_circular.num_inputs]=0\n",
    "        \n",
    "        if self.maze.maze[x,y+1] == Maze.WALL:\n",
    "            self.brain[0]=1\n",
    "        else: self.brain[0]=0\n",
    "        \n",
    "        if self.maze.maze[x-1,y+1] == Maze.WALL:\n",
    "            self.brain[1]=1\n",
    "        else: self.brain[1]=0\n",
    "        \n",
    "        if self.maze.maze[x+1,y+1] == Maze.WALL:\n",
    "            self.brain[2] = 1\n",
    "        else: self.brain[2]=0\n",
    "        \n",
    "        if self.maze.maze[x-1,y] == Maze.WALL:\n",
    "            self.brain[4]=1\n",
    "        else: self.brain[4]=0\n",
    "        \n",
    "        if self.maze.maze[x+1,y] == Maze.WALL:\n",
    "            self.brain[5]=1\n",
    "        else: self.brain[5]=0\n",
    "        \n",
    "        if y in self.maze.wall_position:\n",
    "            self.brain[3] = self.maze.maze[x, y]\n",
    "        \n",
    "\n",
    "            \n",
    "    \n",
    "    def step(self):\n",
    "        x,y = self.position\n",
    "        r = (self.maze.longest_shortest - self.maze.get_distance_escape(x,y))/self.maze.longest_shortest + self.pass_maze\n",
    "        self.score +=  r\n",
    "        #print(\"x=%d, y=%d, escape_distance=%d, score=%f \" % (x,y,agent.maze.get_distance_escape(x,y), agent.score))\n",
    "        #print(\"value=%f \", (agent.maze.longest_shortest - agent.maze.get_distance_escape(x,y))/agent.maze.longest_shortest)\n",
    "        \n",
    "        \n",
    "        fitness = 0\n",
    "        time_step_shot = self.time_step\n",
    "        self.thinking_times = self.thinking_times + 1\n",
    "        # print(\"time_step:%d\" % self.time_step)\n",
    "        # print(\"thinking time: %d\" % self.thinking_times)\n",
    "        if self.thinking_times>self.life-1:# or self.thinking_times >= 3000: \n",
    "            self.end = True\n",
    "            fitness = self.get_fitness()\n",
    "            self.fitness = fitness\n",
    "            \n",
    "        elif self.brain[Agent_circular.num_inputs+Agent_circular.num_memory] == 1 and self.brain[Agent_circular.num_inputs+Agent_circular.num_memory+1] == 0:\n",
    "            if self.maze.maze[x+1,y]==Maze.WALL:\n",
    "                self.brain[Agent_circular.num_inputs+Agent_circular.num_memory] = 0\n",
    "                self.brain[Agent_circular.num_inputs+Agent_circular.num_memory+1] = 1\n",
    "            else:\n",
    "            #if  self.maze.maze[x+1,y] != Maze.WALL:\n",
    "                self.position = x+1, y\n",
    "                self.time_step = self.time_step+1\n",
    "        elif self.brain[Agent_circular.num_inputs+Agent_circular.num_memory] == 0 and self.brain[Agent_circular.num_inputs+Agent_circular.num_memory+1] == 1:\n",
    "            if self.maze.maze[x-1,y] == Maze.WALL:\n",
    "                self.brain[Agent_circular.num_inputs+Agent_circular.num_memory] = 1\n",
    "                self.brain[Agent_circular.num_inputs+Agent_circular.num_memory+1] = 0\n",
    "            else:\n",
    "            #if  self.maze.maze[x-1,y] != Maze.WALL:\n",
    "                self.position = x-1, y\n",
    "                self.time_step = self.time_step+1\n",
    "                \n",
    "        elif self.brain[Agent_circular.num_inputs+Agent_circular.num_memory] == 1 and self.brain[Agent_circular.num_inputs+Agent_circular.num_memory+1] == 1:\n",
    "            if self.maze.maze[x,y+1] != Maze.WALL:\n",
    "                self.position = x,y+1\n",
    "                self.time_step = self.time_step+1\n",
    "            \"\"\"\n",
    "            elif y in self.maze.wall_position: # in a door\n",
    "                self.position = x,y+1\n",
    "                self.time_step = self.time_step+1\n",
    "            elif y+1 in self.maze.wall_position and self.maze.maze[x,y+1]!=2: # before a door\n",
    "                #print('before a door >;<')\n",
    "                self.position = x,y+1\n",
    "                self.time_step = self.time_step+1\n",
    "            \"\"\"\n",
    "            x,y = self.position\n",
    "            if y == self.maze.length-1: # reach the end of the maze\n",
    "                self.pass_maze = self.pass_maze + 1\n",
    "                self.init_locate()\n",
    "            \n",
    "        elif self.brain[Agent_circular.num_inputs+Agent_circular.num_memory] == 0 and self.brain[Agent_circular.num_inputs+Agent_circular.num_memory+1] == 0:\n",
    "            self.position = x,y\n",
    "            self.time_step = self.time_step+1\n",
    "            '''else:\n",
    "                # shouldn't have this\n",
    "                self.brain[10] = 1\n",
    "                self.brain[11] = 0\n",
    "            '''    \n",
    "        '''elif self.brain[10] == 0 and self.brain[11] == 0:\n",
    "            self.brain[10] = 0\n",
    "            self.brain[11] = 1\n",
    "        ''' \n",
    "        \n",
    "        # if the brain's order is legal, keep it\n",
    "        # illegal order is omitted\n",
    "        if self.time_step > time_step_shot:    \n",
    "            self.trajectory[self.time_step,:] = self.position\n",
    "        \n",
    "        return fitness, r\n",
    "    \n",
    "    def get_fitness(self):\n",
    "        \n",
    "        return self.score/self.maze.best_score \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "                \n",
    "    \n",
    "#np.random.seed(9)\n",
    "#import cProfile\n",
    "#cProfile.run('test()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================utils===========================================================\n",
    "def calculate_discout_reward(rewards, gamma):\n",
    "    discounted_reward = []\n",
    "    cumulative_sum = 0\n",
    "    for i, r in enumerate(reversed(rewards)):\n",
    "        cumulative_sum = cumulative_sum*gamma + r\n",
    "        discounted_reward.append(cumulative_sum)\n",
    "    return discounted_reward[::-1]\n",
    "\n",
    "def calculate_discout_reward_window(reward, gamma, length=3):\n",
    "    target_discount_reward = []\n",
    "    convolution_filter = [gamma**i for i in range(length)]\n",
    "    return np.convolve(reward, convolution_filter, 'valid')\n",
    "\n",
    "\n",
    "\n",
    "#=========================Agent===========================================================\n",
    "def collect_experience(env, agent, number_action=8):\n",
    "\n",
    "    observations, rewards, is_not_done = [], [], []\n",
    "    action_probs = []\n",
    "    action_taken_for_memory, action_taken_for_control = [],[]\n",
    "    \n",
    "    agent.perception()\n",
    "    obs = np.copy(agent.brain)\n",
    "    \n",
    "    while (agent.end == False):        \n",
    "        \n",
    "        obs_for_memory = obs[agent.memory_index]\n",
    "        out_for_memory = agent.Gate_memory(\n",
    "            tf.expand_dims(\n",
    "                tf.convert_to_tensor(obs_for_memory, dtype=tf.float32), axis=0\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if isinstance(out_for_memory, tuple):\n",
    "            prob_for_memory, _ = out_for_memory\n",
    "        else:\n",
    "            prob_for_memory = out_for_memory\n",
    "\n",
    "        action_prob_for_memory = tf.nn.softmax(prob_for_memory)\n",
    "\n",
    "        # Sample\n",
    "        samples_for_memory = tf.random.multinomial(tf.log(action_prob_for_memory), 1).numpy()[0][0]\n",
    "\n",
    "        action_for_memory = np.zeros(2)\n",
    "        action_for_memory[samples_for_memory] = 1\n",
    "\n",
    "\n",
    "        sample_str = bin(samples_for_memory)\n",
    "        sample = np.zeros(1, dtype=np.int)\n",
    "        for i, v in enumerate(sample_str[2:]):\n",
    "            sample[i]=int(v)\n",
    "        agent.brain[6] = sample\n",
    "        \n",
    "        #==============================================\n",
    "        \n",
    "        obs_for_control = obs[agent.control_index]\n",
    "        out_for_control = agent.Gate_control(\n",
    "            tf.expand_dims(\n",
    "                tf.convert_to_tensor(obs_for_control, dtype=tf.float32), axis=0\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if isinstance(out_for_control, tuple):\n",
    "            prob_for_control, _ = out_for_control\n",
    "        else:\n",
    "            prob_for_control = out_for_control\n",
    "\n",
    "        action_prob_for_control = tf.nn.softmax(prob_for_control)\n",
    "\n",
    "        # Sample\n",
    "        samples_for_control = tf.random.multinomial(tf.log(action_prob_for_control), 1).numpy()[0][0]\n",
    "\n",
    "        action_for_control = np.zeros(4)\n",
    "        action_for_control[samples_for_control] = 1\n",
    "\n",
    "\n",
    "        sample_str = bin(samples_for_control)\n",
    "        sample = np.zeros(2, dtype=np.int)\n",
    "        for i, v in enumerate(sample_str[2:]):\n",
    "            sample[i]=int(v)\n",
    "        agent.brain[7:] = sample\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        _,r = agent.step()\n",
    "        agent.perception()\n",
    "        next_obs = np.copy(agent.brain)\n",
    "\n",
    "        observations.append(obs)\n",
    "        rewards.append(r)\n",
    "        action_taken_for_memory.append(action_for_memory)\n",
    "        action_taken_for_control.append(action_for_control)\n",
    "\n",
    "\n",
    "        obs = next_obs\n",
    "            \n",
    "    observations.append(obs)\n",
    "\n",
    "    return np.stack(observations), np.stack(rewards), np.stack(action_taken_for_memory), np.stack(action_taken_for_control)\n",
    "\n",
    "            \n",
    "            \n",
    "#=========================main===========================================================\n",
    "logger = logging.getLogger(os.path.basename(sys.argv[0]))\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    # -------------------- * --------------------\n",
    "    argparser = argparse.ArgumentParser('PG', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    argparser.add_argument('--gamma', action=\"store\", type=float, default=0.99)\n",
    "    argparser.add_argument('--learning-rate', action=\"store\", type=float, default=8e-4)\n",
    "    argparser.add_argument('--episode-train', action=\"store\", type=int, default=500)\n",
    "\n",
    "    argparser.add_argument('--output-save-img', action=\"store\", type=str, default=None)\n",
    "\n",
    "    args = argparser.parse_args(argv)\n",
    "    gamma = args.gamma\n",
    "    learning_rate = args.learning_rate\n",
    "    episode_train = args.episode_train\n",
    "    output_save_img = args.output_save_img\n",
    "    # -------------------- * --------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    gamma =  0.99\n",
    "    episode_train = 50\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    maze = Maze(10,50)\n",
    "    agent = Agent_circular(maze)\n",
    "    \n",
    "    \"\"\"checkpoint_directory = \"./tfmodel\"\n",
    "    checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt-1\")\n",
    "    checkpoint = tf.train.Checkpoint(Gate=agent.Gate)   \n",
    "    checkpoint.restore(checkpoint_prefix)\"\"\"\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    average_reward, reward_curve = 0, []\n",
    "    \n",
    "    for sample_rate_update_ind in range(10):\n",
    "        print(\"episode \", sample_rate_update_ind)\n",
    "    #===================sample N=5 times for updating nodes rates========================================\n",
    "        nodes_for_memory, nodes_for_control, rewards = [], [], []\n",
    "        # repeat to get stable gradients for node rates\n",
    "        for sample_rate_ind in range(5):\n",
    "        #training an agent \n",
    "            for rpt in range(5): # sample number of mazes\n",
    "                maze = Maze(10,50)\n",
    "                agent.maze = maze\n",
    "                print(agent.maze.average_reward)\n",
    "                for eps in range(episode_train): # train on a maze\n",
    "\n",
    "                    agent.simple_reinit()\n",
    "                    observation_rollout, reward_rollout, action_rollout_for_memory, action_rollout_for_control = collect_experience(agent.maze, agent)\n",
    "                    discounted_reward_rollout = calculate_discout_reward(reward_rollout, gamma)\n",
    "\n",
    "                    # Remove last observation\n",
    "                    observation_rollout = observation_rollout[:-1]\n",
    "\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        tape.watch(agent.Gate_memory.variables)\n",
    "\n",
    "                        all_prob = tf.log(tf.nn.softmax(agent.Gate_memory(\n",
    "                            tf.convert_to_tensor(observation_rollout[:,agent.memory_index], dtype=tf.float32)\n",
    "                        )))\n",
    "\n",
    "                        all_prob_masked = tf.reduce_sum(action_rollout_for_memory *  all_prob, axis=-1)\n",
    "                        loss = tf.reduce_sum(all_prob_masked * discounted_reward_rollout * -1)\n",
    "                    grad = tape.gradient(loss, agent.Gate_memory.variables)\n",
    "                    optimizer.apply_gradients(zip(grad, agent.Gate_memory.variables))\n",
    "\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        tape.watch(agent.Gate_control.variables)\n",
    "\n",
    "                        all_prob = tf.log(tf.nn.softmax(agent.Gate_control(\n",
    "                            tf.convert_to_tensor(observation_rollout[:, agent.control_index], dtype=tf.float32)\n",
    "                        )))\n",
    "\n",
    "                        all_prob_masked = tf.reduce_sum(action_rollout_for_control *  all_prob, axis=-1)\n",
    "                        loss_c = tf.reduce_sum(all_prob_masked * discounted_reward_rollout * -1)\n",
    "                    grad_c = tape.gradient(loss_c, agent.Gate_control.variables)\n",
    "                    optimizer.apply_gradients(zip(grad_c, agent.Gate_control.variables))\n",
    "\n",
    "\n",
    "                    average_reward += np.mean(reward_rollout)\n",
    "\n",
    "                    if eps%5 == 0 and eps > 0:\n",
    "                        print(\"Currently in episode %d and the average reward is %f\" % (eps, average_reward))\n",
    "                        #logger.info(\"Currently in episode {eps} and the average reward is {average_reward}\" )\n",
    "                        reward_curve.append(average_reward)\n",
    "                        average_reward = 0\n",
    "\n",
    "\n",
    "                nodes_for_memory.append(agent.memory_index)\n",
    "                nodes_for_control.append(agent.control_index)\n",
    "                rewards.append(average_reward)\n",
    "\n",
    "            #====================resampling the input nodes=========================================================\n",
    "            agent.memory_index = [] # np.array([], dtype = np.int)\n",
    "            for ind in np.arange(agent.brain_size):\n",
    "                if np.random.binomial(1, agent.memory_rate[ind]):\n",
    "                    agent.memory_index.append(ind)\n",
    "                    # np.append(self.memory_index, ind)\n",
    "            agent.memory_index = np.array(agent.memory_index, dtype=np.int)\n",
    "\n",
    "            agent.control_index = []\n",
    "            for ind in np.arange(agent.brain_size):\n",
    "                if np.random.binomial(1, agent.control_rate[ind]):\n",
    "                    agent.control_index.append(ind)\n",
    "            agent.control_index = np.array(agent.control_index, dtype=np.int)    \n",
    "\n",
    "            agent.Gate_memory = NeuroGate_memory(len(agent.memory_index))\n",
    "            agent.Gate_control = NeuroGate_control(len(agent.control_index))\n",
    "\n",
    "        #=========================update nodes sampling rate===============================    \n",
    "        memory_nodes_rewards = np.zeros(agent.brain_size)\n",
    "        control_nodes_rewards = np.zeros(agent.brain_size)\n",
    "        for val, rewards_val in zip(nodes_for_memory, rewards):\n",
    "            for nodes_val in val:\n",
    "                memory_nodes_rewards[nodes_val] += rewards_val\n",
    "        for val, rewards_val in zip(nodes_for_control, rewards):\n",
    "            for nodes_val in val:\n",
    "                control_nodes_rewards[nodes_val] += rewards_val\n",
    "\n",
    "        agent.memory_rate = memory_nodes_rewards/memory_nodes_rewards.sum()\n",
    "        agent.control_rate = control_nodes_rewards/control_nodes_rewards.sum()\n",
    "        \n",
    "        print(agent.memory_rate)\n",
    "        print(agent.control_rate)\n",
    "        \n",
    "        \n",
    "\n",
    "    #sns.lineplot(y=reward_curve, x=list(range(len(reward_curve))))\n",
    "    plt.plot(range(len(reward_curve)), reward_curve)\n",
    "    plt.show()\n",
    "    #plt.savefig(output_save_img)\n",
    "    \n",
    "    checkpoint_directory = \"./tfmodel\"\n",
    "    checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(Gate_memory=agent.Gate_memory, Gate_control = agent.Gate_control)\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    #status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n",
    "\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#print(' '.join(sys.argv))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():        \n",
    "    maze = Maze(10,40)\n",
    "    maze.print_maze()\n",
    " \n",
    "    agent = Agent_circular(maze)                    \n",
    "    checkpoint_directory = \"./tfmodel\"\n",
    "    checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt-1\")\n",
    "    checkpoint = tf.train.Checkpoint(Gate=agent.Gate)   \n",
    "    checkpoint.restore(checkpoint_prefix)\n",
    "\n",
    "  \n",
    "    agent.perception()\n",
    "    obs = agent.brain\n",
    "    while(agent.end == False):\n",
    "        \n",
    "        out = agent.Gate(\n",
    "            tf.expand_dims(\n",
    "                tf.convert_to_tensor(obs, dtype=tf.float32), axis=0\n",
    "            )\n",
    "        )\n",
    "        action_prob = tf.nn.softmax(out)\n",
    "\n",
    "        # Sample\n",
    "        samples = tf.random.multinomial(tf.log(action_prob), 1).numpy()[0][0]\n",
    "\n",
    "        action = np.zeros((64))\n",
    "        action[samples] = 1\n",
    "\n",
    "\n",
    "        sample_str = bin(samples)\n",
    "        sample = np.zeros(3, dtype=np.int)\n",
    "        for i, v in enumerate(sample_str[2:]):\n",
    "            sample[i]=int(v)\n",
    "        agent.brain[6:] = sample\n",
    "        print(agent.brain)\n",
    "        \n",
    "        _,r = agent.step()\n",
    "        agent.perception()\n",
    "        next_obs = np.copy(agent.brain)\n",
    "        \n",
    "        obs = next_obs\n",
    "    \n",
    "    print(agent.fitness)\n",
    "    print(agent.pass_maze)\n",
    "    print(agent.trajectory)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "  \n",
    "\n",
    "#np.random.seed(9)\n",
    "test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
