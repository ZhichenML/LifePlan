{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import os, sys\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "np.random.seed(5)\n",
    "tf.random.set_random_seed(5)\n",
    "random.seed(5)\n",
    "\n",
    "np.set_printoptions(precision=2, threshold=np.inf)\n",
    "\n",
    "class Maze(object):\n",
    "    WALL = 2\n",
    "    EMPTY = 8\n",
    "    LEFT = 0\n",
    "    RIGHT = 1 # right or forward\n",
    "    def __init__(self, width, length): \n",
    "        self.length = length\n",
    "        self.width = width\n",
    "        self.maze = np.ones((self.width, self.length)) * Maze.WALL\n",
    "\n",
    "        self.generate_maze()\n",
    "        \n",
    "        #self.maze_mask\n",
    "        #self.shortest_solutions\n",
    "        self.get_shortest_solutions()\n",
    "        \n",
    "        #self.longest_shortest, used to calculate objective value\n",
    "        self.get_longest_shortest_solutions()\n",
    "        \n",
    "        # used to normalize objective value\n",
    "        self.best_score = self.get_attainable_score()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def generate_maze(self):\n",
    "        # generate walls, doors\n",
    "        \n",
    "        spaces = np.random.randint(low=1, high=4, size=self.length)\n",
    "        cum_spaces = np.cumsum(spaces) # leave the first col empty\n",
    " \n",
    "        for ind, val in enumerate(cum_spaces):\n",
    "            if val >= self.length-1:\n",
    "                self.wall_position = cum_spaces[:ind]\n",
    "                break\n",
    "        if self.wall_position[0] > 1:\n",
    "            self.wall_position[0] = 1\n",
    "        if self.wall_position[-1] < self.length-1:\n",
    "            self.wall_position = np.append(self.wall_position, self.length-1)\n",
    "                \n",
    "        self.road_position = np.array([]).astype(np.int)\n",
    "        for ind in np.arange(self.length-1):\n",
    "            if ind not in self.wall_position:\n",
    "                self.road_position = np.append(self.road_position, ind)\n",
    "        \n",
    "        for i in self.road_position:\n",
    "            self.maze[1:-1,i]=Maze.EMPTY\n",
    "        \n",
    "        self.door_position = np.random.randint(low=1, high=self.width-1, size=len(self.wall_position))\n",
    "        #print(self.door_position)\n",
    "    \n",
    "        # get door position\n",
    "        self.door_position = np.zeros(len(self.wall_position), dtype = np.int)\n",
    "        self.door_position[-1] = np.random.randint(low=1, high=self.width-1) #1~self.width-2 available door position\n",
    "        for ind in np.arange(len(self.wall_position)-2, -1, -1):\n",
    "            if self.wall_position[ind] == self.wall_position[ind+1] -1: # two walls together\n",
    "                self.door_position[ind] = self.door_position[ind+1]\n",
    "                \n",
    "            else:\n",
    "                self.door_position[ind] = np.random.randint(low=1, high=self.width-1)\n",
    "        \n",
    "        # Fill door cue\n",
    "        self.maze[ self.door_position[-1], self.wall_position[-1] ] = Maze.RIGHT # default last door due\n",
    "        for i in np.arange(len(self.wall_position)-2, -1, -1):\n",
    "            if self.door_position[i+1] < self.door_position[i]:\n",
    "                self.maze[self.door_position[i], self.wall_position[i]] = Maze.LEFT\n",
    "            else: \n",
    "                self.maze[self.door_position[i], self.wall_position[i]] = Maze.RIGHT\n",
    "                \n",
    "                \n",
    "                \n",
    "       \n",
    "                \n",
    "    def print_maze(self, x=-1, y=-1):\n",
    "        if x>=0 and y>=0:\n",
    "            tmp = self.maze[x,y]\n",
    "            self.maze[x,y] = -1 # position of the agent\n",
    "            \n",
    "        print(\"  \", end=\"\")    \n",
    "        #for i in np.arange(self.length):\n",
    "        #    print('%d ' % i, end='')\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        for j in np.arange(self.width):\n",
    "            print('%d ' % j, end='')\n",
    "            for i in np.arange(self.length):\n",
    "            \n",
    "                if self.maze[j,i]==Maze.WALL: # wall position\n",
    "                    print('H ',end='')\n",
    "                elif self.maze[j,i]==Maze.EMPTY:\n",
    "                    print('  ',end='')# road\n",
    "                elif self.maze[j,i]==-1:\n",
    "                    print('T ',end='')\n",
    "                    self.maze[x,y]= tmp\n",
    "                else:\n",
    "                    print('%d ' % self.maze[j,i], end='')\n",
    "            print('\\n')\n",
    "\n",
    "        \n",
    "    def get_shortest_solutions(self):\n",
    "        # get the shortest length to the end of maze from each layer\n",
    "        \n",
    "        self.maze_mask = np.zeros(self.length, dtype=np.int)\n",
    "        for ind, val in enumerate(self.wall_position):\n",
    "            self.maze_mask[val] = self.door_position[ind]\n",
    "       \n",
    "        self.shortest_solutions = np.zeros(self.length, dtype=np.int)\n",
    "        step = 0\n",
    "        next_wall = self.length-1\n",
    "        for ind in np.arange(self.length-2, -1, -1):\n",
    "            if self.maze_mask[ind] == 0: # road\n",
    "                step += 1\n",
    "                self.shortest_solutions[ind] = step\n",
    "            else: # wall\n",
    "                step += np.abs(self.maze_mask[next_wall] - self.maze_mask[ind])+1 #1 out the door, +diff for vert.\n",
    "                self.shortest_solutions[ind] = step\n",
    "                next_wall = ind\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_distance_escape(self, x, y):\n",
    "        # get the shortest distance to escape from the current position\n",
    "        vertical_distance = 0\n",
    "        if y in self.road_position:\n",
    "            for next_wall_ind in np.arange(y+1, y+4, 1):\n",
    "                if next_wall_ind in self.wall_position: break\n",
    "            vertical_distance = np.abs(self.maze_mask[next_wall_ind] - x)\n",
    "        return self.shortest_solutions[y]+vertical_distance\n",
    "                \n",
    "\n",
    "        \n",
    "    def get_longest_shortest_solutions(self):\n",
    "        # get the shortest length from corner of starting to the end out maze\n",
    "        left = self.get_distance_escape(1,0)\n",
    "        right = self.get_distance_escape(self.width-2,0)\n",
    "        \n",
    "        self.longest_shortest = np.maximum(left, right)+5 # higher than true value\n",
    "    \n",
    "    \n",
    "    def get_attainable_score(self):\n",
    "        position = []\n",
    "        x = self.door_position[0] # in front of the first door\n",
    "        y = 0\n",
    "        score = np.float32(0)\n",
    "        pass_maze = 0\n",
    "        door_signal=self.maze[self.door_position[0], 1]\n",
    "        for _ in np.arange(Agent_circular.LIFE-1, -1, -1):\n",
    "            position.append([x,y])\n",
    "            if y != self.length-1:\n",
    "                score += (self.longest_shortest - self.get_distance_escape(x,y) )/self.longest_shortest + pass_maze\n",
    "            if self.maze[x, y+1]!=Maze.WALL: # road\n",
    "                y += 1\n",
    "                if y in self.wall_position:\n",
    "                    door_signal = self.maze[x,y]\n",
    "                if y == self.length-1:\n",
    "                    pass_maze += 1\n",
    "                    y=0\n",
    "            else: # wall\n",
    "                if door_signal == 0 and self.maze[x-1,y]==Maze.WALL: # init location make door signal no more signal\n",
    "                    door_signal = 1\n",
    "                if door_signal == 1 and self.maze[x+1,y]==Maze.WALL:\n",
    "                    door_signal = 0\n",
    "                x += int(door_signal*2-1)\n",
    "        \n",
    "        #print(position)\n",
    "     \n",
    "        return score\n",
    "\n",
    "\n",
    "    \n",
    "#=========================models===========================================================\n",
    "class NeuroGate(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.func = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(12,)),\n",
    "            tf.keras.layers.Dense(units=64, activation=tf.nn.sigmoid),\n",
    "            tf.keras.layers.Dense(units=2)\n",
    "        ])\n",
    "\n",
    "    def call(self, obs):\n",
    "        return self.func(obs)\n",
    "                   \n",
    "class Agent_circular:\n",
    "    LIFE = 3000\n",
    "    brain_size=12\n",
    "    def __init__(self, maze):\n",
    "        \n",
    "        self.maze = maze\n",
    "        self.brain_size = Agent_circular.brain_size\n",
    "        self.brain = np.zeros(self.brain_size)\n",
    "        self.score = np.float32(0)\n",
    "        \n",
    "        self.Gate6 = NeuroGate()\n",
    "        self.Gate7 = NeuroGate()\n",
    "        self.Gate8 = NeuroGate()\n",
    "        self.Gate9 = NeuroGate()\n",
    "        self.Gate10 = NeuroGate()\n",
    "        self.Gate11 = NeuroGate()\n",
    "    \n",
    "        \n",
    "        self.end = False # reach the end of maze\n",
    "        self.time_step = 0 # +1 for every move\n",
    "        self.thinking_times = 0 # +1 for every step\n",
    "        #self.life = np.maximum(300, 10*self.maze.length)\n",
    "        self.life = Agent_circular.LIFE\n",
    "        self.pass_maze = 0\n",
    "        \n",
    "        #self.position = np.array([self.maze.door_position[0], 0]) # in front of the first door\n",
    "        #self.position = np.array([np.random.choice(np.arange(1,self.maze.width-1)), 0])\n",
    "        self.position = np.array([self.maze.door_position[-1], 0]) # in front of the last door\n",
    "        self.trajectory = np.ones((self.life, 2))*-1\n",
    "        self.trajectory[self.time_step,:] = self.position\n",
    "        \n",
    "        self.door_direction()\n",
    "        self.perception()\n",
    "        \n",
    "\n",
    "        \n",
    "    # reinit when the genome has no changes, used in fitness evaluation\n",
    "    def simple_reinit(self):\n",
    "        \n",
    "        #self.brain[:6] = 0\n",
    "        #self.brain[10:]=0 # keep hidden nodes' state\n",
    "        self.brain = np.zeros(self.brain_size)\n",
    "        self.score = np.float32(0)\n",
    " \n",
    "        self.end = False # reach the end of maze\n",
    "        self.time_step = 0 # +1 for every move\n",
    "        self.thinking_times = 0 # +1 for every step\n",
    "        #self.life = np.maximum(300, 10*self.maze.length)\n",
    "        self.life = Agent_circular.LIFE\n",
    "        self.pass_maze = 0\n",
    "        \n",
    "        #self.position = np.array([np.random.choice(np.arange(1,self.maze.width-1)), 0])\n",
    "        self.position = np.array([self.maze.door_position[-1], 0]) # in front of the last door\n",
    "        self.trajectory = np.ones((self.life, 2))*-1\n",
    "        self.trajectory[self.time_step,:] = self.position\n",
    "        \n",
    "        self.door_direction()\n",
    "        self.perception()\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    def init_locate(self):\n",
    "        # if the agent reaches the end of maze, pull it back to the origin\n",
    "        \n",
    "        #self.position = np.array([np.random.choice(np.arange(1,self.maze.width-1)), 0])\n",
    "        self.position = np.array([self.maze.door_position[-1], 0]) # in front of the last door\n",
    "        self.end = False\n",
    "    \n",
    "        self.brain[:6] = 0 # reset brain\n",
    "        self.brain[10:]=0 # keep hidden nodes' state\n",
    "        \n",
    "        self.door_direction()\n",
    "        self.perception()\n",
    "    \n",
    "\n",
    "        \n",
    "    def door_direction(self):\n",
    "        # let the agent know the first door's position\n",
    "        pass\n",
    "        \"\"\"\n",
    "        next_wall = self.maze.wall_position[0] # the first wall\n",
    "        left = self.maze.maze[1:self.position[0], next_wall]\n",
    "        right = self.maze.maze[self.position[0]:self.maze.width-1, next_wall]\n",
    "        \n",
    "        for land in left:\n",
    "            if land != Maze.WALL: \n",
    "                self.brain[3] = 0\n",
    "                break\n",
    "        for land in right:\n",
    "            if land != Maze.WALL: \n",
    "                self.brain[3] = 1\n",
    "                break\n",
    "        \"\"\"\n",
    "                \n",
    "    def perception(self):\n",
    "        x,y = self.position\n",
    "        #print(\"x=%d, y=%d\", (x,y))\n",
    "        # reset agent's input before set new values\n",
    "        #self.brain[0:3] = 0\n",
    "        #self.brain[4:6] =0\n",
    "        self.brain[:6]=0\n",
    "        \n",
    "        if self.maze.maze[x,y+1] == Maze.WALL:\n",
    "            self.brain[0]=1\n",
    "        else: self.brain[0]=0\n",
    "        \n",
    "        if self.maze.maze[x-1,y+1] == Maze.WALL:\n",
    "            self.brain[1]=1\n",
    "        else: self.brain[1]=0\n",
    "        \n",
    "        if self.maze.maze[x+1,y+1] == Maze.WALL:\n",
    "            self.brain[2] = 1\n",
    "        else: self.brain[2]=0\n",
    "        \n",
    "        if self.maze.maze[x-1,y] == Maze.WALL:\n",
    "            self.brain[4]=1\n",
    "        else: self.brain[4]=0\n",
    "        \n",
    "        if self.maze.maze[x+1,y] == Maze.WALL:\n",
    "            self.brain[5]=1\n",
    "        else: self.brain[5]=0\n",
    "        \n",
    "        if y in self.maze.wall_position:\n",
    "            self.brain[3] = self.maze.maze[x, y]\n",
    "        \n",
    "\n",
    "            \n",
    "    \n",
    "    def step(self):\n",
    "        x,y = self.position\n",
    "        r = (self.maze.longest_shortest - self.maze.get_distance_escape(x,y))/self.maze.longest_shortest + self.pass_maze\n",
    "        self.score +=  r\n",
    "        #print(\"x=%d, y=%d, escape_distance=%d, score=%f \" % (x,y,agent.maze.get_distance_escape(x,y), agent.score))\n",
    "        #print(\"value=%f \", (agent.maze.longest_shortest - agent.maze.get_distance_escape(x,y))/agent.maze.longest_shortest)\n",
    "        \n",
    "        \n",
    "        fitness = 0\n",
    "        time_step_shot = self.time_step\n",
    "        self.thinking_times = self.thinking_times + 1\n",
    "        # print(\"time_step:%d\" % self.time_step)\n",
    "        # print(\"thinking time: %d\" % self.thinking_times)\n",
    "        if self.thinking_times>self.life-1:# or self.thinking_times >= 3000: \n",
    "            self.end = True\n",
    "            fitness = self.get_fitness()\n",
    "            self.fitness = fitness\n",
    "            \n",
    "        elif self.brain[10] == 1 and self.brain[11] == 0:\n",
    "            #if self.maze.maze[x+1,y]==Maze.WALL:\n",
    "            #    self.brain[10] = 0\n",
    "            #    self.brain[11] = 1\n",
    "            #else:\n",
    "            if  self.maze.maze[x+1,y] != Maze.WALL:\n",
    "                self.position = x+1, y\n",
    "                self.time_step = self.time_step+1\n",
    "        elif self.brain[10] == 0 and self.brain[11] == 1:\n",
    "            #if self.maze.maze[x-1,y] == Maze.WALL:\n",
    "            #    self.brain[10] = 1\n",
    "            #    self.brain[11] = 0\n",
    "            #else:\n",
    "            if  self.maze.maze[x-1,y] != Maze.WALL:\n",
    "                self.position = x-1, y\n",
    "                self.time_step = self.time_step+1\n",
    "                \n",
    "        elif self.brain[10] == 1 and self.brain[11] == 1:\n",
    "            if self.maze.maze[x,y+1] != Maze.WALL:\n",
    "                self.position = x,y+1\n",
    "                self.time_step = self.time_step+1\n",
    "            \"\"\"\n",
    "            elif y in self.maze.wall_position: # in a door\n",
    "                self.position = x,y+1\n",
    "                self.time_step = self.time_step+1\n",
    "            elif y+1 in self.maze.wall_position and self.maze.maze[x,y+1]!=2: # before a door\n",
    "                #print('before a door >;<')\n",
    "                self.position = x,y+1\n",
    "                self.time_step = self.time_step+1\n",
    "            \"\"\"\n",
    "            x,y = self.position\n",
    "            if y == self.maze.length-1: # reach the end of the maze\n",
    "                self.pass_maze = self.pass_maze + 1\n",
    "                self.init_locate()\n",
    "            \n",
    "        elif self.brain[10] == 0 and self.brain[11] == 0:\n",
    "            self.position = x,y\n",
    "            self.time_step = self.time_step+1\n",
    "            '''else:\n",
    "                # shouldn't have this\n",
    "                self.brain[10] = 1\n",
    "                self.brain[11] = 0\n",
    "            '''    \n",
    "        '''elif self.brain[10] == 0 and self.brain[11] == 0:\n",
    "            self.brain[10] = 0\n",
    "            self.brain[11] = 1\n",
    "        ''' \n",
    "        \n",
    "        # if the brain's order is legal, keep it\n",
    "        # illegal order is omitted\n",
    "        if self.time_step > time_step_shot:    \n",
    "            self.trajectory[self.time_step,:] = self.position\n",
    "        \n",
    "        return fitness, r\n",
    "    \n",
    "    def get_fitness(self):\n",
    "        \n",
    "        return self.score/self.maze.best_score \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "                \n",
    "    \n",
    "#np.random.seed(9)\n",
    "#import cProfile\n",
    "#cProfile.run('test()')\n",
    "\n",
    "        \n",
    "                \n",
    "            \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================utils===========================================================\n",
    "def calculate_discout_reward(rewards, gamma):\n",
    "    discounted_reward = []\n",
    "    cumulative_sum = 0\n",
    "    for i, r in enumerate(reversed(rewards)):\n",
    "        cumulative_sum = cumulative_sum*gamma + r\n",
    "        discounted_reward.append(cumulative_sum)\n",
    "    return discounted_reward[::-1]\n",
    "\n",
    "def calculate_discout_reward_window(reward, gamma, length=3):\n",
    "    target_discount_reward = []\n",
    "    convolution_filter = [gamma**i for i in range(length)]\n",
    "    return np.convolve(reward, convolution_filter, 'valid')\n",
    "\n",
    "\n",
    "\n",
    "#=========================Agent===========================================================\n",
    "def collect_experience(env, agent, number_action=2):\n",
    "\n",
    "    observations, rewards, is_not_done = [], [], []\n",
    "    action_taken6, action_taken7, action_taken8, action_taken9,action_taken10, action_taken11=[],[],[],[],[],[]\n",
    "    action_probs = []\n",
    "    \n",
    "    agent.perception()\n",
    "    obs = np.copy(agent.brain)\n",
    "    \n",
    "    while (agent.end == False):        \n",
    "\n",
    "        out6 = agent.Gate6(\n",
    "            tf.expand_dims(\n",
    "                tf.convert_to_tensor(obs, dtype=tf.float32), axis=0\n",
    "            )\n",
    "        )\n",
    "        out7 = agent.Gate7(\n",
    "            tf.expand_dims(\n",
    "                tf.convert_to_tensor(obs, dtype=tf.float32), axis=0\n",
    "            )\n",
    "        )\n",
    "        out8 = agent.Gate8(\n",
    "            tf.expand_dims(\n",
    "                tf.convert_to_tensor(obs, dtype=tf.float32), axis=0\n",
    "            )\n",
    "        )\n",
    "        out9 = agent.Gate9(\n",
    "            tf.expand_dims(\n",
    "                tf.convert_to_tensor(obs, dtype=tf.float32), axis=0\n",
    "            )\n",
    "        )\n",
    "        out10 = agent.Gate10(\n",
    "            tf.expand_dims(\n",
    "                tf.convert_to_tensor(obs, dtype=tf.float32), axis=0\n",
    "            )\n",
    "        )\n",
    "        out11 = agent.Gate11(\n",
    "            tf.expand_dims(\n",
    "                tf.convert_to_tensor(obs, dtype=tf.float32), axis=0\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if isinstance(out6, tuple):\n",
    "            prob6, _ = out6\n",
    "        else:\n",
    "            prob6 = out6\n",
    "        if isinstance(out7, tuple):\n",
    "            prob7, _ = out7\n",
    "        else:\n",
    "            prob7 = out7\n",
    "        if isinstance(out8, tuple):\n",
    "            prob8, _ = out8\n",
    "        else:\n",
    "            prob8 = out8\n",
    "        if isinstance(out9, tuple):\n",
    "            prob9, _ = out9\n",
    "        else:\n",
    "            prob9 = out9\n",
    "        if isinstance(out10, tuple):\n",
    "            prob10, _ = out10\n",
    "        else:\n",
    "            prob10 = out10\n",
    "        if isinstance(out11, tuple):\n",
    "            prob11, _ = out11\n",
    "        else:\n",
    "            prob11 = out11\n",
    "\n",
    "        action_prob6 = tf.nn.softmax(prob6)\n",
    "        action_prob7 = tf.nn.softmax(prob7)\n",
    "        action_prob8 = tf.nn.softmax(prob8)\n",
    "        action_prob9 = tf.nn.softmax(prob9)\n",
    "        action_prob10 = tf.nn.softmax(prob10)\n",
    "        action_prob11 = tf.nn.softmax(prob11)\n",
    "\n",
    "        # Sample\n",
    "        samples6 = tf.random.multinomial(tf.log(action_prob6), 1).numpy()[0][0]\n",
    "        samples7 = tf.random.multinomial(tf.log(action_prob7), 1).numpy()[0][0]\n",
    "        samples8 = tf.random.multinomial(tf.log(action_prob8), 1).numpy()[0][0]\n",
    "        samples9 = tf.random.multinomial(tf.log(action_prob9), 1).numpy()[0][0]\n",
    "        samples10 = tf.random.multinomial(tf.log(action_prob10), 1).numpy()[0][0]\n",
    "        samples11 = tf.random.multinomial(tf.log(action_prob11), 1).numpy()[0][0]\n",
    "        \n",
    "\n",
    "        action6 = np.zeros((number_action))\n",
    "        action6[samples6] = 1\n",
    "        action7 = np.zeros((number_action))\n",
    "        action7[samples7] = 1\n",
    "        action8 = np.zeros((number_action))\n",
    "        action8[samples8] = 1\n",
    "        action9 = np.zeros((number_action))\n",
    "        action9[samples9] = 1\n",
    "        action10 = np.zeros((number_action))\n",
    "        action10[samples10] = 1\n",
    "        action11 = np.zeros((number_action))\n",
    "        action11[samples11] = 1\n",
    "\n",
    "\n",
    "        \"\"\"sample_str = bin(samples)\n",
    "        sample = np.zeros(6, dtype=np.int)\n",
    "        for i, v in enumerate(sample_str[2:]):\n",
    "            sample[i]=int(v)\n",
    "        agent.brain[6:] = sample\"\"\"\n",
    "        agent.brain[6] = samples6\n",
    "        agent.brain[7] = samples7\n",
    "        agent.brain[8] = samples8\n",
    "        agent.brain[9] = samples9\n",
    "        agent.brain[10] = samples10\n",
    "        agent.brain[11] = samples11\n",
    "\n",
    "        _,r = agent.step()\n",
    "        agent.perception()\n",
    "        next_obs = np.copy(agent.brain)\n",
    "\n",
    "        observations.append(obs)\n",
    "        rewards.append(r)\n",
    "        action_taken6.append(action6)\n",
    "        action_taken7.append(action7)\n",
    "        action_taken8.append(action8)\n",
    "        action_taken9.append(action9)\n",
    "        action_taken10.append(action10)\n",
    "        action_taken11.append(action11)\n",
    "\n",
    "\n",
    "        obs = next_obs\n",
    "            \n",
    "    observations.append(obs)\n",
    "\n",
    "    return np.stack(observations), np.stack(rewards), np.stack(action_taken6), np.stack(action_taken7), np.stack(action_taken8), np.stack(action_taken9), np.stack(action_taken10), np.stack(action_taken11)\n",
    "            \n",
    "\n",
    "           \n",
    "#=========================main===========================================================\n",
    "logger = logging.getLogger(os.path.basename(sys.argv[0]))\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    # -------------------- * --------------------\n",
    "    argparser = argparse.ArgumentParser('PG', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    argparser.add_argument('--gamma', action=\"store\", type=float, default=0.99)\n",
    "    argparser.add_argument('--learning-rate', action=\"store\", type=float, default=8e-4)\n",
    "    argparser.add_argument('--episode-train', action=\"store\", type=int, default=500)\n",
    "\n",
    "    argparser.add_argument('--output-save-img', action=\"store\", type=str, default=None)\n",
    "\n",
    "    args = argparser.parse_args(argv)\n",
    "    gamma = args.gamma\n",
    "    learning_rate = args.learning_rate\n",
    "    episode_train = args.episode_train\n",
    "    output_save_img = args.output_save_img\n",
    "    # -------------------- * --------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    gamma =  1\n",
    "    episode_train = 50\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "    maze = Maze(10,50)\n",
    "    agent = Agent_circular(maze)\n",
    "    \n",
    "    \"\"\"checkpoint_directory = \"./tfmodel\"\n",
    "    checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt-1\")\n",
    "    checkpoint = tf.train.Checkpoint(Gate=agent.Gate)   \n",
    "    checkpoint.restore(checkpoint_prefix)\"\"\"\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    average_reward, reward_curve = 0, []\n",
    "    for rpt in range(10):\n",
    "        maze = Maze(10,50)\n",
    "        agent.maze = maze\n",
    "        for eps in range(episode_train):\n",
    "\n",
    "            agent.simple_reinit()\n",
    "            observation_rollout, reward_rollout, action_rollout6, action_rollout7, action_rollout8, action_rollout9, action_rollout10, action_rollout11 = collect_experience(agent.maze, agent)\n",
    "            discounted_reward_rollout = calculate_discout_reward(reward_rollout, gamma)\n",
    "\n",
    "            # Remove last observation\n",
    "            observation_rollout = observation_rollout[:-1]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(agent.Gate6.variables)\n",
    "                all_prob6 = tf.log(tf.nn.softmax(agent.Gate6(\n",
    "                    tf.convert_to_tensor(observation_rollout, dtype=tf.float32)\n",
    "                )))\n",
    "                \n",
    "                all_prob_masked6 = tf.reduce_sum(action_rollout6 *  all_prob6, axis=-1)\n",
    "                loss6 = tf.reduce_sum(all_prob_masked6 * discounted_reward_rollout * -1)\n",
    "            grad6 = tape.gradient(loss6, agent.Gate6.variables)\n",
    "            optimizer.apply_gradients(zip(grad6, agent.Gate6.variables))\n",
    "                \n",
    "                \n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(agent.Gate7.variables)\n",
    "                all_prob7 = tf.log(tf.nn.softmax(agent.Gate7(\n",
    "                    tf.convert_to_tensor(observation_rollout, dtype=tf.float32)\n",
    "                )))\n",
    "                \n",
    "                all_prob_masked7 = tf.reduce_sum(action_rollout7 *  all_prob7, axis=-1)\n",
    "                loss7 = tf.reduce_sum(all_prob_masked7 * discounted_reward_rollout * -1)\n",
    "                \n",
    "            grad7 = tape.gradient(loss7, agent.Gate7.variables)\n",
    "            optimizer.apply_gradients(zip(grad7, agent.Gate7.variables))\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(agent.Gate8.variables)\n",
    "                all_prob8 = tf.log(tf.nn.softmax(agent.Gate8(\n",
    "                    tf.convert_to_tensor(observation_rollout, dtype=tf.float32)\n",
    "                )))\n",
    "                all_prob_masked8 = tf.reduce_sum(action_rollout8 *  all_prob8, axis=-1)\n",
    "                loss8 = tf.reduce_sum(all_prob_masked8 * discounted_reward_rollout * -1)\n",
    "                \n",
    "            grad8 = tape.gradient(loss8, agent.Gate8.variables)\n",
    "            optimizer.apply_gradients(zip(grad8, agent.Gate8.variables))\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(agent.Gate9.variables)\n",
    "                all_prob9 = tf.log(tf.nn.softmax(agent.Gate9(\n",
    "                    tf.convert_to_tensor(observation_rollout, dtype=tf.float32)\n",
    "                )))\n",
    "                all_prob_masked9 = tf.reduce_sum(action_rollout9 *  all_prob9, axis=-1)\n",
    "                loss9 = tf.reduce_sum(all_prob_masked9 * discounted_reward_rollout * -1)\n",
    "                \n",
    "            grad9 = tape.gradient(loss9, agent.Gate9.variables)\n",
    "            optimizer.apply_gradients(zip(grad9, agent.Gate9.variables))\n",
    "                \n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(agent.Gate10.variables)\n",
    "                all_prob10 = tf.log(tf.nn.softmax(agent.Gate10(\n",
    "                    tf.convert_to_tensor(observation_rollout, dtype=tf.float32)\n",
    "                )))\n",
    "                all_prob_masked10 = tf.reduce_sum(action_rollout10 *  all_prob10, axis=-1)\n",
    "                loss10 = tf.reduce_sum(all_prob_masked10 * discounted_reward_rollout * -1)\n",
    "                \n",
    "            grad10 = tape.gradient(loss10, agent.Gate10.variables)\n",
    "            optimizer.apply_gradients(zip(grad10, agent.Gate10.variables))\n",
    "                \n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(agent.Gate11.variables)\n",
    "                all_prob11 = tf.log(tf.nn.softmax(agent.Gate11(\n",
    "                    tf.convert_to_tensor(observation_rollout, dtype=tf.float32)\n",
    "                )))\n",
    "                all_prob_masked11 = tf.reduce_sum(action_rollout11 *  all_prob11, axis=-1)\n",
    "                loss11 = tf.reduce_sum(all_prob_masked11 * discounted_reward_rollout * -1)\n",
    "\n",
    "            grad11 = tape.gradient(loss11, agent.Gate11.variables)\n",
    "            optimizer.apply_gradients(zip(grad11, agent.Gate11.variables))\n",
    "\n",
    "            average_reward += np.mean(reward_rollout)\n",
    "\n",
    "            if eps%5 == 0 and eps > 0:\n",
    "                print(\"Currently in episode %d and the average reward is %f\" % (eps, average_reward))\n",
    "                #logger.info(\"Currently in episode {eps} and the average reward is {average_reward}\" )\n",
    "                reward_curve.append(average_reward)\n",
    "                average_reward = 0\n",
    "\n",
    "    #sns.lineplot(y=reward_curve, x=list(range(len(reward_curve))))\n",
    "    plt.plot(range(len(reward_curve)), reward_curve)\n",
    "    plt.show()\n",
    "    #plt.savefig(output_save_img)\n",
    "    \n",
    "    checkpoint_directory = \"./tfmodel\"\n",
    "    checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(Gate=agent.Gate)\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    #status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n",
    "\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#print(' '.join(sys.argv))\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():        \n",
    "    maze = Maze(7,10)\n",
    "    maze.print_maze()\n",
    " \n",
    "    agent = Agent_circular(maze)                    \n",
    "    checkpoint_directory = \"./tfmodel\"\n",
    "    checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt-1\")\n",
    "    checkpoint = tf.train.Checkpoint(agent=agent)   \n",
    "    checkpoint.restore(checkpoint_prefix)\n",
    "\n",
    "  \n",
    "    agent.perception()\n",
    "    obs = agent.brain\n",
    "    while(agent.end == False):\n",
    "        \n",
    "        out = agent.Gate(\n",
    "            tf.expand_dims(\n",
    "                tf.convert_to_tensor(obs, dtype=tf.float32), axis=0\n",
    "            )\n",
    "        )\n",
    "        action_prob = tf.nn.softmax(out)\n",
    "\n",
    "        # Sample\n",
    "        samples = tf.random.multinomial(tf.log(action_prob), 1).numpy()[0][0]\n",
    "\n",
    "        action = np.zeros((64))\n",
    "        action[samples] = 1\n",
    "\n",
    "\n",
    "        sample_str = bin(samples)\n",
    "        sample = np.zeros(6, dtype=np.int)\n",
    "        for i, v in enumerate(sample_str[2:]):\n",
    "            sample[i]=int(v)\n",
    "        agent.brain[6:] = sample\n",
    "        print(agent.brain)\n",
    "        \n",
    "        _,r = agent.step()\n",
    "        agent.perception()\n",
    "        next_obs = np.copy(agent.brain)\n",
    "        \n",
    "        obs = next_obs\n",
    "        \n",
    "    print(agent.trajectory)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "  \n",
    "\n",
    "#np.random.seed(9)\n",
    "test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
