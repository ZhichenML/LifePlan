{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    LIFE = 300\n",
    "    num_inputs = 6\n",
    "    num_memory = 1\n",
    "    num_outputs = 2\n",
    "    brain_size = num_inputs + num_memory + num_outputs\n",
    "    \n",
    "    def __init__(self, maze):\n",
    "        \n",
    "        self.maze = maze\n",
    "        self.brain_size = Agent.brain_size\n",
    "        self.brain = np.zeros(self.brain_size)\n",
    "        self.score = np.float32(0)\n",
    "        \n",
    "        self.memory_rate = np.ones(self.brain_size)*0.5\n",
    "        self.control_rate = np.ones(self.brain_size)*0.5\n",
    "        \n",
    "        \"\"\"\n",
    "        self.memory_index = [] # np.array([], dtype = np.int)\n",
    "        for ind in np.arange(self.brain_size):\n",
    "            if np.random.binomial(1, self.memory_rate[ind])==1:\n",
    "                self.memory_index.append(ind)\n",
    "                # np.append(self.memory_index, ind)\n",
    "        self.memory_index = np.array(self.memory_index, dtype = np.int)\n",
    "        \n",
    "        self.control_index = []\n",
    "        for ind in np.arange(self.brain_size):\n",
    "            if np.random.binomial(1, self.control_rate[ind]):\n",
    "                self.control_index.append(ind)\n",
    "        self.control_index = np.array(self.control_index, dtype = np.int)    \n",
    "        \"\"\"\n",
    "        self.memory_index = np.array([3, 6], dtype = np.int)\n",
    "        self.control_index = np.array([0,1,2,4,5,6,7,8], dtype = np.int)\n",
    "\n",
    "        \n",
    "        self.Gate_memory = np.random.rand(2**len(self.memory_index), 2)\n",
    "        #self.Gate_memory = np.array([[1,0],[0,1]])\n",
    "        self.Gate_memory = self.Gate_memory.astype(np.float64) / np.sum(self.Gate_memory, axis=1, dtype=np.float64)[:, None]\n",
    "        \n",
    "        \n",
    "        self.Gate_control = np.random.rand(2**len(self.control_index), 4)\n",
    "        self.Gate_control = self.Gate_control.astype(np.float64) / np.sum(self.Gate_control, axis=1, dtype=np.float64)[:, None]\n",
    "                    \n",
    "    \n",
    "        \n",
    "        self.end = False # reach the end of maze\n",
    "        self.time_step = 0 # +1 for every move\n",
    "        self.thinking_times = 0 # +1 for every step\n",
    "        #self.life = np.maximum(300, 10*self.maze.length)\n",
    "        self.life = Agent.LIFE\n",
    "        self.pass_maze = 0\n",
    "        \n",
    "        #self.position = np.array([self.maze.door_position[0], 0]) # in front of the first door\n",
    "        #self.position = np.array([np.random.choice(np.arange(1,self.maze.width-1)), 0])\n",
    "        self.position = np.array([self.maze.door_position[-1], 0]) # in front of the last door\n",
    "        self.trajectory = np.ones((self.life, 2))*-1\n",
    "        self.trajectory[self.time_step,:] = self.position\n",
    "        \n",
    "        self.door_direction()\n",
    "        self.perception()\n",
    "  \n",
    "        \n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "    def brain_update(self):\n",
    "        self.perception()\n",
    "        \n",
    "        next_brain = np.copy(self.brain)\n",
    "        next_brain[6:] = 0\n",
    "\n",
    "        if np.random.rand()>0.2:\n",
    "            mg_input_index, marker = 0, 1\n",
    "            # Create an integer from bytes representation (loop is faster than previous implementation)\n",
    "            for mg_input_id in reversed(self.memory_index):\n",
    "                if self.brain[mg_input_id]:\n",
    "                    mg_input_index += marker\n",
    "                marker *= 2\n",
    "\n",
    "            # Determine the corresponding output values for this Markov Gate\n",
    "            markov_gate_subarray = self.Gate_memory[mg_input_index,:]  # selects a Markov Gate subarray\n",
    "\n",
    "            mg_output_index = np.random.choice(len(markov_gate_subarray),p = markov_gate_subarray)\n",
    "            # Converts the index into a string of '1's and '0's (binary representation)\n",
    "            mg_output_values = bin(mg_output_index)  # bin() is much faster than np.binaryrepr()\n",
    "\n",
    "            # Loops through 'mg_output_values' and alter 'self.states'\n",
    "            for i, mg_output_value in enumerate(mg_output_values[2:]):\n",
    "                if mg_output_value == '1':\n",
    "                    next_brain[6] = 1   #.astype(np.int32)\n",
    "\n",
    "\n",
    "            #====\n",
    "            mg_input_index, marker = 0, 1\n",
    "            # Create an integer from bytes representation (loop is faster than previous implementation)\n",
    "            for mg_input_id in reversed(self.control_index):\n",
    "                if self.brain[mg_input_id]:\n",
    "                    mg_input_index += marker\n",
    "                marker *= 2\n",
    "\n",
    "            # Determine the corresponding output values for this Markov Gate\n",
    "            markov_gate_subarray = self.Gate_control[mg_input_index,:]  # selects a Markov Gate subarray\n",
    "\n",
    "            mg_output_index = np.random.choice(len(markov_gate_subarray),p = markov_gate_subarray)\n",
    "            # Converts the index into a string of '1's and '0's (binary representation)\n",
    "            mg_output_values = bin(mg_output_index)  # bin() is much faster than np.binaryrepr()\n",
    "\n",
    "            # Loops through 'mg_output_values' and alter 'self.states'\n",
    "            for i, mg_output_value in enumerate(mg_output_values[2:]):\n",
    "                if mg_output_value == '1':\n",
    "                    next_brain[7+i] = 1   #.astype(np.int32)\n",
    "        else:\n",
    "            next_brain[6:] = np.random.randint(2,size=3)\n",
    "\n",
    "        # Replace original input values\n",
    "        self.brain = np.copy(next_brain)\n",
    "\n",
    "\n",
    "                \n",
    "        \n",
    "    # reinit when the genome has no changes, used in fitness evaluation\n",
    "    def simple_reinit(self):\n",
    "        \n",
    "        #self.brain[:6] = 0\n",
    "        #self.brain[10:]=0 # keep hidden nodes' state\n",
    "        self.brain = np.zeros(self.brain_size)\n",
    "        self.score = np.float32(0)\n",
    " \n",
    "        self.end = False # reach the end of maze\n",
    "        self.time_step = 0 # +1 for every move\n",
    "        self.thinking_times = 0 # +1 for every step\n",
    "        #self.life = np.maximum(300, 10*self.maze.length)\n",
    "        self.life = Agent.LIFE\n",
    "        self.pass_maze = 0\n",
    "        \n",
    "        #self.position = np.array([np.random.choice(np.arange(1,self.maze.width-1)), 0])\n",
    "        self.position = np.array([self.maze.door_position[-1], 0]) # in front of the last door\n",
    "        self.trajectory = np.ones((self.life, 2))*-1\n",
    "        self.trajectory[self.time_step,:] = self.position\n",
    "        \n",
    "        self.door_direction()\n",
    "        self.perception()\n",
    "\n",
    "          \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    def init_locate(self):\n",
    "        # if the agent reaches the end of maze, pull it back to the origin\n",
    "        \n",
    "        #self.position = np.array([np.random.choice(np.arange(1,self.maze.width-1)), 0])\n",
    "        self.position = np.array([self.maze.door_position[-1], 0]) # in front of the last door\n",
    "        self.end = False\n",
    "    \n",
    "        self.brain[:Agent.num_inputs] = 0 # reset brain\n",
    "        self.brain[Agent.num_inputs+Agent.num_memory:]=0 # keep hidden nodes' state\n",
    "        \n",
    "        self.door_direction()\n",
    "        self.perception()\n",
    "    \n",
    "\n",
    "        \n",
    "    def door_direction(self):\n",
    "        # let the agent know the first door's position\n",
    "        pass\n",
    "        \"\"\"\n",
    "        next_wall = self.maze.wall_position[0] # the first wall\n",
    "        left = self.maze.maze[1:self.position[0], next_wall]\n",
    "        right = self.maze.maze[self.position[0]:self.maze.width-1, next_wall]\n",
    "        \n",
    "        for land in left:\n",
    "            if land != Maze.WALL: \n",
    "                self.brain[3] = 0\n",
    "                break\n",
    "        for land in right:\n",
    "            if land != Maze.WALL: \n",
    "                self.brain[3] = 1\n",
    "                break\n",
    "        \"\"\"\n",
    "                \n",
    "    def perception(self):\n",
    "        x,y = self.position\n",
    "        #print(\"x=%d, y=%d\", (x,y))\n",
    "        # reset agent's input before set new values\n",
    "        #self.brain[0:3] = 0\n",
    "        #self.brain[4:6] =0\n",
    "        self.brain[:Agent.num_inputs]=0\n",
    "        \n",
    "        if self.maze.maze[x,y+1] == Maze.WALL:\n",
    "            self.brain[0]=1\n",
    "        else: self.brain[0]=0\n",
    "        \n",
    "        if self.maze.maze[x-1,y+1] == Maze.WALL:\n",
    "            self.brain[1]=1\n",
    "        else: self.brain[1]=0\n",
    "        \n",
    "        if self.maze.maze[x+1,y+1] == Maze.WALL:\n",
    "            self.brain[2] = 1\n",
    "        else: self.brain[2]=0\n",
    "        \n",
    "        if self.maze.maze[x-1,y] == Maze.WALL:\n",
    "            self.brain[4]=1\n",
    "        else: self.brain[4]=0\n",
    "        \n",
    "        if self.maze.maze[x+1,y] == Maze.WALL:\n",
    "            self.brain[5]=1\n",
    "        else: self.brain[5]=0\n",
    "        \n",
    "        if y in self.maze.wall_position:\n",
    "            self.brain[3] = self.maze.maze[x, y]\n",
    "        \n",
    "    def random_walk(self, x, y):\n",
    "        feasible = []\n",
    "\n",
    "        if self.maze.maze[x-1, y] != Maze.WALL:\n",
    "            feasible.append([x-1, y])\n",
    "        if self.maze.maze[x+1, y] != Maze.WALL:\n",
    "            feasible.append([x+1, y])\n",
    "        if self.maze.maze[x, y+1] != Maze.WALL:\n",
    "            feasible.append([x, y+1])\n",
    "\n",
    "        if len(feasible)>0:\n",
    "            idx = np.random.randint(len(feasible))\n",
    "            return feasible[idx]\n",
    "        else:\n",
    "            return [x,y]\n",
    "        \n",
    "    \n",
    "            \n",
    "    \n",
    "    def step(self):\n",
    "        x,y = self.position\n",
    "        r = (self.maze.longest_shortest - self.maze.get_distance_escape(x,y))/self.maze.longest_shortest + self.pass_maze\n",
    "        self.score +=  r\n",
    "        #print(\"x=%d, y=%d, escape_distance=%d, score=%f \" % (x,y,agent.maze.get_distance_escape(x,y), agent.score))\n",
    "        #print(\"value=%f \", (agent.maze.longest_shortest - agent.maze.get_distance_escape(x,y))/agent.maze.longest_shortest)\n",
    "        \n",
    "        \n",
    "        fitness = 0\n",
    "        time_step_shot = self.time_step\n",
    "        self.thinking_times = self.thinking_times + 1\n",
    "        # print(\"time_step:%d\" % self.time_step)\n",
    "        # print(\"thinking time: %d\" % self.thinking_times)\n",
    "        \n",
    "        if self.thinking_times>self.life-1:# or self.thinking_times >= 3000: \n",
    "            self.end = True\n",
    "            fitness = self.get_fitness()\n",
    "            self.fitness = fitness\n",
    "     \n",
    "        #if np.random.rand()>0.5:\n",
    "\n",
    "        if self.brain[Agent.num_inputs+Agent.num_memory] == 1 and self.brain[Agent.num_inputs+Agent.num_memory+1] == 0:\n",
    "            #if self.maze.maze[x+1,y]==Maze.WALL:\n",
    "            #    self.brain[Agent.num_inputs+Agent.num_memory] = 0\n",
    "            #    self.brain[Agent.num_inputs+Agent.num_memory+1] = 1\n",
    "            #else:\n",
    "\n",
    "            if  self.maze.maze[x+1,y] != Maze.WALL:\n",
    "                self.position = x+1, y\n",
    "                self.time_step = self.time_step+1\n",
    "\n",
    "        elif self.brain[Agent.num_inputs+Agent.num_memory] == 0 and self.brain[Agent.num_inputs+Agent.num_memory+1] == 1:\n",
    "            #if self.maze.maze[x-1,y] == Maze.WALL:\n",
    "            #    self.brain[Agent.num_inputs+Agent.num_memory] = 1\n",
    "            #    self.brain[Agent.num_inputs+Agent.num_memory+1] = 0\n",
    "            #else:\n",
    "\n",
    "            if  self.maze.maze[x-1,y] != Maze.WALL:\n",
    "                self.position = x-1, y\n",
    "                self.time_step = self.time_step+1\n",
    "\n",
    "\n",
    "        elif self.brain[Agent.num_inputs+Agent.num_memory] == 1 and self.brain[Agent.num_inputs+Agent.num_memory+1] == 1:\n",
    "\n",
    "            if self.maze.maze[x,y+1] != Maze.WALL:\n",
    "                self.position = x,y+1\n",
    "                self.time_step = self.time_step+1\n",
    "\n",
    "            \"\"\"\n",
    "            elif y in self.maze.wall_position: # in a door\n",
    "                self.position = x,y+1\n",
    "                self.time_step = self.time_step+1\n",
    "            elif y+1 in self.maze.wall_position and self.maze.maze[x,y+1]!=2: # before a door\n",
    "                #print('before a door >;<')\n",
    "                self.position = x,y+1\n",
    "                self.time_step = self.time_step+1\n",
    "            \"\"\"\n",
    "            x,y = self.position\n",
    "            if y == self.maze.length-1: # reach the end of the maze\n",
    "                self.pass_maze = self.pass_maze + 1\n",
    "                self.init_locate()\n",
    "\n",
    "        elif self.brain[Agent.num_inputs+Agent.num_memory] == 0 and self.brain[Agent.num_inputs+Agent.num_memory+1] == 0:\n",
    "            \"\"\"if self.maze.maze[x,y+1] != Maze.WALL:\n",
    "                self.position = x,y+1\n",
    "                self.time_step = self.time_step+1\n",
    "\n",
    "            elif y in self.maze.wall_position: # in a door\n",
    "                self.position = x,y+1\n",
    "                self.time_step = self.time_step+1\n",
    "            elif y+1 in self.maze.wall_position and self.maze.maze[x,y+1]!=2: # before a door\n",
    "                #print('before a door >;<')\n",
    "                self.position = x,y+1\n",
    "                self.time_step = self.time_step+1\n",
    "\n",
    "            x,y = self.position\n",
    "            if y == self.maze.length-1: # reach the end of the maze\n",
    "                self.pass_maze = self.pass_maze + 1\n",
    "                self.init_locate()\n",
    "                \"\"\"\n",
    "            self.position = x,y\n",
    "            self.time_step = self.time_step+1\n",
    "            '''else:\n",
    "                # shouldn't have this\n",
    "                self.brain[10] = 1\n",
    "                self.brain[11] = 0\n",
    "            '''    \n",
    "        '''elif self.brain[10] == 0 and self.brain[11] == 0:\n",
    "            self.brain[10] = 0\n",
    "            self.brain[11] = 1\n",
    "        ''' \n",
    "        \"\"\"else:\n",
    "            xx, yy = self.random_walk(x,y)\n",
    "            self.position = xx, yy\n",
    "            self.time_step = self.time_step+1\n",
    "            \n",
    "            x,y = self.position\n",
    "            if y == self.maze.length-1: # reach the end of the maze\n",
    "                self.pass_maze = self.pass_maze + 1\n",
    "                self.init_locate()\n",
    "\"\"\"\n",
    "            \n",
    "        # if the brain's order is legal, keep it\n",
    "        # illegal order is omitted\n",
    "        if self.time_step > time_step_shot:    \n",
    "            self.trajectory[self.time_step,:] = self.position\n",
    "        \n",
    "        return fitness, r\n",
    "    \n",
    "    def get_fitness(self):\n",
    "        \n",
    "        return self.score/self.maze.best_score \n",
    "    \n",
    "    \n",
    "        \n",
    "def test():        \n",
    "    maze = Maze(6, 10)\n",
    "    maze.print_maze()\n",
    "\n",
    "    agent = Agent(maze)\n",
    "    \n",
    "    for i in range(1):\n",
    "        #Gate_memory = np.random.rand(2**len(agent.memory_index), 2)\n",
    "        #agent.Gate_memory = Gate_memory.astype(np.float64) / np.sum(Gate_memory, axis=1, dtype=np.float64)[:, None]\n",
    "\n",
    "        #Gate_control = np.random.rand(2**len(agent.control_index), 4)\n",
    "        #agent.Gate_control = Gate_control.astype(np.float64) / np.sum(Gate_control, axis=1, dtype=np.float64)[:, None]\n",
    "        \n",
    "        agent.simple_reinit()\n",
    "        while (agent.end == False):\n",
    "            #maze.print_maze(agent.position[0], agent.position[1])\n",
    "            agent.brain_update()\n",
    "            print(agent.brain, agent.position)\n",
    "            fitness,r = agent.step()\n",
    "\n",
    "\n",
    "        print(fitness)\n",
    "        #print(agent.trajectory)\n",
    "        #print(agent.gene)\n",
    "\n",
    "    \n",
    "np.random.seed(5)       \n",
    "test()\n",
    "        \n",
    "                \n",
    "    \n",
    "#np.random.seed(9)\n",
    "#import cProfile\n",
    "#cProfile.run('test()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "\n",
    "num_generation = 200\n",
    "pop_size = 20\n",
    "\n",
    "\n",
    "\n",
    "k=3\n",
    "fit_times = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def init_pop_agent(pop_size, agent):\n",
    "    \n",
    "    pop = np.zeros((pop_size, 2**len(agent.control_index), 4))\n",
    "    for i in np.arange(pop_size):\n",
    "        Gate_control = np.random.rand(2**len(agent.control_index), 4)\n",
    "        Gate_control = Gate_control.astype(np.float64) / np.sum(Gate_control, axis=1, dtype=np.float64)[:, None]\n",
    "        pop[i] = Gate_control\n",
    "    #    pop.append(Agent_circular(maze))\n",
    "    return pop\n",
    "\n",
    "        \n",
    "\n",
    "def mutate(Gate_control):\n",
    "    Gate_control = Gate_control + np.random.rand()*0.3\n",
    "    Gate_control = Gate_control.astype(np.float64) / np.sum(Gate_control, axis=1, dtype=np.float64)[:, None]\n",
    "    return Gate_control\n",
    "\n",
    "    \n",
    "            \n",
    "    \n",
    "def select(fitness, k):    # nature selection wrt pop's fitness\n",
    "    #idx = np.random.choice(np.arange(pop_size), size=pop_size-k, replace=True,\n",
    "    #                       p=(fitness_pop+1)/(fitness_pop+1).sum())\n",
    "    fitness_pop = np.copy(fitness)\n",
    "    count = np.zeros(pop_size)\n",
    "    ret_idx = np.zeros(pop_size-k, dtype = int)\n",
    "    for i in np.arange(pop_size-k):\n",
    "        idx = np.random.choice(np.arange(pop_size), size=1, replace=True,\n",
    "                           p=(fitness_pop+1)/(fitness_pop+1).sum())\n",
    "        ret_idx[i] = idx\n",
    "        count[idx] += 1\n",
    "        if count[idx]>=2:\n",
    "            fitness_pop[idx]=0\n",
    "    \n",
    "    return ret_idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "def evolution():\n",
    "    # initialize the maze environment\n",
    "    maze_width = 6\n",
    "    maze_length = 10\n",
    "    maze = Maze(maze_width, maze_length)\n",
    "    agent = Agent(maze)\n",
    "    # initialize the agent population\n",
    "    pop = init_pop_agent(pop_size, agent)\n",
    "    maze.print_maze()\n",
    "        \n",
    "    \n",
    "    # save statistics\n",
    "    generation_trend = np.zeros(num_generation)\n",
    "    elite_trend = np.zeros(num_generation)\n",
    "    generation_trend_meanfit = np.zeros(num_generation)\n",
    "    elite_trend_meanfit = np.zeros(num_generation)\n",
    "    \n",
    "    generation_pop_allfit = np.zeros((num_generation, pop_size, fit_times))\n",
    "    generation_pop_fit = np.zeros((num_generation, pop_size))\n",
    "\n",
    "    \n",
    "    # evole\n",
    "    for i_ in np.arange(num_generation):\n",
    "        print(\".\", end=\"\")\n",
    "        if i_ % 50 == 0 and i_>0: print(\" \")\n",
    "    \n",
    "        # eval the population fitness\n",
    "        fitness_pop = np.zeros(pop_size, dtype = np.float32)\n",
    "        fitness_pop_meanfit = np.zeros(pop_size, dtype = np.float32)\n",
    "        \n",
    "        #================1=================\n",
    "        # Start evolution & evaluate fitness\n",
    "        for ind in np.arange(len(pop)):\n",
    "            agent.Gate_control = pop[ind]\n",
    "            #print(agent.Gate_control)\n",
    "            fitness_tmp = np.zeros(fit_times)\n",
    "            for repeat_fit in np.arange(fit_times):\n",
    "                agent.simple_reinit()\n",
    "                while(agent.end==False):\n",
    "                    agent.brain_update()\n",
    "                    #print(agent.brain, agent.position)\n",
    "                    fitness_once,r = agent.step()\n",
    "                fitness_tmp[repeat_fit] = fitness_once\n",
    "                \n",
    "            \n",
    "            \n",
    "            fitness = np.prod(fitness_tmp)**(1/fit_times) * 100\n",
    "            #print(fitness)\n",
    "            fitness_pop[ind] = fitness\n",
    "            agent.fitness = fitness\n",
    "            \n",
    "            generation_pop_fit[i_,ind] = fitness\n",
    "            \n",
    "            fitness_meanfit = np.mean(fitness_tmp)\n",
    "            fitness_pop_meanfit[ind] = fitness_meanfit\n",
    "\n",
    "        \n",
    "        #print(fitness_pop)\n",
    "\n",
    "        #================2=================\n",
    "        # keep k elites without mutation\n",
    "        elite_idx = np.argpartition(fitness_pop, -k)[-k:]\n",
    "        elites = copy.deepcopy(pop[elite_idx])\n",
    "        #print(\"elite_idx \", elite_idx)\n",
    "\n",
    "        \n",
    "        #================3=================\n",
    "        # select & mutation w.r.t. non-elites\n",
    "        #others_idx = np.argpartition(fitness_pop, -k)[:-k]\n",
    "        #print(\"others_idx \", others_idx)\n",
    "        #sort_fit = np.concatenate((fitness_pop[elite_idx], fitness_pop[others_idx]))\n",
    "        #print(sort_fit)\n",
    "        \n",
    "        idx = select(fitness_pop, k)\n",
    "        other_pop = copy.deepcopy(pop[idx])\n",
    "\n",
    "        \n",
    "        #================4=================\n",
    "        # save data \n",
    "        elite_trend[i_] = np.mean(fitness_pop[elite_idx])\n",
    "        elite_trend_meanfit[i_] = np.mean(fitness_pop_meanfit[elite_idx])\n",
    "        generation_trend[i_] = np.mean(fitness_pop)\n",
    "        generation_trend_meanfit[i_] = np.mean(fitness_pop_meanfit)\n",
    "        \n",
    "        if i_ > 0 and i_%100==0 or i_ == num_generation-1:\n",
    "            with open(\"./save_model/pop\"+str(i_)+\".pickle\",\"wb\") as f:\n",
    "                pickle.dump(pop, f)\n",
    "            with open(\"./save_model/elites\"+str(i_)+\".pickle\",\"wb\") as f:\n",
    "                pickle.dump(elites, f)\n",
    "                \n",
    "                \n",
    "            with open(\"./save_model/generation_trend.pickle\",\"wb\") as f:\n",
    "                pickle.dump(generation_trend, f)\n",
    "            with open(\"./save_model/elite_trend.pickle\",\"wb\") as f:\n",
    "                pickle.dump(elite_trend, f)\n",
    "            with open(\"./save_model/generation_trend_meanfit.pickle\",\"wb\") as f:\n",
    "                pickle.dump(generation_trend_meanfit, f)\n",
    "            with open(\"./save_model/elite_trend_meanfit.pickle\",\"wb\") as f:\n",
    "                pickle.dump(elite_trend_meanfit, f)\n",
    "\n",
    "\n",
    "            with open(\"./save_model/generation_pop_allfit.pickle\",\"wb\") as f:\n",
    "                pickle.dump(generation_pop_allfit, f)\n",
    "            with open(\"./save_model/generation_pop_fit.pickle\",\"wb\") as f:\n",
    "                pickle.dump(generation_pop_fit, f)\n",
    "\n",
    "                \n",
    "                \n",
    "        #================5=================     \n",
    "        # mutate & cross_over\n",
    "        for mut_ind, parent in enumerate(other_pop):\n",
    "            #if np.random.rand()<cross_rate:\n",
    "            #    cross_over(parent, pop)\n",
    "            mutate(parent)\n",
    "            \n",
    "         \n",
    "        #new_idx = np.array(np.concatenate((elite_idx, idx)))\n",
    "        #pop = pop[new_idx]\n",
    "        pop = np.concatenate((elites, other_pop), axis=0)\n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "        #================6=================\n",
    "        # generate a new maze & reset agents\n",
    "        #if i_>100 and i_ %100==0:\n",
    "        #   print(\"a new maze after \", i_)\n",
    "        #    maze = Maze(maze_width, maze_length)\n",
    "        #    agent.maze = maze\n",
    "                \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    #================7=================        \n",
    "    # after evolution plot the results\n",
    "    plt.title('Evolution Trend')\n",
    "    plt.plot(np.arange(num_generation), generation_trend, color='green', label='Generation gmean trend',linestyle = ':')\n",
    "    plt.plot(np.arange(num_generation), elite_trend, color='red', label='Elite gmean trend',linestyle='-.')\n",
    "    #plt.plot(np.arange(num_generation), generation_trend_meanfit, color='blue', label='Generation mean trend',linestyle = ':')\n",
    "    #plt.plot(np.arange(num_generation), elite_trend_meanfit, color='black', label='Elite mean trend',linestyle='-.')\n",
    "    plt.xlabel(\"Evolution times\")\n",
    "    plt.ylabel(\"fitness (number of maze passes)\")\n",
    "    plt.legend() # 显示图例\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Running\")\n",
    "evolution()\n",
    "\n",
    "print(\"End\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================utils===========================================================\n",
    "def calculate_discout_reward(rewards, gamma):\n",
    "    discounted_reward = []\n",
    "    cumulative_sum = 0\n",
    "    for i, r in enumerate(reversed(rewards)):\n",
    "        cumulative_sum = cumulative_sum*gamma + r\n",
    "        discounted_reward.append(cumulative_sum)\n",
    "    return discounted_reward[::-1]\n",
    "\n",
    "def calculate_discout_reward_window(reward, gamma, length=3):\n",
    "    target_discount_reward = []\n",
    "    convolution_filter = [gamma**i for i in range(length)]\n",
    "    return np.convolve(reward, convolution_filter, 'valid')\n",
    "\n",
    "\n",
    "\n",
    "#=========================Agent===========================================================\n",
    "def collect_experience(env, agent, number_action=8):\n",
    "\n",
    "    observations, rewards, is_not_done = [], [], []\n",
    "    action_probs = []\n",
    "    action_taken_for_memory, action_taken_for_control = [],[]\n",
    "    \n",
    "    agent.perception()\n",
    "    obs = np.copy(agent.brain)\n",
    "    \n",
    "    while (agent.end == False):        \n",
    "        \n",
    "        obs_for_memory = obs[agent.memory_index]\n",
    "        out_for_memory = agent.Gate_memory(\n",
    "            tf.expand_dims(\n",
    "                tf.convert_to_tensor(obs_for_memory, dtype=tf.float32), axis=0\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if isinstance(out_for_memory, tuple):\n",
    "            prob_for_memory, _ = out_for_memory\n",
    "        else:\n",
    "            prob_for_memory = out_for_memory\n",
    "\n",
    "        action_prob_for_memory = tf.nn.softmax(prob_for_memory)\n",
    "\n",
    "        # Sample\n",
    "        samples_for_memory = tf.random.multinomial(tf.log(action_prob_for_memory), 1).numpy()[0][0]\n",
    "\n",
    "        action_for_memory = np.zeros(2)\n",
    "        action_for_memory[samples_for_memory] = 1\n",
    "\n",
    "\n",
    "        sample_str = bin(samples_for_memory)\n",
    "        sample = np.zeros(1, dtype=np.int)\n",
    "        for i, v in enumerate(sample_str[2:]):\n",
    "            sample[i]=int(v)\n",
    "        agent.brain[6] = sample\n",
    "        \n",
    "        #==============================================\n",
    "        \n",
    "        obs_for_control = obs[agent.control_index]\n",
    "        out_for_control = agent.Gate_control(\n",
    "            tf.expand_dims(\n",
    "                tf.convert_to_tensor(obs_for_control, dtype=tf.float32), axis=0\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if isinstance(out_for_control, tuple):\n",
    "            prob_for_control, _ = out_for_control\n",
    "        else:\n",
    "            prob_for_control = out_for_control\n",
    "\n",
    "        action_prob_for_control = tf.nn.softmax(prob_for_control)\n",
    "\n",
    "        # Sample\n",
    "        samples_for_control = tf.random.multinomial(tf.log(action_prob_for_control), 1).numpy()[0][0]\n",
    "\n",
    "        action_for_control = np.zeros(4)\n",
    "        action_for_control[samples_for_control] = 1\n",
    "\n",
    "\n",
    "        sample_str = bin(samples_for_control)\n",
    "        sample = np.zeros(2, dtype=np.int)\n",
    "        for i, v in enumerate(sample_str[2:]):\n",
    "            sample[i]=int(v)\n",
    "        agent.brain[7:] = sample\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        _,r = agent.step()\n",
    "        agent.perception()\n",
    "        next_obs = np.copy(agent.brain)\n",
    "\n",
    "        observations.append(obs)\n",
    "        rewards.append(r)\n",
    "        action_taken_for_memory.append(action_for_memory)\n",
    "        action_taken_for_control.append(action_for_control)\n",
    "\n",
    "\n",
    "        obs = next_obs\n",
    "            \n",
    "    observations.append(obs)\n",
    "\n",
    "    return np.stack(observations), np.stack(rewards), np.stack(action_taken_for_memory), np.stack(action_taken_for_control)\n",
    "\n",
    "            \n",
    "            \n",
    "#=========================main===========================================================\n",
    "logger = logging.getLogger(os.path.basename(sys.argv[0]))\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    # -------------------- * --------------------\n",
    "    argparser = argparse.ArgumentParser('PG', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    argparser.add_argument('--gamma', action=\"store\", type=float, default=0.99)\n",
    "    argparser.add_argument('--learning-rate', action=\"store\", type=float, default=8e-4)\n",
    "    argparser.add_argument('--episode-train', action=\"store\", type=int, default=500)\n",
    "\n",
    "    argparser.add_argument('--output-save-img', action=\"store\", type=str, default=None)\n",
    "\n",
    "    args = argparser.parse_args(argv)\n",
    "    gamma = args.gamma\n",
    "    learning_rate = args.learning_rate\n",
    "    episode_train = args.episode_train\n",
    "    output_save_img = args.output_save_img\n",
    "    # -------------------- * --------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    gamma =  0.99\n",
    "    episode_train = 50\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    maze = Maze(10,50)\n",
    "    agent = Agent(maze)\n",
    "    \n",
    "    \"\"\"checkpoint_directory = \"./tfmodel\"\n",
    "    checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt-1\")\n",
    "    checkpoint = tf.train.Checkpoint(Gate=agent.Gate)   \n",
    "    checkpoint.restore(checkpoint_prefix)\"\"\"\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    average_reward, reward_curve = 0, []\n",
    "    \n",
    "    for sample_rate_update_ind in range(10):\n",
    "        print(\"episode \", sample_rate_update_ind)\n",
    "    #===================sample N=5 times for updating nodes rates========================================\n",
    "        nodes_for_memory, nodes_for_control, rewards = [], [], []\n",
    "        # repeat to get stable gradients for node rates\n",
    "        for sample_rate_ind in range(50):\n",
    "        #training an agent \n",
    "            for rpt in range(1): # sample number of mazes\n",
    "                maze = Maze(10,50)\n",
    "                agent.maze = maze\n",
    "                print(agent.maze.average_reward)\n",
    "                for eps in range(episode_train): # train on a maze\n",
    "\n",
    "                    agent.simple_reinit()\n",
    "                    observation_rollout, reward_rollout, action_rollout_for_memory, action_rollout_for_control = collect_experience(agent.maze, agent)\n",
    "                    discounted_reward_rollout = calculate_discout_reward(reward_rollout, gamma)\n",
    "\n",
    "                    # Remove last observation\n",
    "                    observation_rollout = observation_rollout[:-1]\n",
    "\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        tape.watch(agent.Gate_memory.variables)\n",
    "\n",
    "                        all_prob = tf.log(tf.nn.softmax(agent.Gate_memory(\n",
    "                            tf.convert_to_tensor(observation_rollout[:,agent.memory_index], dtype=tf.float32)\n",
    "                        )))\n",
    "\n",
    "                        all_prob_masked = tf.reduce_sum(action_rollout_for_memory *  all_prob, axis=-1)\n",
    "                        loss = tf.reduce_sum(all_prob_masked * discounted_reward_rollout * -1)\n",
    "                    grad = tape.gradient(loss, agent.Gate_memory.variables)\n",
    "                    optimizer.apply_gradients(zip(grad, agent.Gate_memory.variables))\n",
    "\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        tape.watch(agent.Gate_control.variables)\n",
    "\n",
    "                        all_prob = tf.log(tf.nn.softmax(agent.Gate_control(\n",
    "                            tf.convert_to_tensor(observation_rollout[:, agent.control_index], dtype=tf.float32)\n",
    "                        )))\n",
    "\n",
    "                        all_prob_masked = tf.reduce_sum(action_rollout_for_control *  all_prob, axis=-1)\n",
    "                        loss_c = tf.reduce_sum(all_prob_masked * discounted_reward_rollout * -1)\n",
    "                    grad_c = tape.gradient(loss_c, agent.Gate_control.variables)\n",
    "                    optimizer.apply_gradients(zip(grad_c, agent.Gate_control.variables))\n",
    "\n",
    "\n",
    "                    average_reward += np.mean(reward_rollout)\n",
    "\n",
    "                    if eps%5 == 0 and eps > 0:\n",
    "                        print(\"Currently in episode %d and the average reward is %f\" % (eps, average_reward))\n",
    "                        #logger.info(\"Currently in episode {eps} and the average reward is {average_reward}\" )\n",
    "                        reward_curve.append(average_reward)\n",
    "                        average_reward = 0\n",
    "\n",
    "\n",
    "                nodes_for_memory.append(agent.memory_index)\n",
    "                nodes_for_control.append(agent.control_index)\n",
    "                rewards.append(average_reward)\n",
    "\n",
    "            #====================resampling the input nodes=========================================================\n",
    "            agent.memory_index = [] # np.array([], dtype = np.int)\n",
    "            for ind in np.arange(agent.brain_size):\n",
    "                if np.random.binomial(1, agent.memory_rate[ind]):\n",
    "                    agent.memory_index.append(ind)\n",
    "                    # np.append(self.memory_index, ind)\n",
    "            agent.memory_index = np.array(agent.memory_index, dtype=np.int)\n",
    "\n",
    "            agent.control_index = []\n",
    "            for ind in np.arange(agent.brain_size):\n",
    "                if np.random.binomial(1, agent.control_rate[ind]):\n",
    "                    agent.control_index.append(ind)\n",
    "            agent.control_index = np.array(agent.control_index, dtype=np.int)    \n",
    "\n",
    "            agent.Gate_memory = NeuroGate_memory(len(agent.memory_index))\n",
    "            agent.Gate_control = NeuroGate_control(len(agent.control_index))\n",
    "\n",
    "        #=========================update nodes sampling rate===============================    \n",
    "        memory_nodes_rewards = np.zeros(agent.brain_size)\n",
    "        control_nodes_rewards = np.zeros(agent.brain_size)\n",
    "        for val, rewards_val in zip(nodes_for_memory, rewards):\n",
    "            for nodes_val in val:\n",
    "                memory_nodes_rewards[nodes_val] += rewards_val\n",
    "        for val, rewards_val in zip(nodes_for_control, rewards):\n",
    "            for nodes_val in val:\n",
    "                control_nodes_rewards[nodes_val] += rewards_val\n",
    "\n",
    "        memory_nodes_rewards = np.exp(memory_nodes_rewards)\n",
    "        control_nodes_rewards = np.exp(control_nodes_rewards)\n",
    "        agent.memory_rate = memory_nodes_rewards/memory_nodes_rewards.sum()\n",
    "        agent.control_rate = control_nodes_rewards/control_nodes_rewards.sum()\n",
    "        \n",
    "        print(agent.memory_rate)\n",
    "        print(agent.control_rate)\n",
    "        \n",
    "        \n",
    "\n",
    "    #sns.lineplot(y=reward_curve, x=list(range(len(reward_curve))))\n",
    "    plt.plot(range(len(reward_curve)), reward_curve)\n",
    "    plt.show()\n",
    "    #plt.savefig(output_save_img)\n",
    "    \n",
    "    checkpoint_directory = \"./tfmodel\"\n",
    "    checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(Gate_memory=agent.Gate_memory, Gate_control = agent.Gate_control)\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    #status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n",
    "\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#print(' '.join(sys.argv))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():        \n",
    "    maze = Maze(10,40)\n",
    "    maze.print_maze()\n",
    " \n",
    "    agent = Agent_circular(maze)                    \n",
    "    checkpoint_directory = \"./tfmodel\"\n",
    "    checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt-1\")\n",
    "    checkpoint = tf.train.Checkpoint(Gate=agent.Gate)   \n",
    "    checkpoint.restore(checkpoint_prefix)\n",
    "\n",
    "  \n",
    "    agent.perception()\n",
    "    obs = agent.brain\n",
    "    while(agent.end == False):\n",
    "        \n",
    "        out = agent.Gate(\n",
    "            tf.expand_dims(\n",
    "                tf.convert_to_tensor(obs, dtype=tf.float32), axis=0\n",
    "            )\n",
    "        )\n",
    "        action_prob = tf.nn.softmax(out)\n",
    "\n",
    "        # Sample\n",
    "        samples = tf.random.multinomial(tf.log(action_prob), 1).numpy()[0][0]\n",
    "\n",
    "        action = np.zeros((64))\n",
    "        action[samples] = 1\n",
    "\n",
    "\n",
    "        sample_str = bin(samples)\n",
    "        sample = np.zeros(3, dtype=np.int)\n",
    "        for i, v in enumerate(sample_str[2:]):\n",
    "            sample[i]=int(v)\n",
    "        agent.brain[6:] = sample\n",
    "        print(agent.brain)\n",
    "        \n",
    "        _,r = agent.step()\n",
    "        agent.perception()\n",
    "        next_obs = np.copy(agent.brain)\n",
    "        \n",
    "        obs = next_obs\n",
    "    \n",
    "    print(agent.fitness)\n",
    "    print(agent.pass_maze)\n",
    "    print(agent.trajectory)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "  \n",
    "\n",
    "#np.random.seed(9)\n",
    "test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
