{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import os, sys\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "np.random.seed(15)\n",
    "tf.random.set_random_seed(15)\n",
    "random.seed(15)\n",
    "\n",
    "np.set_printoptions(precision=2, threshold=np.inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze(object):\n",
    "    WALL = 2\n",
    "    EMPTY = 8\n",
    "    LEFT = 0\n",
    "    RIGHT = 1 # right or forward\n",
    "    BONUS = 1000\n",
    "    def __init__(self, width, length): \n",
    "        self.length = length\n",
    "        self.width = width\n",
    "        self.maze = np.ones((self.width, self.length)) * Maze.WALL\n",
    "\n",
    "        self.generate_maze()\n",
    "        \n",
    "        #set self.maze_mask\n",
    "        #self.shortest_solutions\n",
    "        self.get_shortest_solutions()\n",
    "        \n",
    "        #self.longest_shortest, used to calculate objective value\n",
    "        self.get_longest_shortest_solutions()\n",
    "        \n",
    "        # used to normalize objective value\n",
    "        self.best_score = self.get_attainable_score()\n",
    "\n",
    "        #initialize the agent position in the maze\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def generate_maze(self):\n",
    "        # generate walls, doors\n",
    "        \n",
    "        spaces = np.random.randint(low=1, high=4, size=self.length)\n",
    "        cum_spaces = np.cumsum(spaces) # leave the first col empty\n",
    " \n",
    "        for ind, val in enumerate(cum_spaces):\n",
    "            if val >= self.length-1:\n",
    "                self.wall_position = cum_spaces[:ind]\n",
    "                break\n",
    "        if self.wall_position[0] > 1:\n",
    "            self.wall_position[0] = 1\n",
    "        if self.wall_position[-1] < self.length-1:\n",
    "            self.wall_position = np.append(self.wall_position, self.length-1)\n",
    "                \n",
    "        self.road_position = np.array([]).astype(np.int)\n",
    "        for ind in np.arange(self.length-1):\n",
    "            if ind not in self.wall_position:\n",
    "                self.road_position = np.append(self.road_position, ind)\n",
    "        \n",
    "        for i in self.road_position:\n",
    "            self.maze[1:-1,i]=Maze.EMPTY\n",
    "        \n",
    "        self.door_position = np.random.randint(low=1, high=self.width-1, size=len(self.wall_position))\n",
    "        #print(self.door_position)\n",
    "    \n",
    "        # get door position\n",
    "        self.door_position = np.zeros(len(self.wall_position), dtype = np.int)\n",
    "        self.door_position[-1] = np.random.randint(low=1, high=self.width-1) #1~self.width-2 available door position\n",
    "        for ind in np.arange(len(self.wall_position)-2, -1, -1):\n",
    "            if self.wall_position[ind] == self.wall_position[ind+1] -1: # two walls together\n",
    "                self.door_position[ind] = self.door_position[ind+1]\n",
    "                \n",
    "            else:\n",
    "                self.door_position[ind] = np.random.randint(low=1, high=self.width-1)\n",
    "        \n",
    "        # Fill door cue\n",
    "        self.maze[ self.door_position[-1], self.wall_position[-1] ] = Maze.RIGHT # default last door due\n",
    "        for i in np.arange(len(self.wall_position)-2, -1, -1):\n",
    "            if self.door_position[i+1] < self.door_position[i]:\n",
    "                self.maze[self.door_position[i], self.wall_position[i]] = Maze.LEFT\n",
    "            else: \n",
    "                self.maze[self.door_position[i], self.wall_position[i]] = Maze.RIGHT\n",
    "                \n",
    "                \n",
    "                \n",
    "       \n",
    "                \n",
    "    def print_maze(self, x=-1, y=-1):\n",
    "        if x>=0 and y>=0:\n",
    "            tmp = self.maze[x,y]\n",
    "            self.maze[x,y] = -1 # position of the agent\n",
    "            \n",
    "        print(\"  \", end=\"\")    \n",
    "        #for i in np.arange(self.length):\n",
    "        #    print('%d ' % i, end='')\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        for j in np.arange(self.width):\n",
    "            print('%d ' % j, end='')\n",
    "            for i in np.arange(self.length):\n",
    "            \n",
    "                if self.maze[j,i]==Maze.WALL: # wall position\n",
    "                    print('H ',end='')\n",
    "                elif self.maze[j,i]==Maze.EMPTY:\n",
    "                    print('  ',end='')# road\n",
    "                elif self.maze[j,i]==-1:\n",
    "                    print('T ',end='')\n",
    "                    self.maze[x,y]= tmp\n",
    "                else:\n",
    "                    print('%d ' % self.maze[j,i], end='')\n",
    "            print('\\n')\n",
    "\n",
    "        \n",
    "    def get_shortest_solutions(self):\n",
    "        # get the shortest length to the end of maze from each layer\n",
    "        \n",
    "        self.maze_mask = np.zeros(self.length, dtype=np.int)\n",
    "        for ind, val in enumerate(self.wall_position):\n",
    "            self.maze_mask[val] = self.door_position[ind]\n",
    "       \n",
    "        self.shortest_solutions = np.zeros(self.length, dtype=np.int)\n",
    "        step = 0\n",
    "        next_wall = self.length-1\n",
    "        for ind in np.arange(self.length-2, -1, -1):\n",
    "            if self.maze_mask[ind] == 0: # road\n",
    "                step += 1\n",
    "                self.shortest_solutions[ind] = step\n",
    "            else: # wall\n",
    "                step += np.abs(self.maze_mask[next_wall] - self.maze_mask[ind])+1 #1 out the door, +diff for vert.\n",
    "                self.shortest_solutions[ind] = step\n",
    "                next_wall = ind\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_distance_escape(self, x, y):\n",
    "        # get the shortest distance to escape from the current position\n",
    "        vertical_distance = 0\n",
    "        if y in self.road_position:\n",
    "            for next_wall_ind in np.arange(y+1, y+4, 1):\n",
    "                if next_wall_ind in self.wall_position: break\n",
    "            vertical_distance = np.abs(self.maze_mask[next_wall_ind] - x)\n",
    "        return self.shortest_solutions[y]+vertical_distance\n",
    "                \n",
    "\n",
    "        \n",
    "    def get_longest_shortest_solutions(self):\n",
    "        # get the shortest length from corner of starting to the end out maze\n",
    "        left = self.get_distance_escape(1,0)\n",
    "        right = self.get_distance_escape(self.width-2,0)\n",
    "        \n",
    "        self.longest_shortest = np.maximum(left, right)+5 # higher than true value\n",
    "    \n",
    "    \n",
    "    def get_attainable_score(self):\n",
    "        position = []\n",
    "        x = self.door_position[0] # in front of the first door\n",
    "        y = 0\n",
    "        position.append([x,y])\n",
    "        \n",
    "        score = np.float32(0)\n",
    "        door_signal=self.maze[self.door_position[0], 1]\n",
    "        r=[]\n",
    "        self.steps = 0\n",
    "        \n",
    "        while True:\n",
    "            pass_wall = False\n",
    "            self.steps += 1\n",
    "            if self.maze[x, y+1]!=Maze.WALL: # road\n",
    "                y += 1\n",
    "                pass_wall=True\n",
    "                if y in self.wall_position:\n",
    "                    door_signal = self.maze[x,y]\n",
    "            else: # wall\n",
    "                if door_signal == 0 and self.maze[x-1,y]==Maze.WALL: # init location make door signal no more signal\n",
    "                    door_signal = 1\n",
    "                if door_signal == 1 and self.maze[x+1,y]==Maze.WALL:\n",
    "                    door_signal = 0\n",
    "                x += int(door_signal*2-1)\n",
    "                \n",
    "            position.append([x,y])\n",
    "            r.append((self.longest_shortest - self.get_distance_escape(x,y) )/self.longest_shortest-1+int(pass_wall))\n",
    "            score += (self.longest_shortest - self.get_distance_escape(x,y) )/self.longest_shortest-1+int(pass_wall)\n",
    "            if y == self.length-1:\n",
    "                r[-1] += Maze.BONUS\n",
    "                score += Maze.BONUS\n",
    "                break\n",
    "        \n",
    "        #print(position)\n",
    "        self.average_reward = np.mean(r)\n",
    "     \n",
    "        return score\n",
    "        \n",
    "    \"\"\"\n",
    "    def get_attainable_score(self):\n",
    "        position = []\n",
    "        x = self.door_position[0] # in front of the first door\n",
    "        y = 0\n",
    "        score = np.float32(0)\n",
    "        pass_maze = 0\n",
    "        door_signal=self.maze[self.door_position[0], 1]\n",
    "        r=[]\n",
    "        for _ in np.arange(300, -1, -1):\n",
    "            position.append([x,y])\n",
    "            if y != self.length-1:\n",
    "                r.append((self.longest_shortest - self.get_distance_escape(x,y) )/self.longest_shortest + pass_maze)\n",
    "                score += (self.longest_shortest - self.get_distance_escape(x,y) )/self.longest_shortest + pass_maze\n",
    "            if self.maze[x, y+1]!=Maze.WALL: # road\n",
    "                y += 1\n",
    "                if y in self.wall_position:\n",
    "                    door_signal = self.maze[x,y]\n",
    "                if y == self.length-1:\n",
    "                    pass_maze += 1\n",
    "                    y=0\n",
    "            else: # wall\n",
    "                if door_signal == 0 and self.maze[x-1,y]==Maze.WALL: # init location make door signal no more signal\n",
    "                    door_signal = 1\n",
    "                if door_signal == 1 and self.maze[x+1,y]==Maze.WALL:\n",
    "                    door_signal = 0\n",
    "                x += int(door_signal*2-1)\n",
    "        \n",
    "        #print(position)\n",
    "        self.average_reward = np.mean(r)\n",
    "     \n",
    "        return score\n",
    "    \"\"\"\n",
    "    \n",
    "    def reset(self):\n",
    "        self.score = 0 \n",
    "        \n",
    "        self.position = np.array([self.door_position[-1], 0]) # in front of the last door\n",
    "        self.trajectory = []\n",
    "        self.trajectory.append(self.position)\n",
    "        \n",
    "        \n",
    "        x, y = self.position\n",
    "        observation = self.perception()\n",
    "        \n",
    "        return observation\n",
    "        \n",
    "    def perception(self):\n",
    "        x, y = self.position\n",
    "        observation = np.zeros(6)\n",
    "        \n",
    "        if self.maze[x,y+1] == Maze.WALL:\n",
    "            observation[0]=1\n",
    "        else: observation[0]=0\n",
    "        \n",
    "        if self.maze[x-1,y+1] == Maze.WALL:\n",
    "            observation[1]=1\n",
    "        else: observation[1]=0\n",
    "        \n",
    "        if self.maze[x+1,y+1] == Maze.WALL:\n",
    "            observation[2] = 1\n",
    "        else: observation[2]=0\n",
    "        \n",
    "        if self.maze[x-1,y] == Maze.WALL:\n",
    "            observation[4]=1\n",
    "        else: observation[4]=0\n",
    "        \n",
    "        if self.maze[x+1,y] == Maze.WALL:\n",
    "            observation[5]=1\n",
    "        else: observation[5]=0\n",
    "        \n",
    "        if y in self.wall_position:\n",
    "            observation[3] = self.maze[x, y]\n",
    "            \n",
    "        return observation\n",
    "            \n",
    "    def step(self, action):\n",
    "        \n",
    "        x, y = self.position\n",
    "        \n",
    "        up = int(action[0])\n",
    "        down = int(action[1])\n",
    "        \n",
    "        crash_wall = False\n",
    "        pass_wall = False\n",
    "        if down == 1 and up == 0:\n",
    "            if self.maze[x+1,y]==Maze.WALL:\n",
    "                crash_wall = True\n",
    " \n",
    "            if  self.maze[x+1,y] != Maze.WALL:\n",
    "                self.position = x+1, y\n",
    "                self.trajectory.append(self.position)\n",
    "\n",
    "        elif down == 0 and up == 1:\n",
    "            if self.maze[x-1,y] == Maze.WALL:\n",
    "                crash_wall = True\n",
    "            \n",
    "            if  self.maze[x-1,y] != Maze.WALL:\n",
    "                self.position = x-1, y\n",
    "                self.trajectory.append(self.position)\n",
    "\n",
    "\n",
    "        elif down == 1 and up == 1 or down == 0 and up == 0:\n",
    "        \n",
    "            if self.maze[x,y+1] != Maze.WALL:\n",
    "                pass_wall = True\n",
    "                self.position = x,y+1\n",
    "                self.trajectory.append(self.position)\n",
    "            else:\n",
    "                crash_wall = True\n",
    "                \n",
    "        #elif down == 0 and up == 0:\n",
    "        #    self.position = x, y\n",
    "        #    self.trajectory.append(self.position)\n",
    "            \n",
    "        \n",
    "        x,y = self.position\n",
    "        reward = (self.longest_shortest - self.get_distance_escape(x,y))/self.longest_shortest -1 \n",
    "\n",
    "        reward += int(pass_wall) - int(crash_wall)\n",
    "\n",
    "        self.score +=  reward    \n",
    "        fitness = self.get_fitness()\n",
    "        \n",
    "  \n",
    "        \n",
    "        if y == self.length-1:# at the end of the maze \n",
    "            done = True\n",
    "            observation_ = np.ones(6)\n",
    "            reward += Maze.BONUS # the final reward should be larger than sum of small reward on the way\n",
    "            self.score += Maze.BONUS\n",
    "            fitness = self.get_fitness()\n",
    "        else:\n",
    "            done = False\n",
    "            observation_ = self.perception()\n",
    "\n",
    "\n",
    "        return observation_, fitness, reward, done\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_fitness(self):\n",
    "        \n",
    "        return self.score#/self.best_score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Draw(fitness, fitness_1, xlabel=\"Episodes\", ylabel=\"Fitness\", label = 'fitness trend'):\n",
    "    plt.plot(np.arange(len(fitness)), fitness, color='blue', label=label,linestyle = '-')\n",
    "    plt.plot(np.arange(len(fitness)), fitness_1, color='red', label='Best',linestyle = '-')\n",
    "    #if constant !=0:\n",
    "    #    plt.plot(np.arange(len(fitness)), [constant]*len(fitness), color='red', label=\"Best\",linestyle = '-')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend() # 显示图例\n",
    "    plt.show()\n",
    "\n",
    "def num2action(num):\n",
    "    # set the output nodes according to the action\n",
    "    # used when have an action and want to set the output nodes\n",
    "        numbers = {\n",
    "            0 : '011',\n",
    "            1 : '100',\n",
    "            2 : '110',\n",
    "            3 : '111'\n",
    "        }\n",
    "        return numbers.get(num, None)\n",
    "    \n",
    "def action2num(action):\n",
    "    # get the action according to the output nodes\n",
    "    # used when have the action and want to get the action index and update q_table\n",
    "    numbers = {\n",
    "            '011':0,\n",
    "            '100':1,\n",
    "            '110':2,\n",
    "            '111':3\n",
    "        }\n",
    "    return numbers.get(action, None)\n",
    "    \n",
    "    \n",
    "def observation2index(observation): \n",
    "    # observation: array\n",
    "    # get the state index in the q-table\n",
    "    input_val, marker = 0, 1\n",
    "        \n",
    "    for val in observation: # 03456\n",
    "        if val == 1:\n",
    "            input_val += marker\n",
    "        marker *= 2\n",
    "    return int(input_val)\n",
    "\n",
    "\n",
    "\n",
    "class QAgent:\n",
    "    best_input_ids = [0,3,4,5,6]\n",
    "    best_output_ids = [6,7,8]\n",
    "    best_gates = np.array([[0,0,0,0,0,0,1,0],\n",
    "                        [0,0,0,0,1,0,0,0],\n",
    "                        [0,0,0,0,0,0,1,0],\n",
    "                        [0,0,0,0,1,0,0,0],\n",
    "                        [0,0,0,0,0,0,1,0],\n",
    "                        [0,0,0,1,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,1,0],\n",
    "                        [0,0,0,1,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,1,0],\n",
    "                        [0,0,0,0,1,0,0,0],\n",
    "                        [0,0,0,0,0,0,1,0],\n",
    "                        [0,0,0,0,1,0,0,0],\n",
    "                        [0,0,0,0,0,0,1,0],\n",
    "                        [1,0,0,0,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,0,1],\n",
    "                        [1,0,0,0,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,0,1],\n",
    "                        [0,0,0,1,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,0,1],\n",
    "                        [0,0,0,1,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,0,1],\n",
    "                        [0,0,0,1,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,0,1],\n",
    "                        [0,0,0,1,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,0,1],\n",
    "                        [0,0,0,0,1,0,0,0],\n",
    "                        [0,0,0,0,0,0,0,1],\n",
    "                        [0,0,0,0,1,0,0,0],\n",
    "                        [0,0,0,0,0,0,1,0],\n",
    "                        [1,0,0,0,0,0,0,0],\n",
    "                        [0,0,0,0,0,0,0,1],\n",
    "                        [1,0,0,0,0,0,0,0]])\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, input_ids= [0,1,2,3,4,5], num_memory=1, action_in=0, learning_rate=0.01, reward_decay=0.99, e_greedy=0.9):\n",
    "        # input: input_ids + memory + action_input\n",
    "        # action_input: 0: left, 1 right, 01: both\n",
    "        # output: action + memory if any\n",
    "        self.input_ids=input_ids\n",
    "        self.num_memory=num_memory\n",
    "        if num_memory > 0:\n",
    "            self.memory = np.zeros(num_memory)\n",
    "        self.last_action = np.zeros(2)\n",
    "        \n",
    "        \n",
    "        self.action_in_flag = False\n",
    "        if action_in > 0:\n",
    "            self.action_in_flag = True\n",
    "            action_inputs = {\n",
    "                1:np.array([0]),\n",
    "                2:np.array([1]),\n",
    "                3:np.array([0,1])\n",
    "            }\n",
    "            self.action_input = action_inputs.get(action_in, None)\n",
    "            self.num_input = len(input_ids) + num_memory + len(self.action_input)\n",
    "        else:\n",
    "            self.num_input = len(input_ids) + num_memory\n",
    "            \n",
    "        self.num_output = num_memory + 2\n",
    "            \n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = np.zeros((2**self.num_input, 2**self.num_output ))\n",
    "\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # observation 03456\n",
    "        # action 876\n",
    "        # action selection\n",
    "        if np.random.uniform() < self.epsilon: # choose best action\n",
    "            \n",
    "            input_val = observation2index(observation)\n",
    "            \n",
    "            state_action = self.q_table[input_val, :]\n",
    "            # some actions may have the same value, randomly choose on in these actions\n",
    "            max_index = np.argwhere(state_action == np.max(state_action)).flatten().tolist()\n",
    "            output_val = np.random.choice(max_index)\n",
    "            #print(observation, input_val, state_action, max_index, output_val, num2action(output_val))\n",
    "        else:\n",
    "            # choose random action\n",
    "            output_val = np.random.choice(2**(self.num_output))\n",
    "            # output_val = np.random.choice(2**2)\n",
    "        \n",
    "        action = np.binary_repr(output_val, width=self.num_output)\n",
    "        # action = num2action(output_val)\n",
    "        \n",
    "        # set last action\n",
    "        for ind, val in enumerate(action[:2]):\n",
    "            self.last_action[ind] = int(val)\n",
    "         \n",
    "        # set memory\n",
    "        if self.num_memory >0:\n",
    "            for ind, val in enumerate(action[2:]):\n",
    "                self.memory[ind] = int(val)\n",
    "                    \n",
    "        return action\n",
    "       \n",
    "    def learn(self, s, a, r, s_, done):\n",
    "        input_val = observation2index(s)\n",
    "        #action = action2num(str(a))\n",
    "        action = int(a, 2)\n",
    "        q_predict = self.q_table[input_val, action]\n",
    "        \n",
    "        next_input_val = observation2index(s_)\n",
    "        \n",
    "        if not done:#s_ != 'Terminal':\n",
    "            q_target = r + self.gamma * self.q_table[next_input_val, :].max()  # next state is not terminal\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "        self.q_table[input_val, action] += self.lr * (q_target - q_predict)  # update\n",
    "\n",
    "\n",
    "def update(input_ids=[0,3,4,5], num_memory=1, action_input=0, called=False, seed = 0):\n",
    "\n",
    "    save_file = './Qsave_model/'\n",
    "    agent = QAgent(input_ids, num_memory, action_input)\n",
    "    \n",
    "    max_episode = 300\n",
    "    max_step = 300\n",
    "    fitness = np.zeros(max_episode)\n",
    "    average_reward = np.zeros(max_episode)\n",
    "    maze_average_reward = np.zeros(max_episode)\n",
    "    maze_fitness = np.zeros(max_episode)\n",
    "    \n",
    "    for episode in range(max_episode):\n",
    "        if not called:\n",
    "            if episode > 0 and episode%10 == 0:\n",
    "                print('.',end='')\n",
    "            if episode > 0 and episode%500 == 0: print(\" \")\n",
    "\n",
    "        maze = Maze(10, 50)\n",
    "        observation = maze.reset()\n",
    "        \n",
    "        observation = observation[agent.input_ids]\n",
    "        \n",
    "        if agent.num_memory > 0:\n",
    "            observation = np.append(observation, agent.memory)\n",
    "        if agent.action_in_flag == True:\n",
    "            observation = np.append(observation, agent.last_action[agent.action_input])\n",
    "        \n",
    "        \n",
    "        \n",
    "        rewards = []\n",
    "        \n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "        \n",
    "            action = agent.choose_action(observation)  \n",
    "            \n",
    "            a = maze.position\n",
    "            \n",
    "            observation_, fitness[episode], reward, done = maze.step(action)\n",
    "            \n",
    "            #if (episode == max_episode-1):\n",
    "            #    print(\"step :\", a, observation, action, reward)\n",
    "\n",
    "            observation_ = observation_[agent.input_ids]\n",
    "            if agent.num_memory > 0:\n",
    "                observation_ = np.append(observation_, agent.memory)\n",
    "            if agent.action_in_flag == True:\n",
    "                observation_ = np.append(observation_, agent.last_action[agent.action_input])\n",
    "        \n",
    "                \n",
    "            rewards.append(reward)\n",
    "\n",
    "            \n",
    "            # RL learn from this transition\n",
    "            agent.learn(observation, action, reward, observation_, done)\n",
    "\n",
    "        # swap observation\n",
    "            observation = observation_\n",
    "\n",
    "            steps += 1\n",
    "                \n",
    "            if done or steps>max_step: break\n",
    "            \n",
    "        average_reward[episode] = np.mean(rewards)\n",
    "        maze_average_reward[episode] = maze.average_reward\n",
    "        maze_fitness[episode] = maze.best_score\n",
    "    \n",
    "\n",
    "    if not called:\n",
    "        print(\"\\n agent's average reward: %f, best average reward: %f. Rate:%f\" % (np.mean(rewards), maze.average_reward, np.mean(rewards)/maze.average_reward))\n",
    "        print(np.tan(average_reward[-1]/maze.average_reward * np.pi/2))\n",
    "        #print(agent.q_table)\n",
    "        Draw(fitness, fitness_1 = maze_fitness)\n",
    "        Draw(average_reward, fitness_1=maze_average_reward, ylabel = \"Average reward\", label = 'Reward trend')\n",
    "\n",
    "        # end of game\n",
    "        print('game over')\n",
    "        with open(save_file + \"agent_run\"+str(0)+\".pickle\",\"wb\") as f:\n",
    "            pickle.dump(agent, f)\n",
    "    else:\n",
    "        with open(save_file+\"agent_run\"+str(num_memory)+str(action_input)+str(seed)+\".pickle\",\"wb\") as f:\n",
    "            pickle.dump(agent, f)\n",
    "        return np.tan(average_reward[-1]/maze.average_reward * np.pi/2)\n",
    "\n",
    "np.random.seed(0)\n",
    "update(input_ids= [0,1,2,3,4,5], num_memory=2, action_input=3)#input_ids= [0,1,2,3,4,5], num_memory=1, action_input=[]\n",
    "\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Rollout():\n",
    "\n",
    "    with open('./Qsave_model/agent_run0.pickle','rb') as f:\n",
    "        agent = pickle.load(f)\n",
    "    \n",
    "    experience_buffer = []\n",
    "\n",
    "    max_step = 300\n",
    "\n",
    "\n",
    "    maze = Maze(10, 50)\n",
    "    maze.print_maze()\n",
    "    observation = maze.reset()\n",
    "    observation = observation[agent.input_ids]\n",
    "        \n",
    "    if agent.num_memory > 0:\n",
    "        observation = np.append(observation, agent.memory)\n",
    "    if agent.action_in_flag == True:\n",
    "        observation = np.append(observation, agent.last_action[agent.action_input])\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        action = agent.choose_action(observation)        \n",
    "\n",
    "        observation_, fitness, reward, done = maze.step(action)\n",
    "        observation_ = observation_[agent.input_ids]\n",
    "        if agent.num_memory > 0:\n",
    "            observation_ = np.append(observation_, agent.memory)\n",
    "        if agent.action_in_flag == True:\n",
    "            observation_ = np.append(observation_, agent.last_action[agent.action_input])\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "        #if (episode == max_episode-1):\n",
    "        #    print(\"step :\", maze.position, observation, action, reward)\n",
    "        # RL learn from this transition\n",
    "        agent.learn(observation, action, reward, observation_, done)\n",
    "\n",
    "        # swap observation\n",
    "        observation = observation_\n",
    "\n",
    "        steps += 1\n",
    "        if done or steps>max_step: break\n",
    "            \n",
    "    print(fitness)\n",
    "    print(\"\\n agent's average reward: %f, best average reward: %f. Rate: %f\" % (np.mean(rewards), maze.average_reward, np.mean(rewards)/maze.average_reward))\n",
    "    print(\"\\n agent's steps: %f, best steps: %f. Extra: %f \"% (steps, maze.steps, steps-maze.steps))\n",
    "    print(np.stack(maze.trajectory))\n",
    "   \n",
    "    # end of game\n",
    "    print('game over')\n",
    "\n",
    "Rollout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_sampler():\n",
    "\n",
    "    memory = [0,1,2,3,4]\n",
    "    action = [0,1,2,3]\n",
    "    save_file = './node_sampling/'\n",
    "    \n",
    "    memory_record = []\n",
    "    action_record = []\n",
    "    \n",
    "    for m in range(len(memory)):\n",
    "        for a in range(len(action)):\n",
    "\n",
    "            num_memory_mask = np.zeros(len(memory))\n",
    "            num_memory_mask[m] = 1\n",
    "            action_mask = np.zeros(len(action))\n",
    "            action_mask[a] = 1\n",
    "\n",
    "            memory_record.append(num_memory_mask)\n",
    "            action_record.append(action_mask)\n",
    "            \n",
    "    \n",
    "    memory_record = np.stack(memory_record)\n",
    "    action_record = np.stack(action_record)\n",
    "    \n",
    "    with open(save_file+'memory_record.pickle','wb') as f:\n",
    "        pickle.dump(memory_record,f)\n",
    "    with open(save_file+'action_input_record.pickle','wb') as f:\n",
    "        pickle.dump(action_record,f)\n",
    "        \n",
    "    print(\"Done\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def node_training():\n",
    "    save_file = './node_sampling/'\n",
    "    with open(save_file+'memory_record.pickle','rb') as f:\n",
    "        memory_record = pickle.load(f)\n",
    "    with open(save_file+'action_input_record.pickle','rb') as f:\n",
    "        action_record = pickle.load(f)\n",
    "\n",
    "    reward_record = []\n",
    "    for num_memory, action_input in zip(memory_record, action_record):\n",
    "        print(\".\",end=\"\")\n",
    "        for seed in range(10):\n",
    "            np.random.seed(seed)\n",
    "            update([0,1,2,3,4,5], np.where(num_memory==1)[0][0], np.where(action_input==1)[0][0], called = True, seed=seed)\n",
    "    print(\"Training Done\")\n",
    "\n",
    "\n",
    "def Rollout_eval(num_memory, action_input, seed):\n",
    "    save_file = './Qsave_model/'\n",
    "    with open(save_file+\"agent_run\"+str(num_memory)+str(action_input)+str(seed)+\".pickle\",\"rb\") as f:\n",
    "        agent = pickle.load(f)\n",
    "    agent.epsilon = 0.95\n",
    "    max_step = 300\n",
    "    \n",
    "    maze = Maze(10, 50)\n",
    "    observation = maze.reset()\n",
    "    observation = observation[agent.input_ids]\n",
    "    if agent.num_memory > 0:\n",
    "        observation = np.append(observation, agent.memory)\n",
    "    if agent.action_in_flag == True:\n",
    "        observation = np.append(observation, agent.last_action[agent.action_input])\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        action = agent.choose_action(observation)        \n",
    "\n",
    "        observation_, fitness, reward, done = maze.step(action)\n",
    "        observation_ = observation_[agent.input_ids]\n",
    "        if agent.num_memory > 0:\n",
    "            observation_ = np.append(observation_, agent.memory)\n",
    "        if agent.action_in_flag == True:\n",
    "            observation_ = np.append(observation_, agent.last_action[agent.action_input])\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "        #if (episode == max_episode-1):\n",
    "        #    print(\"step :\", maze.position, observation, action, reward)\n",
    "        # RL learn from this transition\n",
    "        agent.learn(observation, action, reward, observation_, done)\n",
    "\n",
    "        # swap observation\n",
    "        observation = observation_\n",
    "\n",
    "        steps += 1\n",
    "        if done or steps>max_step: break\n",
    "    \n",
    "    average_reward = np.mean(rewards)\n",
    "    return np.tan(average_reward/maze.average_reward * np.pi/2)\n",
    "\n",
    "def node_validation():\n",
    "    node_save_file = './node_sampling/'\n",
    "    with open(node_save_file+'memory_record.pickle','rb') as f:\n",
    "        memory_record = pickle.load(f)\n",
    "    with open(node_save_file+'action_input_record.pickle','rb') as f:\n",
    "        action_record = pickle.load(f)\n",
    "      \n",
    "    repeat_times = 10\n",
    "    reward_record = []\n",
    "    reward_std_record = []\n",
    "    for num_memory, action_input in zip(memory_record, action_record):\n",
    "        seed_time_reward = np.zeros((10, repeat_times))\n",
    "        for seed in range(10):\n",
    "            for time in range(repeat_times):\n",
    "                seed_time_reward[seed, time] = Rollout_eval(np.where(num_memory==1)[0][0], np.where(action_input==1)[0][0], seed)\n",
    "        reward_record.append(np.mean(seed_time_reward))\n",
    "        reward_std_record.append(np.std(seed_time_reward))\n",
    "\n",
    "    reward_record = np.vstack(reward_record)\n",
    "    reward_std_record = np.vstack(reward_std_record)\n",
    "    with open(node_save_file+'reward_record.pickle','wb') as f:\n",
    "        pickle.dump(reward_record, f)\n",
    "    with open(node_save_file+'reward_std_record.pickle','wb') as f:\n",
    "        pickle.dump(reward_std_record, f)\n",
    "\n",
    "def data_visual():\n",
    "    node_save_file = './node_sampling/'\n",
    "    with open(node_save_file+'memory_record.pickle','rb') as f:\n",
    "        memory_record = pickle.load(f)\n",
    "    with open(node_save_file+'action_input_record.pickle','rb') as f:\n",
    "        action_record = pickle.load(f)\n",
    "    with open(node_save_file+'reward_record.pickle','rb') as f:\n",
    "        reward_record = pickle.load(f)\n",
    "    with open(node_save_file+'reward_std_record.pickle','rb') as f:\n",
    "        reward_std_record = pickle.load(f)\n",
    "        \n",
    "    memory_counts = np.sum(memory_record*reward_record, axis=0)/4\n",
    "    action_count = np.sum(action_record*reward_record, axis=0)/5\n",
    "    \n",
    "    memory_std_counts = np.sum(memory_record*reward_std_record, axis=0)/4\n",
    "    action_std_count = np.sum(action_record*reward_std_record, axis=0)/5\n",
    "    \n",
    "    with open(node_save_file+'memory_counts.pickle','wb') as f:\n",
    "        pickle.dump(memory_counts, f)\n",
    "    with open(node_save_file+'action_count.pickle','wb') as f:\n",
    "        pickle.dump(action_count, f)\n",
    "        \n",
    "    with open(node_save_file+'memory_std_counts.pickle','wb') as f:\n",
    "        pickle.dump(memory_counts, f)\n",
    "    with open(node_save_file+'action_std_count.pickle','wb') as f:\n",
    "        pickle.dump(action_count, f)\n",
    "        \n",
    "    plt.bar(range(len(memory_counts)), memory_counts, color='green', yerr=memory_std_counts, tick_label=['0', '1', '2', '3','4'])\n",
    "    plt.xlabel('Number of Memory Nodes')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title('Scores by Memory Number')\n",
    "    plt.savefig(node_save_file+'MemoryNumber.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    plt.bar(range(len(action_count)), action_count, color='green', yerr=action_std_count)\n",
    "    plt.xlabel('Types of Action Input')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title('Scores by Action Inputs')\n",
    "    plt.xticks(np.arange(4), ('None', 'Left', 'Right', 'Both'))\n",
    "    plt.savefig(node_save_file+'ActionNumber.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    reward_record = reward_record.flatten()\n",
    "    print(reward_record)\n",
    "    \n",
    "    #plt.bar(range(20), reward_record, color='green', yerr=reward_std_record)\n",
    "    #plt.bar(range(12), reward_record[:12], color='green', yerr=reward_std_record[:12])\n",
    "    \n",
    "    total_width, n = 0.8, 5\n",
    "    width = total_width / n\n",
    "    x = np.arange(4)\n",
    "    x = x - (total_width - width) / 2\n",
    "\n",
    "    plt.bar(x, reward_record[0:4],  width=width, label='memory=0', yerr=reward_std_record[0:4])\n",
    "    plt.bar(x + width, reward_record[4:8],  width=width, label='memory=1', yerr=reward_std_record[4:8])\n",
    "    plt.bar(x + 2 * width, reward_record[8:12],  width=width, label='memory=2', yerr=reward_std_record[8:12])\n",
    "    plt.bar(x + 3 * width, reward_record[12:16],  width=width, label='memory=3', yerr=reward_std_record[12:16])\n",
    "    plt.bar(x + 4 * width, reward_record[16:],  width=width, label='memory=4', yerr=reward_std_record[16:20])\n",
    "    plt.xticks(np.arange(4), ('None', 'Left', 'Right', 'Both'))\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(node_save_file+'ActionMemory.pdf')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    total_width, n = 0.8, 4\n",
    "    width = total_width / n\n",
    "    x = np.arange(5)\n",
    "    x = x - (total_width - width) / 2\n",
    "\n",
    "    plt.bar(x, reward_record[[0,4,8,12,16]],  width=width, label='action=None', yerr=reward_std_record[[0,4,8,12,16]])\n",
    "    plt.bar(x + width, reward_record[[1,5,9,13,17]],  width=width, label='Left', yerr=reward_std_record[[1,5,9,13,17]])\n",
    "    plt.bar(x + 2 * width, reward_record[[2,6,10,14,18]],  width=width, label='Right', yerr=reward_std_record[[2,6,10,14,18]])\n",
    "    plt.bar(x + 3 * width, reward_record[[3,7,11,15,19]],  width=width, label='Both', yerr=reward_std_record[[3,7,11,15,19]])\n",
    "    plt.xticks(np.arange(5), ('Memory=0', '1', '2', '3','4'))\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(node_save_file+'MemoryAction.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "np.random.seed(6)\n",
    "#node_sampler()\n",
    "#node_training()\n",
    "#node_validation()\n",
    "data_visual()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
